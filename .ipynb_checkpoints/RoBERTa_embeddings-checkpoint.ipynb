{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaForMaskedLM, RobertaConfig\n",
    "# import matplotlib.pyplot as plt\n",
    "# % matplotlib inline\n",
    "# from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/crystal.butler/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure we're in the transformers directory with fine-tuned model output.\n",
    "os.chdir('/Users/crystal.butler/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from the tutorial at https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
    "# and Transformers documentation: https://huggingface.co/transformers/model_doc/roberta.html#robertaformaskedlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('/Users/crystal.butler/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/data/output_wiki-103_filtered')\n",
    "config = RobertaConfig.from_pretrained('/Users/crystal.butler/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/data/output_wiki-103_filtered')\n",
    "model = RobertaForMaskedLM.from_pretrained('/Users/crystal.butler/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/data/output_wiki-103_filtered', config=config)\n",
    "# Outputting hidden states allows direct access to hidden layers of the model.\n",
    "# config.output_hidden_states = True\n",
    "model.eval()\n",
    "\n",
    "context_file = \"/Users/crystal.butler/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/data/wiki.test.raw.out\"\n",
    "output_file = '/Users/crystal.butler/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/data/roberta_test.txt'\n",
    "count_file = '/Users/crystal.butler/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/data/roberta_test_counts.txt'\n",
    "vocab_file = '/Users/crystal.butler/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/data/vocab_checked.txt'\n",
    "vocab = make_vocab(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "aback\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for aback.\n",
      "Saved the count of sentences used to create aback embedding\n",
      "Run time for aback was 0.06877287099996465 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ab\n",
      "ashed\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for abashed.\n",
      "Saved the count of sentences used to create abashed embedding\n",
      "Run time for abashed was 0.02842349699994884 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "abhor\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for abhor.\n",
      "Saved the count of sentences used to create abhor embedding\n",
      "Run time for abhor was 0.024280316999920615 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "abhor\n",
      "red\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for abhorred.\n",
      "Saved the count of sentences used to create abhorred embedding\n",
      "Run time for abhorred was 0.024078905999886047 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "abhor\n",
      "rence\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for abhorrence.\n",
      "Saved the count of sentences used to create abhorrence embedding\n",
      "Run time for abhorrence was 0.02399762599998212 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "abhor\n",
      "rent\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for abhorrent.\n",
      "Saved the count of sentences used to create abhorrent embedding\n",
      "Run time for abhorrent was 0.02395725999986098 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "ab\n",
      "omin\n",
      "able\n",
      "\n",
      "Instance 1 of abominable.\n",
      "Looking for vocab token: ab\n",
      "Looking for vocab token: omin\n",
      "Looking for vocab token: able\n",
      "Indices are [15, 16]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "ab at index 15: [0.06792555004358292, 0.28369536995887756, 0.05051126331090927, 0.46096566319465637, -0.4600364565849304]\n",
      "omin at index 16: [-0.016813814640045166, 0.19277450442314148, 0.06668046861886978, -0.04752948507666588, 0.9479538202285767]\n",
      "Grand sum of 1 tensor sets is: [0.025555867701768875, 0.23823493719100952, 0.058595865964889526, 0.2067180871963501, 0.24395868182182312]\n",
      "Mean of tensors is: tensor([0.0256, 0.2382, 0.0586, 0.2067, 0.2440]) (768 features in tensor)\n",
      "Saved the embedding for abominable.\n",
      "Saved the count of sentences used to create abominable embedding\n",
      "Run time for abominable was 0.22967837600003804 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "abound\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for abound.\n",
      "Saved the count of sentences used to create abound embedding\n",
      "Run time for abound was 0.025298281000004863 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "absent\n",
      "\n",
      "Instance 1 of absent.\n",
      "Looking for vocab token: absent\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "absent at index 31: [0.14843827486038208, 0.04582355171442032, 0.17491157352924347, -0.08133866637945175, 0.2885882556438446]\n",
      "Grand sum of 1 tensor sets is: [0.14843827486038208, 0.04582355171442032, 0.17491157352924347, -0.08133866637945175, 0.2885882556438446]\n",
      "\n",
      "Instance 2 of absent.\n",
      "Looking for vocab token: absent\n",
      "Indices are [16, 37]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "absent at index 16: [-0.02350948005914688, 0.16972917318344116, 0.1736392080783844, -0.15934348106384277, -0.31368789076805115]\n",
      "absent at index 37: [0.01831918954849243, 0.13984918594360352, 0.2514174282550812, -0.13553345203399658, -0.03203746676445007]\n",
      "Grand sum of 2 tensor sets is: [0.14584313333034515, 0.20061272382736206, 0.38743990659713745, -0.22877714037895203, 0.115725576877594]\n",
      "\n",
      "Instance 3 of absent.\n",
      "Looking for vocab token: absent\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "absent at index 23: [0.07817971706390381, 0.16546979546546936, 0.11813221126794815, -0.15412814915180206, 0.39116743206977844]\n",
      "Grand sum of 3 tensor sets is: [0.22402285039424896, 0.3660825192928314, 0.5055721402168274, -0.3829053044319153, 0.5068930387496948]\n",
      "\n",
      "Instance 4 of absent.\n",
      "Looking for vocab token: absent\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "absent at index 17: [0.0689462199807167, -0.024369552731513977, 0.09811405092477798, -0.12455013394355774, 0.019996173679828644]\n",
      "Grand sum of 4 tensor sets is: [0.29296907782554626, 0.34171295166015625, 0.6036862134933472, -0.5074554681777954, 0.5268892049789429]\n",
      "Mean of tensors is: tensor([ 0.0732,  0.0854,  0.1509, -0.1269,  0.1317]) (768 features in tensor)\n",
      "Saved the embedding for absent.\n",
      "Saved the count of sentences used to create absent embedding\n",
      "Run time for absent was 0.3253938959999232 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "absorbed\n",
      "\n",
      "Instance 1 of absorbed.\n",
      "Looking for vocab token: absorbed\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "absorbed at index 38: [0.16908174753189087, 0.10756009817123413, -0.008513839915394783, 0.27851057052612305, 0.3770231008529663]\n",
      "Grand sum of 1 tensor sets is: [0.16908174753189087, 0.10756009817123413, -0.008513839915394783, 0.27851057052612305, 0.3770231008529663]\n",
      "\n",
      "Instance 2 of absorbed.\n",
      "Looking for vocab token: absorbed\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "absorbed at index 30: [0.16352909803390503, 0.09212237596511841, -0.13305869698524475, -0.048667483031749725, 0.7765839695930481]\n",
      "Grand sum of 2 tensor sets is: [0.3326108455657959, 0.19968247413635254, -0.14157253503799438, 0.22984308004379272, 1.1536071300506592]\n",
      "\n",
      "Instance 3 of absorbed.\n",
      "Looking for vocab token: absorbed\n",
      "Indices are [45]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "absorbed at index 45: [-0.032996103167533875, 0.3596811294555664, -0.02506312169134617, -0.026045866310596466, -0.07001574337482452]\n",
      "Grand sum of 3 tensor sets is: [0.29961472749710083, 0.559363603591919, -0.166635662317276, 0.20379722118377686, 1.083591341972351]\n",
      "Mean of tensors is: tensor([ 0.0999,  0.1865, -0.0555,  0.0679,  0.3612]) (768 features in tensor)\n",
      "Saved the embedding for absorbed.\n",
      "Saved the count of sentences used to create absorbed embedding\n",
      "Run time for absorbed was 0.2862039710000772 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "acceptance\n",
      "\n",
      "Instance 1 of acceptance.\n",
      "Looking for vocab token: acceptance\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "acceptance at index 3: [0.17614762485027313, -0.11515526473522186, 0.18490572273731232, -0.030470168218016624, 0.3363168239593506]\n",
      "Grand sum of 1 tensor sets is: [0.17614762485027313, -0.11515526473522186, 0.18490572273731232, -0.030470168218016624, 0.3363168239593506]\n",
      "\n",
      "Instance 2 of acceptance.\n",
      "Looking for vocab token: acceptance\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "acceptance at index 14: [-0.034419864416122437, 0.1038825511932373, 0.1136576384305954, 0.16635248064994812, -0.1867552101612091]\n",
      "Grand sum of 2 tensor sets is: [0.1417277604341507, -0.011272713541984558, 0.2985633611679077, 0.13588231801986694, 0.14956161379814148]\n",
      "Mean of tensors is: tensor([ 0.0709, -0.0056,  0.1493,  0.0679,  0.0748]) (768 features in tensor)\n",
      "Saved the embedding for acceptance.\n",
      "Saved the count of sentences used to create acceptance embedding\n",
      "Run time for acceptance was 0.20918007499994928 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "accepted\n",
      "\n",
      "Instance 1 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "accepted at index 3: [0.31527239084243774, 0.3824237585067749, 0.03503768518567085, 0.3566093444824219, 0.13334782421588898]\n",
      "Grand sum of 1 tensor sets is: [0.31527239084243774, 0.3824237585067749, 0.03503768518567085, 0.3566093444824219, 0.13334782421588898]\n",
      "\n",
      "Instance 2 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "accepted at index 3: [0.07364761084318161, 0.03981919214129448, 0.026067452505230904, 0.1746038794517517, 0.8003053069114685]\n",
      "Grand sum of 2 tensor sets is: [0.38892000913619995, 0.4222429394721985, 0.061105139553546906, 0.5312132239341736, 0.9336531162261963]\n",
      "\n",
      "Instance 3 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "accepted at index 27: [-0.11929482221603394, 0.02767271175980568, -0.13306935131549835, 0.4885920286178589, 0.16952836513519287]\n",
      "Grand sum of 3 tensor sets is: [0.269625186920166, 0.44991564750671387, -0.07196421176195145, 1.0198051929473877, 1.1031814813613892]\n",
      "\n",
      "Instance 4 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "accepted at index 26: [0.1258816421031952, -0.08449521660804749, -0.12492911517620087, 0.5103059411048889, 0.30571746826171875]\n",
      "Grand sum of 4 tensor sets is: [0.3955068290233612, 0.3654204308986664, -0.1968933343887329, 1.5301110744476318, 1.408898949623108]\n",
      "\n",
      "Instance 5 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "accepted at index 22: [-0.005839262157678604, 0.0065529122948646545, 0.007053704932332039, 0.2620042860507965, 0.30201423168182373]\n",
      "Grand sum of 5 tensor sets is: [0.3896675705909729, 0.37197333574295044, -0.18983963131904602, 1.792115330696106, 1.7109131813049316]\n",
      "\n",
      "Instance 6 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "accepted at index 19: [0.055956169962882996, 0.10654482245445251, -0.1882839798927307, 0.127366304397583, 0.40761083364486694]\n",
      "Grand sum of 6 tensor sets is: [0.4456237554550171, 0.47851815819740295, -0.37812361121177673, 1.919481635093689, 2.1185240745544434]\n",
      "\n",
      "Instance 7 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "accepted at index 3: [0.04771412909030914, -0.06445986032485962, -0.07482258975505829, 0.20206670463085175, 0.9173025488853455]\n",
      "Grand sum of 7 tensor sets is: [0.49333786964416504, 0.41405829787254333, -0.45294618606567383, 2.1215484142303467, 3.0358266830444336]\n",
      "\n",
      "Instance 8 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "accepted at index 24: [0.15371015667915344, 0.27520525455474854, -0.004592472687363625, -0.0031294722575694323, 0.3256704807281494]\n",
      "Grand sum of 8 tensor sets is: [0.6470479965209961, 0.6892635822296143, -0.4575386643409729, 2.1184189319610596, 3.361497163772583]\n",
      "\n",
      "Instance 9 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "accepted at index 14: [0.19017377495765686, 0.11428150534629822, 0.010414614342153072, 0.42573899030685425, 0.11101701855659485]\n",
      "Grand sum of 9 tensor sets is: [0.8372217416763306, 0.8035451173782349, -0.44712406396865845, 2.5441579818725586, 3.4725141525268555]\n",
      "\n",
      "Instance 10 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "accepted at index 11: [-0.07164224237203598, 0.10343874990940094, -0.07630510628223419, 0.40019911527633667, 0.31786173582077026]\n",
      "Grand sum of 10 tensor sets is: [0.7655795216560364, 0.9069838523864746, -0.5234291553497314, 2.94435715675354, 3.7903759479522705]\n",
      "\n",
      "Instance 11 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "accepted at index 36: [0.25432631373405457, 0.10714972019195557, 0.014403698034584522, 0.1570647805929184, -0.3822193741798401]\n",
      "Grand sum of 11 tensor sets is: [1.0199058055877686, 1.0141335725784302, -0.5090254545211792, 3.10142183303833, 3.408156633377075]\n",
      "\n",
      "Instance 12 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "accepted at index 9: [0.043595749884843826, -0.005929537117481232, -0.12919555604457855, 0.21360057592391968, 0.6784359812736511]\n",
      "Grand sum of 12 tensor sets is: [1.0635015964508057, 1.0082039833068848, -0.638221025466919, 3.3150224685668945, 4.086592674255371]\n",
      "\n",
      "Instance 13 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "accepted at index 23: [0.12998247146606445, -0.027080610394477844, 0.08765701204538345, 0.48577961325645447, -0.010006099939346313]\n",
      "Grand sum of 13 tensor sets is: [1.1934840679168701, 0.9811233878135681, -0.5505639910697937, 3.800801992416382, 4.076586723327637]\n",
      "\n",
      "Instance 14 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "accepted at index 5: [0.05722447484731674, 0.3825082778930664, -0.005771718919277191, 0.2277694195508957, 0.012131556868553162]\n",
      "Grand sum of 14 tensor sets is: [1.2507085800170898, 1.3636317253112793, -0.5563356876373291, 4.028571605682373, 4.088718414306641]\n",
      "\n",
      "Instance 15 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "accepted at index 25: [0.12250278145074844, 0.034740619361400604, 0.027965279296040535, 0.35419175028800964, -0.344702810049057]\n",
      "Grand sum of 15 tensor sets is: [1.37321138381958, 1.3983722925186157, -0.5283703804016113, 4.382763385772705, 3.744015693664551]\n",
      "\n",
      "Instance 16 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "accepted at index 4: [0.030197689309716225, -0.021172717213630676, 0.008647509850561619, 0.5794013142585754, 0.21002572774887085]\n",
      "Grand sum of 16 tensor sets is: [1.4034091234207153, 1.3771995306015015, -0.5197228789329529, 4.962164878845215, 3.9540414810180664]\n",
      "\n",
      "Instance 17 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "accepted at index 28: [-0.02572029083967209, -0.17438963055610657, -0.14242656528949738, 0.2915196418762207, -0.08960045874118805]\n",
      "Grand sum of 17 tensor sets is: [1.3776888847351074, 1.2028099298477173, -0.6621494293212891, 5.2536845207214355, 3.86444091796875]\n",
      "\n",
      "Instance 18 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "accepted at index 12: [0.08536431938409805, 0.05067424476146698, -0.15328656136989594, 0.25065332651138306, 0.39228034019470215]\n",
      "Grand sum of 18 tensor sets is: [1.4630532264709473, 1.2534841299057007, -0.8154360055923462, 5.504337787628174, 4.256721496582031]\n",
      "\n",
      "Instance 19 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "accepted at index 33: [-0.05929044634103775, 0.11495724320411682, -0.11384306848049164, 0.3998710811138153, 0.2771539092063904]\n",
      "Grand sum of 19 tensor sets is: [1.4037628173828125, 1.3684413433074951, -0.929279088973999, 5.904208660125732, 4.533875465393066]\n",
      "Mean of tensors is: tensor([ 0.0739,  0.0720, -0.0489,  0.3107,  0.2386]) (768 features in tensor)\n",
      "Saved the embedding for accepted.\n",
      "Saved the count of sentences used to create accepted embedding\n",
      "Run time for accepted was 1.3439671030000682 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "accepting\n",
      "\n",
      "Instance 1 of accepting.\n",
      "Looking for vocab token: accepting\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "accepting at index 10: [0.24595578014850616, 0.07148412615060806, -0.04664992541074753, 0.30898892879486084, 0.4655201733112335]\n",
      "Grand sum of 1 tensor sets is: [0.24595578014850616, 0.07148412615060806, -0.04664992541074753, 0.30898892879486084, 0.4655201733112335]\n",
      "\n",
      "Instance 2 of accepting.\n",
      "Looking for vocab token: accepting\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "accepting at index 11: [0.02351229265332222, 0.19728612899780273, 0.026657521724700928, 0.20548617839813232, 0.7040669918060303]\n",
      "Grand sum of 2 tensor sets is: [0.2694680690765381, 0.2687702476978302, -0.0199924036860466, 0.5144751071929932, 1.1695871353149414]\n",
      "\n",
      "Instance 3 of accepting.\n",
      "Looking for vocab token: accepting\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "accepting at index 33: [-0.07259106636047363, 0.03504542261362076, -0.028884420171380043, -0.03840100020170212, 0.3502596914768219]\n",
      "Grand sum of 3 tensor sets is: [0.19687700271606445, 0.30381566286087036, -0.048876821994781494, 0.47607409954071045, 1.519846796989441]\n",
      "Mean of tensors is: tensor([ 0.0656,  0.1013, -0.0163,  0.1587,  0.5066]) (768 features in tensor)\n",
      "Saved the embedding for accepting.\n",
      "Saved the count of sentences used to create accepting embedding\n",
      "Run time for accepting was 0.2690446840001641 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "accommodating\n",
      "\n",
      "Instance 1 of accommodating.\n",
      "Looking for vocab token: accommodating\n",
      "Indices are [40]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([74, 13, 768])\n",
      "Shape of summed layers is: 74 x 768\n",
      "accommodating at index 40: [0.16071467101573944, 0.1875777542591095, -0.02297905646264553, 0.18755941092967987, 0.7809291481971741]\n",
      "Grand sum of 1 tensor sets is: [0.16071467101573944, 0.1875777542591095, -0.02297905646264553, 0.18755941092967987, 0.7809291481971741]\n",
      "Mean of tensors is: tensor([ 0.1607,  0.1876, -0.0230,  0.1876,  0.7809]) (768 features in tensor)\n",
      "Saved the embedding for accommodating.\n",
      "Saved the count of sentences used to create accommodating embedding\n",
      "Run time for accommodating was 0.15933335800013992 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "accomplished\n",
      "\n",
      "Instance 1 of accomplished.\n",
      "Looking for vocab token: accomplished\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "accomplished at index 18: [-0.04389871656894684, -0.006582438945770264, 0.22913262248039246, -0.1600954830646515, 0.07970038801431656]\n",
      "Grand sum of 1 tensor sets is: [-0.04389871656894684, -0.006582438945770264, 0.22913262248039246, -0.1600954830646515, 0.07970038801431656]\n",
      "\n",
      "Instance 2 of accomplished.\n",
      "Looking for vocab token: accomplished\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "accomplished at index 2: [0.13993659615516663, 0.07892926037311554, 0.40447723865509033, -0.07857934385538101, 0.22581759095191956]\n",
      "Grand sum of 2 tensor sets is: [0.09603787958621979, 0.07234682142734528, 0.6336098909378052, -0.2386748194694519, 0.3055179715156555]\n",
      "\n",
      "Instance 3 of accomplished.\n",
      "Looking for vocab token: accomplished\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "accomplished at index 11: [0.040347304195165634, 0.3676615357398987, 0.15343107283115387, -0.23540496826171875, -0.05082383751869202]\n",
      "Grand sum of 3 tensor sets is: [0.13638518750667572, 0.44000834226608276, 0.7870409488677979, -0.47407978773117065, 0.2546941339969635]\n",
      "Mean of tensors is: tensor([ 0.0455,  0.1467,  0.2623, -0.1580,  0.0849]) (768 features in tensor)\n",
      "Saved the embedding for accomplished.\n",
      "Saved the count of sentences used to create accomplished embedding\n",
      "Run time for accomplished was 0.2077775410000413 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "accord\n",
      "ant\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for accordant.\n",
      "Saved the count of sentences used to create accordant embedding\n",
      "Run time for accordant was 0.03244292699992002 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "acc\n",
      "ursed\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for accursed.\n",
      "Saved the count of sentences used to create accursed embedding\n",
      "Run time for accursed was 0.024359464999861302 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "accus\n",
      "atory\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for accusatory.\n",
      "Saved the count of sentences used to create accusatory embedding\n",
      "Run time for accusatory was 0.024910955999985163 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "accused\n",
      "\n",
      "Instance 1 of accused.\n",
      "Looking for vocab token: accused\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "accused at index 15: [-0.08472411334514618, -0.28736555576324463, 0.20843791961669922, 0.0966869592666626, 0.04399723559617996]\n",
      "Grand sum of 1 tensor sets is: [-0.08472411334514618, -0.28736555576324463, 0.20843791961669922, 0.0966869592666626, 0.04399723559617996]\n",
      "\n",
      "Instance 2 of accused.\n",
      "Looking for vocab token: accused\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "accused at index 4: [-0.0013168156147003174, -0.2663661241531372, 0.23553645610809326, 0.10628014802932739, 0.7536068558692932]\n",
      "Grand sum of 2 tensor sets is: [-0.0860409289598465, -0.5537316799163818, 0.4439743757247925, 0.20296710729599, 0.7976040840148926]\n",
      "\n",
      "Instance 3 of accused.\n",
      "Looking for vocab token: accused\n",
      "Indices are [58]\n",
      "Size of token embeddings is torch.Size([72, 13, 768])\n",
      "Shape of summed layers is: 72 x 768\n",
      "accused at index 58: [-0.2051772177219391, -0.25139403343200684, 0.011400710791349411, -0.05303548276424408, 0.49842119216918945]\n",
      "Grand sum of 3 tensor sets is: [-0.2912181615829468, -0.8051257133483887, 0.455375075340271, 0.1499316245317459, 1.296025276184082]\n",
      "\n",
      "Instance 4 of accused.\n",
      "Looking for vocab token: accused\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "accused at index 4: [0.07826172560453415, -0.13160057365894318, 0.04041782021522522, 0.20116397738456726, 0.3077247738838196]\n",
      "Grand sum of 4 tensor sets is: [-0.21295642852783203, -0.9367262721061707, 0.4957928955554962, 0.35109561681747437, 1.6037499904632568]\n",
      "\n",
      "Instance 5 of accused.\n",
      "Looking for vocab token: accused\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "accused at index 20: [0.008823782205581665, -0.3748401999473572, 0.15085813403129578, 0.005297700874507427, -0.06421923637390137]\n",
      "Grand sum of 5 tensor sets is: [-0.20413264632225037, -1.3115664720535278, 0.646651029586792, 0.35639330744743347, 1.5395307540893555]\n",
      "\n",
      "Instance 6 of accused.\n",
      "Looking for vocab token: accused\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "accused at index 12: [-0.028045237064361572, -0.3713756799697876, 0.20793962478637695, -0.041945356875658035, 0.41213861107826233]\n",
      "Grand sum of 6 tensor sets is: [-0.23217788338661194, -1.6829421520233154, 0.854590654373169, 0.31444793939590454, 1.9516693353652954]\n",
      "\n",
      "Instance 7 of accused.\n",
      "Looking for vocab token: accused\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "accused at index 10: [-0.07170556485652924, -0.23620320856571198, 0.1663372665643692, 0.0634080246090889, 0.1652279794216156]\n",
      "Grand sum of 7 tensor sets is: [-0.30388343334198, -1.9191453456878662, 1.020927906036377, 0.37785595655441284, 2.1168973445892334]\n",
      "\n",
      "Instance 8 of accused.\n",
      "Looking for vocab token: accused\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "accused at index 5: [0.02282571792602539, -0.2696911096572876, 0.11522143334150314, -0.139625683426857, 0.019563600420951843]\n",
      "Grand sum of 8 tensor sets is: [-0.2810577154159546, -2.1888365745544434, 1.136149287223816, 0.23823027312755585, 2.136461019515991]\n",
      "Mean of tensors is: tensor([-0.0351, -0.2736,  0.1420,  0.0298,  0.2671]) (768 features in tensor)\n",
      "Saved the embedding for accused.\n",
      "Saved the count of sentences used to create accused embedding\n",
      "Run time for accused was 0.6202891329999147 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "accusing\n",
      "\n",
      "Instance 1 of accusing.\n",
      "Looking for vocab token: accusing\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "accusing at index 17: [-0.06880398094654083, -0.06326080858707428, 0.033858709037303925, -0.06842761486768723, 0.5156391263008118]\n",
      "Grand sum of 1 tensor sets is: [-0.06880398094654083, -0.06326080858707428, 0.033858709037303925, -0.06842761486768723, 0.5156391263008118]\n",
      "Mean of tensors is: tensor([-0.0688, -0.0633,  0.0339, -0.0684,  0.5156]) (768 features in tensor)\n",
      "Saved the embedding for accusing.\n",
      "Saved the count of sentences used to create accusing embedding\n",
      "Run time for accusing was 0.09393120699996871 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "ac\n",
      "erb\n",
      "ic\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for acerbic.\n",
      "Saved the count of sentences used to create acerbic embedding\n",
      "Run time for acerbic was 0.04151902099988547 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "acidic\n",
      "\n",
      "Instance 1 of acidic.\n",
      "Looking for vocab token: acidic\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([69, 13, 768])\n",
      "Shape of summed layers is: 69 x 768\n",
      "acidic at index 23: [0.05240260809659958, -0.06946180760860443, -0.03852126747369766, -0.027015866711735725, 0.40639230608940125]\n",
      "Grand sum of 1 tensor sets is: [0.05240260809659958, -0.06946180760860443, -0.03852126747369766, -0.027015866711735725, 0.40639230608940125]\n",
      "Mean of tensors is: tensor([ 0.0524, -0.0695, -0.0385, -0.0270,  0.4064]) (768 features in tensor)\n",
      "Saved the embedding for acidic.\n",
      "Saved the count of sentences used to create acidic embedding\n",
      "Run time for acidic was 0.15530332699995597 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "active\n",
      "\n",
      "Instance 1 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "active at index 24: [-0.10148297250270844, 0.06392201781272888, 0.0965723916888237, 0.3678937256336212, -0.11603179574012756]\n",
      "Grand sum of 1 tensor sets is: [-0.10148297250270844, 0.06392201781272888, 0.0965723916888237, 0.3678937256336212, -0.11603179574012756]\n",
      "\n",
      "Instance 2 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "active at index 19: [0.08776158094406128, -0.0004821270704269409, 0.06824815273284912, 0.191207155585289, -0.31917595863342285]\n",
      "Grand sum of 2 tensor sets is: [-0.013721391558647156, 0.06343989074230194, 0.16482055187225342, 0.559100866317749, -0.4352077543735504]\n",
      "\n",
      "Instance 3 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "active at index 17: [0.017609374597668648, 0.2944364547729492, 0.361152708530426, 0.30739685893058777, -0.7852016091346741]\n",
      "Grand sum of 3 tensor sets is: [0.003887983039021492, 0.35787636041641235, 0.5259732604026794, 0.8664977550506592, -1.2204093933105469]\n",
      "\n",
      "Instance 4 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "active at index 26: [0.0973002165555954, 0.0032387971878051758, -0.00901215709745884, 0.3552281856536865, -0.35985302925109863]\n",
      "Grand sum of 4 tensor sets is: [0.10118819773197174, 0.36111515760421753, 0.5169610977172852, 1.2217259407043457, -1.5802624225616455]\n",
      "\n",
      "Instance 5 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "active at index 9: [0.18956197798252106, 0.22880248725414276, 0.11391574889421463, 0.5409116744995117, -0.6028144359588623]\n",
      "Grand sum of 5 tensor sets is: [0.2907501757144928, 0.5899176597595215, 0.6308768391609192, 1.7626376152038574, -2.183076858520508]\n",
      "\n",
      "Instance 6 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "active at index 27: [-0.025129973888397217, -0.08917112648487091, -0.019709086045622826, 0.5450406074523926, -0.842296838760376]\n",
      "Grand sum of 6 tensor sets is: [0.2656202018260956, 0.5007465481758118, 0.6111677289009094, 2.30767822265625, -3.025373697280884]\n",
      "\n",
      "Instance 7 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "active at index 14: [0.2151871621608734, -0.03998718410730362, 0.057494789361953735, 0.10463743656873703, -0.5762385725975037]\n",
      "Grand sum of 7 tensor sets is: [0.480807363986969, 0.46075937151908875, 0.6686625480651855, 2.412315607070923, -3.6016123294830322]\n",
      "\n",
      "Instance 8 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "active at index 29: [0.04733189195394516, -0.12996122241020203, 0.03889086842536926, -0.04028075188398361, 0.663524866104126]\n",
      "Grand sum of 8 tensor sets is: [0.5281392335891724, 0.3307981491088867, 0.7075533866882324, 2.372034788131714, -2.9380874633789062]\n",
      "\n",
      "Instance 9 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "active at index 17: [-0.12019650638103485, -0.2308669537305832, 0.02476239576935768, 0.20746958255767822, -0.7091408371925354]\n",
      "Grand sum of 9 tensor sets is: [0.4079427123069763, 0.09993119537830353, 0.7323157787322998, 2.5795044898986816, -3.647228240966797]\n",
      "\n",
      "Instance 10 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "active at index 5: [0.005576286464929581, 0.0764036551117897, 0.0006837770342826843, 0.5913102626800537, -0.4659793972969055]\n",
      "Grand sum of 10 tensor sets is: [0.4135189950466156, 0.17633485794067383, 0.7329995632171631, 3.1708147525787354, -4.113207817077637]\n",
      "\n",
      "Instance 11 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "active at index 38: [0.08234972506761551, -0.03710886836051941, 0.15418215095996857, 0.08560000360012054, -0.6471866965293884]\n",
      "Grand sum of 11 tensor sets is: [0.4958687126636505, 0.13922598958015442, 0.8871816992759705, 3.2564146518707275, -4.76039457321167]\n",
      "\n",
      "Instance 12 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([60, 13, 768])\n",
      "Shape of summed layers is: 60 x 768\n",
      "active at index 14: [-0.013257201761007309, -0.15667670965194702, 0.19594529271125793, 0.258463054895401, -0.3136965036392212]\n",
      "Grand sum of 12 tensor sets is: [0.4826115071773529, -0.017450720071792603, 1.0831270217895508, 3.5148777961730957, -5.074090957641602]\n",
      "\n",
      "Instance 13 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "active at index 13: [0.21349862217903137, -0.015965953469276428, 0.18963642418384552, 0.15169523656368256, 0.08317317068576813]\n",
      "Grand sum of 13 tensor sets is: [0.6961101293563843, -0.03341667354106903, 1.2727634906768799, 3.6665730476379395, -4.990917682647705]\n",
      "\n",
      "Instance 14 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "active at index 19: [0.1213967427611351, -0.03324344754219055, 0.23199748992919922, 0.30310383439064026, -0.5003369450569153]\n",
      "Grand sum of 14 tensor sets is: [0.8175068497657776, -0.06666012108325958, 1.504760980606079, 3.969676971435547, -5.491254806518555]\n",
      "\n",
      "Instance 15 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "active at index 8: [0.17608051002025604, -0.06387120485305786, -0.023674344643950462, 0.32881563901901245, 0.13299313187599182]\n",
      "Grand sum of 15 tensor sets is: [0.9935873746871948, -0.13053132593631744, 1.4810866117477417, 4.298492431640625, -5.358261585235596]\n",
      "\n",
      "Instance 16 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "active at index 3: [0.053572703152894974, 0.010863777250051498, 0.10981204360723495, 0.5526849627494812, -0.5628464818000793]\n",
      "Grand sum of 16 tensor sets is: [1.047160029411316, -0.11966754496097565, 1.5908986330032349, 4.851177215576172, -5.921108245849609]\n",
      "\n",
      "Instance 17 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "active at index 6: [-0.035960614681243896, 0.07013612985610962, 0.017918026074767113, 0.27866247296333313, -0.22501930594444275]\n",
      "Grand sum of 17 tensor sets is: [1.0111994743347168, -0.04953141510486603, 1.6088166236877441, 5.129839897155762, -6.146127700805664]\n",
      "\n",
      "Instance 18 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [3, 12]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "active at index 3: [-0.016649000346660614, 0.04156996309757233, 0.07571525871753693, 0.6398730278015137, -0.4375948905944824]\n",
      "active at index 12: [-0.024205662310123444, -0.10534630715847015, 0.16392068564891815, 0.489614874124527, -0.648101270198822]\n",
      "Grand sum of 18 tensor sets is: [0.9907721281051636, -0.08141958713531494, 1.7286345958709717, 5.694583892822266, -6.688975811004639]\n",
      "\n",
      "Instance 19 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "active at index 10: [0.051385123282670975, 0.07780876755714417, 0.07646449655294418, 0.5947818756103516, -0.08255429565906525]\n",
      "Grand sum of 19 tensor sets is: [1.0421572923660278, -0.0036108195781707764, 1.8050991296768188, 6.289365768432617, -6.7715301513671875]\n",
      "\n",
      "Instance 20 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "active at index 4: [0.02248813770711422, -0.08689554035663605, -0.08119618147611618, 0.37943235039711, -0.3275769352912903]\n",
      "Grand sum of 20 tensor sets is: [1.0646454095840454, -0.09050635993480682, 1.723902940750122, 6.668797969818115, -7.099107265472412]\n",
      "\n",
      "Instance 21 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "active at index 3: [-0.03464864194393158, 0.002550557255744934, 0.026797380298376083, 0.5373083353042603, -0.25723904371261597]\n",
      "Grand sum of 21 tensor sets is: [1.0299967527389526, -0.08795580267906189, 1.7507003545761108, 7.206106185913086, -7.356346130371094]\n",
      "\n",
      "Instance 22 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "active at index 32: [-0.029190607368946075, 0.25667625665664673, 0.18565872311592102, 0.2579237222671509, -0.016910135746002197]\n",
      "Grand sum of 22 tensor sets is: [1.0008060932159424, 0.16872045397758484, 1.9363590478897095, 7.464029788970947, -7.373256206512451]\n",
      "\n",
      "Instance 23 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "active at index 22: [-0.04695665091276169, 0.01645827293395996, 0.19837507605552673, 0.4608666002750397, -0.49026280641555786]\n",
      "Grand sum of 23 tensor sets is: [0.9538494348526001, 0.1851787269115448, 2.1347341537475586, 7.924896240234375, -7.863519191741943]\n",
      "Mean of tensors is: tensor([ 0.0415,  0.0081,  0.0928,  0.3446, -0.3419]) (768 features in tensor)\n",
      "Saved the embedding for active.\n",
      "Saved the count of sentences used to create active embedding\n",
      "Run time for active was 1.5504142970000885 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "acute\n",
      "\n",
      "Instance 1 of acute.\n",
      "Looking for vocab token: acute\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "acute at index 6: [0.39808544516563416, 0.07963380217552185, 0.21332883834838867, 0.7716720104217529, 0.8014234304428101]\n",
      "Grand sum of 1 tensor sets is: [0.39808544516563416, 0.07963380217552185, 0.21332883834838867, 0.7716720104217529, 0.8014234304428101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of tensors is: tensor([0.3981, 0.0796, 0.2133, 0.7717, 0.8014]) (768 features in tensor)\n",
      "Saved the embedding for acute.\n",
      "Saved the count of sentences used to create acute embedding\n",
      "Run time for acute was 0.12699194299989358 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "adamant\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for adamant.\n",
      "Saved the count of sentences used to create adamant embedding\n",
      "Run time for adamant was 0.028390168999976595 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "add\n",
      "led\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for addled.\n",
      "Saved the count of sentences used to create addled embedding\n",
      "Run time for addled was 0.029982392999954754 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "admiration\n",
      "\n",
      "Instance 1 of admiration.\n",
      "Looking for vocab token: admiration\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "admiration at index 9: [0.047818660736083984, -0.046084366738796234, 0.15116649866104126, -0.4402097761631012, -0.13214081525802612]\n",
      "Grand sum of 1 tensor sets is: [0.047818660736083984, -0.046084366738796234, 0.15116649866104126, -0.4402097761631012, -0.13214081525802612]\n",
      "\n",
      "Instance 2 of admiration.\n",
      "Looking for vocab token: admiration\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "admiration at index 11: [-0.016715526580810547, -0.2254863828420639, 0.2587202787399292, -0.5055378079414368, -0.2607336938381195]\n",
      "Grand sum of 2 tensor sets is: [0.031103134155273438, -0.27157074213027954, 0.40988677740097046, -0.9457476139068604, -0.39287450909614563]\n",
      "\n",
      "Instance 3 of admiration.\n",
      "Looking for vocab token: admiration\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "admiration at index 16: [-0.15213486552238464, -0.23397298157215118, 0.4559718072414398, -0.32083696126937866, -0.3369424641132355]\n",
      "Grand sum of 3 tensor sets is: [-0.1210317313671112, -0.5055437088012695, 0.8658585548400879, -1.2665846347808838, -0.7298169732093811]\n",
      "Mean of tensors is: tensor([-0.0403, -0.1685,  0.2886, -0.4222, -0.2433]) (768 features in tensor)\n",
      "Saved the embedding for admiration.\n",
      "Saved the count of sentences used to create admiration embedding\n",
      "Run time for admiration was 0.21849895600007585 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "admit\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for admit.\n",
      "Saved the count of sentences used to create admit embedding\n",
      "Run time for admit was 0.02719696499980273 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ad\n",
      "oration\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for adoration.\n",
      "Saved the count of sentences used to create adoration embedding\n",
      "Run time for adoration was 0.024677861999862216 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ad\n",
      "oring\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for adoring.\n",
      "Saved the count of sentences used to create adoring embedding\n",
      "Run time for adoring was 0.025809465999827808 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ad\n",
      "rift\n",
      "\n",
      "Instance 1 of adrift.\n",
      "Looking for vocab token: ad\n",
      "Looking for vocab token: rift\n",
      "Indices are [22, 23]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "ad at index 22: [-0.2264108955860138, 0.28767263889312744, -0.021957309916615486, 0.10376035422086716, -0.10914026200771332]\n",
      "rift at index 23: [-0.04530048370361328, -0.031957708299160004, -0.09653664380311966, 0.4133065640926361, 1.3435378074645996]\n",
      "Grand sum of 1 tensor sets is: [-0.13585568964481354, 0.12785746157169342, -0.059246975928545, 0.25853344798088074, 0.6171987652778625]\n",
      "Mean of tensors is: tensor([-0.1359,  0.1279, -0.0592,  0.2585,  0.6172]) (768 features in tensor)\n",
      "Saved the embedding for adrift.\n",
      "Saved the count of sentences used to create adrift embedding\n",
      "Run time for adrift was 0.09069350100003248 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "advers\n",
      "arial\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for adversarial.\n",
      "Saved the count of sentences used to create adversarial embedding\n",
      "Run time for adversarial was 0.03390436399990904 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "aff\n",
      "ability\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for affability.\n",
      "Saved the count of sentences used to create affability embedding\n",
      "Run time for affability was 0.025440698000011253 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "affected\n",
      "\n",
      "Instance 1 of affected.\n",
      "Looking for vocab token: affected\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "affected at index 19: [-0.004114847630262375, 0.6229269504547119, 0.024000372737646103, 0.5934088230133057, -0.4544891119003296]\n",
      "Grand sum of 1 tensor sets is: [-0.004114847630262375, 0.6229269504547119, 0.024000372737646103, 0.5934088230133057, -0.4544891119003296]\n",
      "\n",
      "Instance 2 of affected.\n",
      "Looking for vocab token: affected\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "affected at index 11: [0.2560499310493469, 0.5016027688980103, -0.005015883594751358, 0.6992526054382324, -0.012045100331306458]\n",
      "Grand sum of 2 tensor sets is: [0.25193509459495544, 1.1245297193527222, 0.018984489142894745, 1.292661428451538, -0.46653419733047485]\n",
      "\n",
      "Instance 3 of affected.\n",
      "Looking for vocab token: affected\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "affected at index 10: [-0.23789358139038086, 0.5481199622154236, 0.5174052715301514, 0.34906208515167236, -0.4996485114097595]\n",
      "Grand sum of 3 tensor sets is: [0.014041513204574585, 1.672649621963501, 0.5363897681236267, 1.6417235136032104, -0.9661827087402344]\n",
      "\n",
      "Instance 4 of affected.\n",
      "Looking for vocab token: affected\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "affected at index 8: [0.21088820695877075, 0.593582272529602, 0.010191100649535656, 0.5257152318954468, -0.24742642045021057]\n",
      "Grand sum of 4 tensor sets is: [0.22492972016334534, 2.2662320137023926, 0.5465808510780334, 2.1674387454986572, -1.2136090993881226]\n",
      "\n",
      "Instance 5 of affected.\n",
      "Looking for vocab token: affected\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "affected at index 3: [0.08629624545574188, 0.3434506058692932, 0.08586379140615463, 0.7004913091659546, -0.14348086714744568]\n",
      "Grand sum of 5 tensor sets is: [0.311225950717926, 2.609682559967041, 0.6324446201324463, 2.8679299354553223, -1.3570899963378906]\n",
      "\n",
      "Instance 6 of affected.\n",
      "Looking for vocab token: affected\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "affected at index 5: [0.19279122352600098, 0.5335580706596375, 0.212822824716568, 0.3287631571292877, 0.3169293999671936]\n",
      "Grand sum of 6 tensor sets is: [0.504017174243927, 3.1432406902313232, 0.8452674150466919, 3.196693181991577, -1.0401606559753418]\n",
      "\n",
      "Instance 7 of affected.\n",
      "Looking for vocab token: affected\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "affected at index 5: [0.2380753457546234, 0.4090193510055542, 0.17236706614494324, 0.48851925134658813, 0.1720593273639679]\n",
      "Grand sum of 7 tensor sets is: [0.742092490196228, 3.552259922027588, 1.0176345109939575, 3.6852123737335205, -0.8681013584136963]\n",
      "\n",
      "Instance 8 of affected.\n",
      "Looking for vocab token: affected\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "affected at index 23: [0.12588655948638916, 0.47192567586898804, 0.07193931937217712, 0.6647350788116455, 0.21739661693572998]\n",
      "Grand sum of 8 tensor sets is: [0.8679790496826172, 4.024185657501221, 1.089573860168457, 4.349947452545166, -0.6507047414779663]\n",
      "\n",
      "Instance 9 of affected.\n",
      "Looking for vocab token: affected\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "affected at index 9: [-0.05033008009195328, 0.21489432454109192, 0.08987255394458771, 0.5326311588287354, 0.17906028032302856]\n",
      "Grand sum of 9 tensor sets is: [0.8176489472389221, 4.23907995223999, 1.1794464588165283, 4.8825788497924805, -0.47164446115493774]\n",
      "\n",
      "Instance 10 of affected.\n",
      "Looking for vocab token: affected\n",
      "Indices are [7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "affected at index 7: [0.057363126426935196, 0.5202317833900452, 0.11155638098716736, 0.5733896493911743, 0.09103856980800629]\n",
      "Grand sum of 10 tensor sets is: [0.8750120997428894, 4.759311676025391, 1.291002869606018, 5.455968379974365, -0.38060587644577026]\n",
      "\n",
      "Instance 11 of affected.\n",
      "Looking for vocab token: affected\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "affected at index 25: [0.03514000028371811, 0.14788757264614105, 0.07729656994342804, 0.39655354619026184, -0.3930742144584656]\n",
      "Grand sum of 11 tensor sets is: [0.9101520776748657, 4.907199382781982, 1.3682994842529297, 5.852521896362305, -0.7736800909042358]\n",
      "\n",
      "Instance 12 of affected.\n",
      "Looking for vocab token: affected\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "affected at index 16: [0.14838993549346924, 0.5929445028305054, 0.050800152122974396, 0.5316956043243408, -0.378156840801239]\n",
      "Grand sum of 12 tensor sets is: [1.058542013168335, 5.500144004821777, 1.4190996885299683, 6.384217262268066, -1.15183687210083]\n",
      "Mean of tensors is: tensor([ 0.0882,  0.4583,  0.1183,  0.5320, -0.0960]) (768 features in tensor)\n",
      "Saved the embedding for affected.\n",
      "Saved the count of sentences used to create affected embedding\n",
      "Run time for affected was 0.7910899560001781 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "affection\n",
      "ate\n",
      "\n",
      "Instance 1 of affectionate.\n",
      "Looking for vocab token: affection\n",
      "Looking for vocab token: ate\n",
      "Indices are [21, 22]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "affection at index 21: [0.08059604465961456, 0.31559157371520996, 0.2713240385055542, -0.24563291668891907, -1.9019033908843994]\n",
      "ate at index 22: [-0.07819665968418121, 0.16949772834777832, 0.1435145139694214, 0.7502479553222656, 1.0565195083618164]\n",
      "Grand sum of 1 tensor sets is: [0.0011996924877166748, 0.24254465103149414, 0.2074192762374878, 0.2523075342178345, -0.4226919412612915]\n",
      "Mean of tensors is: tensor([ 0.0012,  0.2425,  0.2074,  0.2523, -0.4227]) (768 features in tensor)\n",
      "Saved the embedding for affectionate.\n",
      "Saved the count of sentences used to create affectionate embedding\n",
      "Run time for affectionate was 0.1137063680000665 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "afflicted\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for afflicted.\n",
      "Saved the count of sentences used to create afflicted embedding\n",
      "Run time for afflicted was 0.026596683999969173 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "aff\n",
      "ront\n",
      "ed\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for affronted.\n",
      "Saved the count of sentences used to create affronted embedding\n",
      "Run time for affronted was 0.02536300399992797 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "a\n",
      "fl\n",
      "utter\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for aflutter.\n",
      "Saved the count of sentences used to create aflutter embedding\n",
      "Run time for aflutter was 0.03637416400010807 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "afraid\n",
      "\n",
      "Instance 1 of afraid.\n",
      "Looking for vocab token: afraid\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "afraid at index 5: [-0.021128512918949127, -0.14597797393798828, 0.4306657314300537, 0.4270578622817993, 1.0973641872406006]\n",
      "Grand sum of 1 tensor sets is: [-0.021128512918949127, -0.14597797393798828, 0.4306657314300537, 0.4270578622817993, 1.0973641872406006]\n",
      "\n",
      "Instance 2 of afraid.\n",
      "Looking for vocab token: afraid\n",
      "Indices are [5, 13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "afraid at index 5: [-0.08513058722019196, -0.29803192615509033, 0.13244567811489105, 0.2129269391298294, 0.9306025505065918]\n",
      "afraid at index 13: [-0.11178436875343323, -0.33336079120635986, 0.18283161520957947, 0.18674224615097046, 1.1694145202636719]\n",
      "Grand sum of 2 tensor sets is: [-0.11958599090576172, -0.4616743326187134, 0.5883044004440308, 0.6268924474716187, 2.1473727226257324]\n",
      "Mean of tensors is: tensor([-0.0598, -0.2308,  0.2942,  0.3134,  1.0737]) (768 features in tensor)\n",
      "Saved the embedding for afraid.\n",
      "Saved the count of sentences used to create afraid embedding\n",
      "Run time for afraid was 0.1517995639999299 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ag\n",
      "ape\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for agape.\n",
      "Saved the count of sentences used to create agape embedding\n",
      "Run time for agape was 0.03584754099983911 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "aggravated\n",
      "\n",
      "Instance 1 of aggravated.\n",
      "Looking for vocab token: aggravated\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "aggravated at index 5: [-0.04036819189786911, 0.2066463679075241, 0.13810168206691742, 0.02779260277748108, 0.06575316190719604]\n",
      "Grand sum of 1 tensor sets is: [-0.04036819189786911, 0.2066463679075241, 0.13810168206691742, 0.02779260277748108, 0.06575316190719604]\n",
      "Mean of tensors is: tensor([-0.0404,  0.2066,  0.1381,  0.0278,  0.0658]) (768 features in tensor)\n",
      "Saved the embedding for aggravated.\n",
      "Saved the count of sentences used to create aggravated embedding\n",
      "Run time for aggravated was 0.08495805300003667 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "aggrav\n",
      "ation\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for aggravation.\n",
      "Saved the count of sentences used to create aggravation embedding\n",
      "Run time for aggravation was 0.03519026200001463 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "aggression\n",
      "\n",
      "Instance 1 of aggression.\n",
      "Looking for vocab token: aggression\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "aggression at index 6: [-0.002479780465364456, 0.1832132190465927, 0.02853044681251049, 0.0570811852812767, -0.36287781596183777]\n",
      "Grand sum of 1 tensor sets is: [-0.002479780465364456, 0.1832132190465927, 0.02853044681251049, 0.0570811852812767, -0.36287781596183777]\n",
      "Mean of tensors is: tensor([-0.0025,  0.1832,  0.0285,  0.0571, -0.3629]) (768 features in tensor)\n",
      "Saved the embedding for aggression.\n",
      "Saved the count of sentences used to create aggression embedding\n",
      "Run time for aggression was 0.08690192900007787 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "aggressive\n",
      "\n",
      "Instance 1 of aggressive.\n",
      "Looking for vocab token: aggressive\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "aggressive at index 21: [0.09531955420970917, -0.1685708612203598, -0.14524495601654053, 0.7110124230384827, -0.012212276458740234]\n",
      "Grand sum of 1 tensor sets is: [0.09531955420970917, -0.1685708612203598, -0.14524495601654053, 0.7110124230384827, -0.012212276458740234]\n",
      "\n",
      "Instance 2 of aggressive.\n",
      "Looking for vocab token: aggressive\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "aggressive at index 31: [0.1439601480960846, 0.08833093196153641, 0.06896568089723587, 0.20601718127727509, 0.21589598059654236]\n",
      "Grand sum of 2 tensor sets is: [0.23927970230579376, -0.0802399292588234, -0.07627927511930466, 0.917029619216919, 0.20368370413780212]\n",
      "\n",
      "Instance 3 of aggressive.\n",
      "Looking for vocab token: aggressive\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "aggressive at index 4: [0.07398677617311478, 0.07459574192762375, 0.0619354322552681, 0.48943108320236206, 0.09881015866994858]\n",
      "Grand sum of 3 tensor sets is: [0.31326648592948914, -0.005644187331199646, -0.01434384286403656, 1.4064607620239258, 0.3024938702583313]\n",
      "\n",
      "Instance 4 of aggressive.\n",
      "Looking for vocab token: aggressive\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "aggressive at index 4: [0.05862955003976822, 0.013850729912519455, -0.031211281195282936, 0.47626039385795593, -0.42292577028274536]\n",
      "Grand sum of 4 tensor sets is: [0.37189602851867676, 0.008206542581319809, -0.04555512219667435, 1.882721185684204, -0.12043190002441406]\n",
      "Mean of tensors is: tensor([ 0.0930,  0.0021, -0.0114,  0.4707, -0.0301]) (768 features in tensor)\n",
      "Saved the embedding for aggressive.\n",
      "Saved the count of sentences used to create aggressive embedding\n",
      "Run time for aggressive was 0.2733595160000277 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "agg\n",
      "rieve\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for aggrieve.\n",
      "Saved the count of sentences used to create aggrieve embedding\n",
      "Run time for aggrieve was 0.030698079000103462 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "agg\n",
      "rieved\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for aggrieved.\n",
      "Saved the count of sentences used to create aggrieved embedding\n",
      "Run time for aggrieved was 0.025144112000134555 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "a\n",
      "gh\n",
      "ast\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for aghast.\n",
      "Saved the count of sentences used to create aghast embedding\n",
      "Run time for aghast was 0.025732483999945543 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "agitated\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for agitated.\n",
      "Saved the count of sentences used to create agitated embedding\n",
      "Run time for agitated was 0.024165454000012687 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ag\n",
      "og\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for agog.\n",
      "Saved the count of sentences used to create agog embedding\n",
      "Run time for agog was 0.028895566000073813 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "agon\n",
      "ized\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for agonized.\n",
      "Saved the count of sentences used to create agonized embedding\n",
      "Run time for agonized was 0.02781303899996601 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "agreeable\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for agreeable.\n",
      "Saved the count of sentences used to create agreeable embedding\n",
      "Run time for agreeable was 0.028221860000030574 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ag\n",
      "ressive\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for agressive.\n",
      "Saved the count of sentences used to create agressive embedding\n",
      "Run time for agressive was 0.02825274899987562 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "air\n",
      "head\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for airhead.\n",
      "Saved the count of sentences used to create airhead embedding\n",
      "Run time for airhead was 0.027600687999893125 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "alarm\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for alarm.\n",
      "Saved the count of sentences used to create alarm embedding\n",
      "Run time for alarm was 0.02738212500003101 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "alarmed\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for alarmed.\n",
      "Saved the count of sentences used to create alarmed embedding\n",
      "Run time for alarmed was 0.027611985000021377 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "alarming\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for alarming.\n",
      "Saved the count of sentences used to create alarming embedding\n",
      "Run time for alarming was 0.027909849000025133 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "alert\n",
      "\n",
      "Instance 1 of alert.\n",
      "Looking for vocab token: alert\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "alert at index 6: [-0.02452503889799118, -0.30278444290161133, -0.05021539330482483, 0.30885303020477295, 0.12821315228939056]\n",
      "Grand sum of 1 tensor sets is: [-0.02452503889799118, -0.30278444290161133, -0.05021539330482483, 0.30885303020477295, 0.12821315228939056]\n",
      "\n",
      "Instance 2 of alert.\n",
      "Looking for vocab token: alert\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "alert at index 11: [-0.12474393844604492, 0.04805102199316025, -0.15804138779640198, -0.2873021066188812, 0.05259528011083603]\n",
      "Grand sum of 2 tensor sets is: [-0.1492689847946167, -0.2547334134578705, -0.2082567811012268, 0.021550923585891724, 0.180808424949646]\n",
      "Mean of tensors is: tensor([-0.0746, -0.1274, -0.1041,  0.0108,  0.0904]) (768 features in tensor)\n",
      "Saved the embedding for alert.\n",
      "Saved the count of sentences used to create alert embedding\n",
      "Run time for alert was 0.17741508099993553 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "alerted\n",
      "\n",
      "Instance 1 of alerted.\n",
      "Looking for vocab token: alerted\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "alerted at index 9: [0.06697985529899597, -0.03200601786375046, 0.046300001442432404, -0.027864914387464523, 0.2281385362148285]\n",
      "Grand sum of 1 tensor sets is: [0.06697985529899597, -0.03200601786375046, 0.046300001442432404, -0.027864914387464523, 0.2281385362148285]\n",
      "\n",
      "Instance 2 of alerted.\n",
      "Looking for vocab token: alerted\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "alerted at index 2: [0.2878829836845398, -0.1626962423324585, 0.041646890342235565, -0.04731352999806404, 0.6773348450660706]\n",
      "Grand sum of 2 tensor sets is: [0.35486283898353577, -0.19470226764678955, 0.08794689178466797, -0.07517844438552856, 0.9054733514785767]\n",
      "Mean of tensors is: tensor([ 0.1774, -0.0974,  0.0440, -0.0376,  0.4527]) (768 features in tensor)\n",
      "Saved the embedding for alerted.\n",
      "Saved the count of sentences used to create alerted embedding\n",
      "Run time for alerted was 0.19372406200000114 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "alienated\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for alienated.\n",
      "Saved the count of sentences used to create alienated embedding\n",
      "Run time for alienated was 0.025078938000206108 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "allergic\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for allergic.\n",
      "Saved the count of sentences used to create allergic embedding\n",
      "Run time for allergic was 0.026364236999825152 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "allev\n",
      "iated\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for alleviated.\n",
      "Saved the count of sentences used to create alleviated embedding\n",
      "Run time for alleviated was 0.02757507199999054 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "all\n",
      "uring\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for alluring.\n",
      "Saved the count of sentences used to create alluring embedding\n",
      "Run time for alluring was 0.028695588000118732 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "al\n",
      "oof\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for aloof.\n",
      "Saved the count of sentences used to create aloof embedding\n",
      "Run time for aloof was 0.02501015300003928 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "am\n",
      "atory\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for amatory.\n",
      "Saved the count of sentences used to create amatory embedding\n",
      "Run time for amatory was 0.02494546899993111 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "amazed\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for amazed.\n",
      "Saved the count of sentences used to create amazed embedding\n",
      "Run time for amazed was 0.02605075100018439 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "amaz\n",
      "ement\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for amazement.\n",
      "Saved the count of sentences used to create amazement embedding\n",
      "Run time for amazement was 0.02854718700018566 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "amazing\n",
      "\n",
      "Instance 1 of amazing.\n",
      "Looking for vocab token: amazing\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "amazing at index 9: [0.11996796727180481, 0.1440725028514862, 0.02760280855000019, -0.37193062901496887, 0.3257947564125061]\n",
      "Grand sum of 1 tensor sets is: [0.11996796727180481, 0.1440725028514862, 0.02760280855000019, -0.37193062901496887, 0.3257947564125061]\n",
      "\n",
      "Instance 2 of amazing.\n",
      "Looking for vocab token: amazing\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([68, 13, 768])\n",
      "Shape of summed layers is: 68 x 768\n",
      "amazing at index 40: [-0.008208934217691422, 0.3720661401748657, -0.14578741788864136, -0.4101196825504303, 0.34287261962890625]\n",
      "Grand sum of 2 tensor sets is: [0.11175903677940369, 0.5161386728286743, -0.11818461120128632, -0.7820503115653992, 0.6686673760414124]\n",
      "\n",
      "Instance 3 of amazing.\n",
      "Looking for vocab token: amazing\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "amazing at index 13: [-0.07336670905351639, 0.16041766107082367, -0.08369830250740051, -0.24965515732765198, -0.48866844177246094]\n",
      "Grand sum of 3 tensor sets is: [0.0383923277258873, 0.6765563488006592, -0.20188291370868683, -1.0317054986953735, 0.17999893426895142]\n",
      "Mean of tensors is: tensor([ 0.0128,  0.2255, -0.0673, -0.3439,  0.0600]) (768 features in tensor)\n",
      "Saved the embedding for amazing.\n",
      "Saved the count of sentences used to create amazing embedding\n",
      "Run time for amazing was 0.26790555000002314 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "ambition\n",
      "\n",
      "Instance 1 of ambition.\n",
      "Looking for vocab token: ambition\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "ambition at index 3: [-0.06092153489589691, -0.29118484258651733, 0.1267220824956894, 0.21915477514266968, -0.40109068155288696]\n",
      "Grand sum of 1 tensor sets is: [-0.06092153489589691, -0.29118484258651733, 0.1267220824956894, 0.21915477514266968, -0.40109068155288696]\n",
      "\n",
      "Instance 2 of ambition.\n",
      "Looking for vocab token: ambition\n",
      "Indices are [8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "ambition at index 8: [0.05712300166487694, -0.4759107828140259, 0.3213956952095032, 0.015220381319522858, -0.09168340265750885]\n",
      "Grand sum of 2 tensor sets is: [-0.0037985332310199738, -0.7670956254005432, 0.44811779260635376, 0.23437514901161194, -0.4927740693092346]\n",
      "\n",
      "Instance 3 of ambition.\n",
      "Looking for vocab token: ambition\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "ambition at index 7: [-0.003787245601415634, -0.1687094122171402, 0.19934116303920746, 0.19455231726169586, -0.7462382316589355]\n",
      "Grand sum of 3 tensor sets is: [-0.007585778832435608, -0.9358050227165222, 0.6474589705467224, 0.428927481174469, -1.2390122413635254]\n",
      "Mean of tensors is: tensor([-0.0025, -0.3119,  0.2158,  0.1430, -0.4130]) (768 features in tensor)\n",
      "Saved the embedding for ambition.\n",
      "Saved the count of sentences used to create ambition embedding\n",
      "Run time for ambition was 0.21724996900002225 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "ambitious\n",
      "\n",
      "Instance 1 of ambitious.\n",
      "Looking for vocab token: ambitious\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "ambitious at index 15: [0.17459988594055176, 0.08305463194847107, 0.2250860035419464, 0.15465351939201355, 0.2233530879020691]\n",
      "Grand sum of 1 tensor sets is: [0.17459988594055176, 0.08305463194847107, 0.2250860035419464, 0.15465351939201355, 0.2233530879020691]\n",
      "Mean of tensors is: tensor([0.1746, 0.0831, 0.2251, 0.1547, 0.2234]) (768 features in tensor)\n",
      "Saved the embedding for ambitious.\n",
      "Saved the count of sentences used to create ambitious embedding\n",
      "Run time for ambitious was 0.09917217000020173 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "amb\n",
      "ival\n",
      "ence\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for ambivalence.\n",
      "Saved the count of sentences used to create ambivalence embedding\n",
      "Run time for ambivalence was 0.02896002900001804 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "amb\n",
      "ivalent\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for ambivalent.\n",
      "Saved the count of sentences used to create ambivalent embedding\n",
      "Run time for ambivalent was 0.029538689999981216 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "am\n",
      "enable\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for amenable.\n",
      "Saved the count of sentences used to create amenable embedding\n",
      "Run time for amenable was 0.033780709999973624 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "am\n",
      "iable\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for amiable.\n",
      "Saved the count of sentences used to create amiable embedding\n",
      "Run time for amiable was 0.028373839000096268 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "am\n",
      "icable\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for amicable.\n",
      "Saved the count of sentences used to create amicable embedding\n",
      "Run time for amicable was 0.027268807999917044 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "amused\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for amused.\n",
      "Saved the count of sentences used to create amused embedding\n",
      "Run time for amused was 0.026241011000138315 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "amusement\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for amusement.\n",
      "Saved the count of sentences used to create amusement embedding\n",
      "Run time for amusement was 0.025630079999928057 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "analytical\n",
      "\n",
      "Instance 1 of analytical.\n",
      "Looking for vocab token: analytical\n",
      "Indices are [49]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "analytical at index 49: [0.15147754549980164, 0.125524640083313, 0.064389668405056, 0.21269172430038452, 0.22992467880249023]\n",
      "Grand sum of 1 tensor sets is: [0.15147754549980164, 0.125524640083313, 0.064389668405056, 0.21269172430038452, 0.22992467880249023]\n",
      "Mean of tensors is: tensor([0.1515, 0.1255, 0.0644, 0.2127, 0.2299]) (768 features in tensor)\n",
      "Saved the embedding for analytical.\n",
      "Saved the count of sentences used to create analytical embedding\n",
      "Run time for analytical was 0.12675268600014533 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "analyzing\n",
      "\n",
      "Instance 1 of analyzing.\n",
      "Looking for vocab token: analyzing\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "analyzing at index 7: [0.27910715341567993, 0.2255050092935562, 0.10954922437667847, 0.12491613626480103, 0.6000325679779053]\n",
      "Grand sum of 1 tensor sets is: [0.27910715341567993, 0.2255050092935562, 0.10954922437667847, 0.12491613626480103, 0.6000325679779053]\n",
      "\n",
      "Instance 2 of analyzing.\n",
      "Looking for vocab token: analyzing\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "analyzing at index 19: [0.17776751518249512, 0.25048187375068665, 0.07400267571210861, 0.2684782147407532, 0.7525385618209839]\n",
      "Grand sum of 2 tensor sets is: [0.45687466859817505, 0.47598689794540405, 0.18355190753936768, 0.3933943510055542, 1.3525711297988892]\n",
      "Mean of tensors is: tensor([0.2284, 0.2380, 0.0918, 0.1967, 0.6763]) (768 features in tensor)\n",
      "Saved the embedding for analyzing.\n",
      "Saved the count of sentences used to create analyzing embedding\n",
      "Run time for analyzing was 0.14437708600007682 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "anger\n",
      "\n",
      "Instance 1 of anger.\n",
      "Looking for vocab token: anger\n",
      "Indices are [48]\n",
      "Size of token embeddings is torch.Size([67, 13, 768])\n",
      "Shape of summed layers is: 67 x 768\n",
      "anger at index 48: [-0.025699160993099213, -0.08612427115440369, -0.04524426907300949, 0.3557019829750061, 0.3233538866043091]\n",
      "Grand sum of 1 tensor sets is: [-0.025699160993099213, -0.08612427115440369, -0.04524426907300949, 0.3557019829750061, 0.3233538866043091]\n",
      "\n",
      "Instance 2 of anger.\n",
      "Looking for vocab token: anger\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "anger at index 33: [-0.009449727833271027, -0.2567516565322876, 0.030517050996422768, 0.40600883960723877, -0.04193046689033508]\n",
      "Grand sum of 2 tensor sets is: [-0.03514888882637024, -0.3428759276866913, -0.014727218076586723, 0.7617108225822449, 0.281423419713974]\n",
      "\n",
      "Instance 3 of anger.\n",
      "Looking for vocab token: anger\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "anger at index 5: [0.0523526668548584, -0.056002259254455566, 0.045360639691352844, 0.47759631276130676, -0.4250553846359253]\n",
      "Grand sum of 3 tensor sets is: [0.01720377802848816, -0.39887818694114685, 0.03063342161476612, 1.239307165145874, -0.1436319649219513]\n",
      "\n",
      "Instance 4 of anger.\n",
      "Looking for vocab token: anger\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "anger at index 5: [0.055991336703300476, -0.23053960502147675, 0.006804395467042923, 0.22060292959213257, 0.44504138827323914]\n",
      "Grand sum of 4 tensor sets is: [0.07319511473178864, -0.6294177770614624, 0.03743781894445419, 1.4599101543426514, 0.30140942335128784]\n",
      "\n",
      "Instance 5 of anger.\n",
      "Looking for vocab token: anger\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "anger at index 32: [0.11743023246526718, -0.08298443257808685, 0.04215235635638237, 0.1773572415113449, 0.10809770226478577]\n",
      "Grand sum of 5 tensor sets is: [0.19062533974647522, -0.7124022245407104, 0.07959017157554626, 1.6372673511505127, 0.4095071256160736]\n",
      "\n",
      "Instance 6 of anger.\n",
      "Looking for vocab token: anger\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "anger at index 12: [0.1809227019548416, -0.1769171953201294, 0.10280376672744751, 0.189121812582016, -0.22888308763504028]\n",
      "Grand sum of 6 tensor sets is: [0.371548056602478, -0.8893194198608398, 0.18239393830299377, 1.826389193534851, 0.18062403798103333]\n",
      "\n",
      "Instance 7 of anger.\n",
      "Looking for vocab token: anger\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "anger at index 5: [0.003400493413209915, -0.24470971524715424, -0.0028918329626321793, 0.16916929185390472, 0.05786709487438202]\n",
      "Grand sum of 7 tensor sets is: [0.37494856119155884, -1.1340291500091553, 0.17950209975242615, 1.995558500289917, 0.23849113285541534]\n",
      "Mean of tensors is: tensor([ 0.0536, -0.1620,  0.0256,  0.2851,  0.0341]) (768 features in tensor)\n",
      "Saved the embedding for anger.\n",
      "Saved the count of sentences used to create anger embedding\n",
      "Run time for anger was 0.517072067999834 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "angered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for angered.\n",
      "Saved the count of sentences used to create angered embedding\n",
      "Run time for angered was 0.03172099100015657 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "angrily\n",
      "\n",
      "Instance 1 of angrily.\n",
      "Looking for vocab token: angrily\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "angrily at index 4: [-0.032908640801906586, 0.06928518414497375, -0.04905584454536438, 0.032097309827804565, 0.5097794532775879]\n",
      "Grand sum of 1 tensor sets is: [-0.032908640801906586, 0.06928518414497375, -0.04905584454536438, 0.032097309827804565, 0.5097794532775879]\n",
      "\n",
      "Instance 2 of angrily.\n",
      "Looking for vocab token: angrily\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "angrily at index 20: [0.03822272643446922, -0.06449541449546814, -0.0017080157995224, -0.18574970960617065, 0.696103572845459]\n",
      "Grand sum of 2 tensor sets is: [0.005314085632562637, 0.004789769649505615, -0.05076386034488678, -0.1536523997783661, 1.2058830261230469]\n",
      "Mean of tensors is: tensor([ 0.0027,  0.0024, -0.0254, -0.0768,  0.6029]) (768 features in tensor)\n",
      "Saved the embedding for angrily.\n",
      "Saved the count of sentences used to create angrily embedding\n",
      "Run time for angrily was 0.15520246099981705 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "angry\n",
      "\n",
      "Instance 1 of angry.\n",
      "Looking for vocab token: angry\n",
      "Indices are [59]\n",
      "Size of token embeddings is torch.Size([67, 13, 768])\n",
      "Shape of summed layers is: 67 x 768\n",
      "angry at index 59: [0.025556271895766258, 0.15035340189933777, 0.1268588900566101, 0.29824262857437134, 0.4213542342185974]\n",
      "Grand sum of 1 tensor sets is: [0.025556271895766258, 0.15035340189933777, 0.1268588900566101, 0.29824262857437134, 0.4213542342185974]\n",
      "\n",
      "Instance 2 of angry.\n",
      "Looking for vocab token: angry\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "angry at index 2: [0.10778298228979111, -0.0431809201836586, 0.14919762313365936, 0.0317004919052124, 0.7220667004585266]\n",
      "Grand sum of 2 tensor sets is: [0.13333925604820251, 0.10717248171567917, 0.27605652809143066, 0.32994312047958374, 1.143420934677124]\n",
      "\n",
      "Instance 3 of angry.\n",
      "Looking for vocab token: angry\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "angry at index 7: [0.11554709821939468, 0.027163542807102203, 0.17652654647827148, 0.26398488879203796, 0.30333977937698364]\n",
      "Grand sum of 3 tensor sets is: [0.2488863468170166, 0.13433602452278137, 0.45258307456970215, 0.5939279794692993, 1.446760654449463]\n",
      "\n",
      "Instance 4 of angry.\n",
      "Looking for vocab token: angry\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "angry at index 11: [0.07962578535079956, 0.06242655590176582, -0.013377612456679344, 0.05090974643826485, 0.5527651309967041]\n",
      "Grand sum of 4 tensor sets is: [0.32851213216781616, 0.1967625766992569, 0.43920546770095825, 0.6448377370834351, 1.999525785446167]\n",
      "\n",
      "Instance 5 of angry.\n",
      "Looking for vocab token: angry\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "angry at index 8: [0.17089807987213135, 0.10320709645748138, 0.3400115668773651, 0.21593666076660156, 0.08231763541698456]\n",
      "Grand sum of 5 tensor sets is: [0.4994102120399475, 0.2999696731567383, 0.779217004776001, 0.8607743978500366, 2.081843376159668]\n",
      "\n",
      "Instance 6 of angry.\n",
      "Looking for vocab token: angry\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "angry at index 11: [0.17164038121700287, -0.04835706204175949, 0.11896181106567383, 0.3060322403907776, 0.685426652431488]\n",
      "Grand sum of 6 tensor sets is: [0.6710506081581116, 0.2516126036643982, 0.8981788158416748, 1.166806697845459, 2.767270088195801]\n",
      "Mean of tensors is: tensor([0.1118, 0.0419, 0.1497, 0.1945, 0.4612]) (768 features in tensor)\n",
      "Saved the embedding for angry.\n",
      "Saved the count of sentences used to create angry embedding\n",
      "Run time for angry was 0.4289674700000887 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "angst\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for angst.\n",
      "Saved the count of sentences used to create angst embedding\n",
      "Run time for angst was 0.029312032999996518 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "anguish\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for anguish.\n",
      "Saved the count of sentences used to create anguish embedding\n",
      "Run time for anguish was 0.02673383000001195 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "ang\n",
      "u\n",
      "ished\n",
      "\n",
      "Instance 1 of anguished.\n",
      "Looking for vocab token: ang\n",
      "Looking for vocab token: u\n",
      "Looking for vocab token: ished\n",
      "Indices are [13, 14, 15]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "ang at index 13: [0.16204509139060974, -0.15306690335273743, 0.11153832823038101, 0.4389304220676422, -0.7705284357070923]\n",
      "u at index 14: [0.02222392149269581, 0.004106208682060242, 0.019230298697948456, 0.4784104526042938, -0.5122988820075989]\n",
      "ished at index 15: [0.10499301552772522, 0.1439545899629593, 0.1374567300081253, 0.4508945643901825, 1.1283552646636963]\n",
      "Grand sum of 1 tensor sets is: [0.09642067551612854, -0.0016687015304341912, 0.08940845727920532, 0.45607849955558777, -0.05149070546030998]\n",
      "\n",
      "Instance 2 of anguished.\n",
      "Looking for vocab token: ang\n",
      "Looking for vocab token: u\n",
      "Looking for vocab token: ished\n",
      "Indices are [13, 14, 15]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "ang at index 13: [0.058709532022476196, -0.11416856944561005, 0.1861008107662201, 0.2434139847755432, 0.43944957852363586]\n",
      "u at index 14: [0.0019599944353103638, -0.06468236446380615, 0.013993922621011734, 0.2814396321773529, 0.16379980742931366]\n",
      "ished at index 15: [0.09497621655464172, 0.24031168222427368, 0.05350429192185402, 0.5086886286735535, 0.9371973872184753]\n",
      "Grand sum of 2 tensor sets is: [0.1483025848865509, 0.018818214535713196, 0.1739414632320404, 0.8005925416946411, 0.4619915783405304]\n",
      "Mean of tensors is: tensor([0.0742, 0.0094, 0.0870, 0.4003, 0.2310]) (768 features in tensor)\n",
      "Saved the embedding for anguished.\n",
      "Saved the count of sentences used to create anguished embedding\n",
      "Run time for anguished was 0.15428778899990903 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "animated\n",
      "\n",
      "Instance 1 of animated.\n",
      "Looking for vocab token: animated\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "animated at index 9: [-0.13959121704101562, 0.1737341582775116, 0.024441640824079514, -0.11185142397880554, -0.11011719703674316]\n",
      "Grand sum of 1 tensor sets is: [-0.13959121704101562, 0.1737341582775116, 0.024441640824079514, -0.11185142397880554, -0.11011719703674316]\n",
      "\n",
      "Instance 2 of animated.\n",
      "Looking for vocab token: animated\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "animated at index 33: [-0.1319926530122757, 0.2044759839773178, 0.10342278331518173, -0.150216743350029, 0.12877792119979858]\n",
      "Grand sum of 2 tensor sets is: [-0.2715838551521301, 0.3782101273536682, 0.12786442041397095, -0.26206815242767334, 0.01866072416305542]\n",
      "\n",
      "Instance 3 of animated.\n",
      "Looking for vocab token: animated\n",
      "Indices are [44]\n",
      "Size of token embeddings is torch.Size([70, 13, 768])\n",
      "Shape of summed layers is: 70 x 768\n",
      "animated at index 44: [0.037575654685497284, 0.059396836906671524, 0.2357286512851715, 0.03179680183529854, -0.04932625591754913]\n",
      "Grand sum of 3 tensor sets is: [-0.23400819301605225, 0.43760696053504944, 0.36359307169914246, -0.2302713543176651, -0.030665531754493713]\n",
      "Mean of tensors is: tensor([-0.0780,  0.1459,  0.1212, -0.0768, -0.0102]) (768 features in tensor)\n",
      "Saved the embedding for animated.\n",
      "Saved the count of sentences used to create animated embedding\n",
      "Run time for animated was 0.25102436900010616 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "animosity\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for animosity.\n",
      "Saved the count of sentences used to create animosity embedding\n",
      "Run time for animosity was 0.02956125199989401 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "annoyance\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for annoyance.\n",
      "Saved the count of sentences used to create annoyance embedding\n",
      "Run time for annoyance was 0.026846535999993648 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "annoyed\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for annoyed.\n",
      "Saved the count of sentences used to create annoyed embedding\n",
      "Run time for annoyed was 0.027491059000112728 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "annoying\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 1 of annoying.\n",
      "Looking for vocab token: annoying\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "annoying at index 11: [0.10794611275196075, 0.3627992272377014, 0.12529073655605316, 0.10958881676197052, 0.2889607846736908]\n",
      "Grand sum of 1 tensor sets is: [0.10794611275196075, 0.3627992272377014, 0.12529073655605316, 0.10958881676197052, 0.2889607846736908]\n",
      "Mean of tensors is: tensor([0.1079, 0.3628, 0.1253, 0.1096, 0.2890]) (768 features in tensor)\n",
      "Saved the embedding for annoying.\n",
      "Saved the count of sentences used to create annoying embedding\n",
      "Run time for annoying was 0.07568186500020602 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "antagon\n",
      "istic\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for antagonistic.\n",
      "Saved the count of sentences used to create antagonistic embedding\n",
      "Run time for antagonistic was 0.032699912999987646 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "antagon\n",
      "ized\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for antagonized.\n",
      "Saved the count of sentences used to create antagonized embedding\n",
      "Run time for antagonized was 0.026415715999974054 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "anticipated\n",
      "\n",
      "Instance 1 of anticipated.\n",
      "Looking for vocab token: anticipated\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "anticipated at index 21: [-0.0014907903969287872, -0.12229287624359131, 0.07041046768426895, 0.026940083131194115, 0.6992916464805603]\n",
      "Grand sum of 1 tensor sets is: [-0.0014907903969287872, -0.12229287624359131, 0.07041046768426895, 0.026940083131194115, 0.6992916464805603]\n",
      "\n",
      "Instance 2 of anticipated.\n",
      "Looking for vocab token: anticipated\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "anticipated at index 21: [0.2141578495502472, 0.24588459730148315, 0.09696508944034576, 0.3095882833003998, -0.140160471200943]\n",
      "Grand sum of 2 tensor sets is: [0.2126670628786087, 0.12359172105789185, 0.1673755645751953, 0.33652836084365845, 0.5591311454772949]\n",
      "\n",
      "Instance 3 of anticipated.\n",
      "Looking for vocab token: anticipated\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "anticipated at index 14: [0.0822591632604599, 0.12259364873170853, -0.007730439305305481, -0.18174125254154205, -0.10468657314777374]\n",
      "Grand sum of 3 tensor sets is: [0.2949262261390686, 0.24618536233901978, 0.15964512526988983, 0.1547871083021164, 0.4544445872306824]\n",
      "\n",
      "Instance 4 of anticipated.\n",
      "Looking for vocab token: anticipated\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "anticipated at index 13: [0.12619692087173462, 0.03008950501680374, -0.01456332765519619, -0.2056160867214203, 0.40256017446517944]\n",
      "Grand sum of 4 tensor sets is: [0.4211231470108032, 0.2762748599052429, 0.1450818032026291, -0.050828978419303894, 0.8570047616958618]\n",
      "\n",
      "Instance 5 of anticipated.\n",
      "Looking for vocab token: anticipated\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "anticipated at index 3: [0.1059303879737854, -0.13581211864948273, 0.30717161297798157, 0.16465668380260468, 0.13923588395118713]\n",
      "Grand sum of 5 tensor sets is: [0.5270535349845886, 0.1404627412557602, 0.45225340127944946, 0.11382770538330078, 0.9962406158447266]\n",
      "\n",
      "Instance 6 of anticipated.\n",
      "Looking for vocab token: anticipated\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "anticipated at index 8: [-0.06317673623561859, 0.08525488525629044, -0.017199682071805, -0.006035104393959045, -0.4241142272949219]\n",
      "Grand sum of 6 tensor sets is: [0.46387678384780884, 0.22571763396263123, 0.4350537061691284, 0.10779260098934174, 0.5721263885498047]\n",
      "\n",
      "Instance 7 of anticipated.\n",
      "Looking for vocab token: anticipated\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "anticipated at index 10: [-0.04275298863649368, -0.03486596792936325, 0.26590293645858765, -0.16161994636058807, -0.0871654748916626]\n",
      "Grand sum of 7 tensor sets is: [0.42112380266189575, 0.19085165858268738, 0.7009566426277161, -0.05382734537124634, 0.4849609136581421]\n",
      "Mean of tensors is: tensor([ 0.0602,  0.0273,  0.1001, -0.0077,  0.0693]) (768 features in tensor)\n",
      "Saved the embedding for anticipated.\n",
      "Saved the count of sentences used to create anticipated embedding\n",
      "Run time for anticipated was 0.4237716370000726 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "anticipating\n",
      "\n",
      "Instance 1 of anticipating.\n",
      "Looking for vocab token: anticipating\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "anticipating at index 24: [0.10652883350849152, 0.1959572583436966, 0.07325226068496704, 0.15284724533557892, 0.7273950576782227]\n",
      "Grand sum of 1 tensor sets is: [0.10652883350849152, 0.1959572583436966, 0.07325226068496704, 0.15284724533557892, 0.7273950576782227]\n",
      "\n",
      "Instance 2 of anticipating.\n",
      "Looking for vocab token: anticipating\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "anticipating at index 29: [0.1753004789352417, 0.026505619287490845, -0.04865042865276337, 0.3018667995929718, 0.23644757270812988]\n",
      "Grand sum of 2 tensor sets is: [0.281829297542572, 0.22246287763118744, 0.024601832032203674, 0.4547140598297119, 0.9638426303863525]\n",
      "Mean of tensors is: tensor([0.1409, 0.1112, 0.0123, 0.2274, 0.4819]) (768 features in tensor)\n",
      "Saved the embedding for anticipating.\n",
      "Saved the count of sentences used to create anticipating embedding\n",
      "Run time for anticipating was 0.15004707599996436 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "anticipation\n",
      "\n",
      "Instance 1 of anticipation.\n",
      "Looking for vocab token: anticipation\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "anticipation at index 12: [0.1374605894088745, -0.046881601214408875, -0.11937476694583893, 0.06743603199720383, -0.5062006711959839]\n",
      "Grand sum of 1 tensor sets is: [0.1374605894088745, -0.046881601214408875, -0.11937476694583893, 0.06743603199720383, -0.5062006711959839]\n",
      "\n",
      "Instance 2 of anticipation.\n",
      "Looking for vocab token: anticipation\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "anticipation at index 14: [-0.03849043697118759, 0.01344374567270279, 0.2764108180999756, 0.0591999813914299, -0.20163637399673462]\n",
      "Grand sum of 2 tensor sets is: [0.09897015243768692, -0.033437855541706085, 0.15703605115413666, 0.12663601338863373, -0.7078370451927185]\n",
      "Mean of tensors is: tensor([ 0.0495, -0.0167,  0.0785,  0.0633, -0.3539]) (768 features in tensor)\n",
      "Saved the embedding for anticipation.\n",
      "Saved the count of sentences used to create anticipation embedding\n",
      "Run time for anticipation was 0.16242762899992158 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "anticip\n",
      "ative\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for anticipative.\n",
      "Saved the count of sentences used to create anticipative embedding\n",
      "Run time for anticipative was 0.02665428799991787 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "anticip\n",
      "atory\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for anticipatory.\n",
      "Saved the count of sentences used to create anticipatory embedding\n",
      "Run time for anticipatory was 0.026298356000097556 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "antip\n",
      "athy\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for antipathy.\n",
      "Saved the count of sentences used to create antipathy embedding\n",
      "Run time for antipathy was 0.029232532000150968 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ants\n",
      "y\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for antsy.\n",
      "Saved the count of sentences used to create antsy embedding\n",
      "Run time for antsy was 0.02712958200004323 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "anxiety\n",
      "\n",
      "Instance 1 of anxiety.\n",
      "Looking for vocab token: anxiety\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "anxiety at index 3: [0.16283810138702393, -0.16517367959022522, 0.15017762780189514, 0.06005309894680977, -0.16442930698394775]\n",
      "Grand sum of 1 tensor sets is: [0.16283810138702393, -0.16517367959022522, 0.15017762780189514, 0.06005309894680977, -0.16442930698394775]\n",
      "\n",
      "Instance 2 of anxiety.\n",
      "Looking for vocab token: anxiety\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "anxiety at index 36: [0.035467375069856644, 0.31055915355682373, 0.4142957627773285, -0.11021118611097336, 0.41283679008483887]\n",
      "Grand sum of 2 tensor sets is: [0.19830547273159027, 0.1453854739665985, 0.5644733905792236, -0.05015808716416359, 0.2484074831008911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of tensors is: tensor([ 0.0992,  0.0727,  0.2822, -0.0251,  0.1242]) (768 features in tensor)\n",
      "Saved the embedding for anxiety.\n",
      "Saved the count of sentences used to create anxiety embedding\n",
      "Run time for anxiety was 0.15094343700002355 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "anxious\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for anxious.\n",
      "Saved the count of sentences used to create anxious embedding\n",
      "Run time for anxious was 0.0297782999998617 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "anx\n",
      "iously\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for anxiously.\n",
      "Saved the count of sentences used to create anxiously embedding\n",
      "Run time for anxiously was 0.026087830000051326 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ap\n",
      "athetic\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for apathetic.\n",
      "Saved the count of sentences used to create apathetic embedding\n",
      "Run time for apathetic was 0.025803064999990966 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ap\n",
      "athy\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for apathy.\n",
      "Saved the count of sentences used to create apathy embedding\n",
      "Run time for apathy was 0.02715367599989804 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "apolog\n",
      "etic\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for apologetic.\n",
      "Saved the count of sentences used to create apologetic embedding\n",
      "Run time for apologetic was 0.02751784499992027 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "appalled\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for appalled.\n",
      "Saved the count of sentences used to create appalled embedding\n",
      "Run time for appalled was 0.02587612699994679 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "app\n",
      "all\n",
      "ingly\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for appallingly.\n",
      "Saved the count of sentences used to create appallingly embedding\n",
      "Run time for appallingly was 0.02384463700013839 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "appe\n",
      "ased\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for appeased.\n",
      "Saved the count of sentences used to create appeased embedding\n",
      "Run time for appeased was 0.02539410500003214 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "appe\n",
      "asing\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for appeasing.\n",
      "Saved the count of sentences used to create appeasing embedding\n",
      "Run time for appeasing was 0.026843367000083163 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "appreci\n",
      "ative\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for appreciative.\n",
      "Saved the count of sentences used to create appreciative embedding\n",
      "Run time for appreciative was 0.025738381000110166 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "apprehension\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for apprehension.\n",
      "Saved the count of sentences used to create apprehension embedding\n",
      "Run time for apprehension was 0.024351416999934372 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "apprehens\n",
      "ive\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for apprehensive.\n",
      "Saved the count of sentences used to create apprehensive embedding\n",
      "Run time for apprehensive was 0.02441132499984633 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "approve\n",
      "\n",
      "Instance 1 of approve.\n",
      "Looking for vocab token: approve\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "approve at index 28: [0.01732192561030388, 0.4021766781806946, -0.062369994819164276, -0.055501632392406464, -0.5963248014450073]\n",
      "Grand sum of 1 tensor sets is: [0.01732192561030388, 0.4021766781806946, -0.062369994819164276, -0.055501632392406464, -0.5963248014450073]\n",
      "Mean of tensors is: tensor([ 0.0173,  0.4022, -0.0624, -0.0555, -0.5963]) (768 features in tensor)\n",
      "Saved the embedding for approve.\n",
      "Saved the count of sentences used to create approve embedding\n",
      "Run time for approve was 0.11147055199990064 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "approved\n",
      "\n",
      "Instance 1 of approved.\n",
      "Looking for vocab token: approved\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "approved at index 24: [0.14637750387191772, 0.007387086749076843, 0.10743226855993271, 0.3400673270225525, -0.5126274228096008]\n",
      "Grand sum of 1 tensor sets is: [0.14637750387191772, 0.007387086749076843, 0.10743226855993271, 0.3400673270225525, -0.5126274228096008]\n",
      "\n",
      "Instance 2 of approved.\n",
      "Looking for vocab token: approved\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "approved at index 4: [-0.0877799391746521, 0.030479885637760162, -0.04717652499675751, 0.5669403076171875, -0.0033235102891921997]\n",
      "Grand sum of 2 tensor sets is: [0.058597564697265625, 0.037866972386837006, 0.0602557435631752, 0.90700763463974, -0.5159509181976318]\n",
      "\n",
      "Instance 3 of approved.\n",
      "Looking for vocab token: approved\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([60, 13, 768])\n",
      "Shape of summed layers is: 60 x 768\n",
      "approved at index 38: [-0.09312461316585541, 0.3032228350639343, -0.07409601658582687, 0.5928658246994019, 0.05345563590526581]\n",
      "Grand sum of 3 tensor sets is: [-0.03452704846858978, 0.34108981490135193, -0.013840273022651672, 1.499873399734497, -0.46249526739120483]\n",
      "\n",
      "Instance 4 of approved.\n",
      "Looking for vocab token: approved\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "approved at index 31: [0.08798334002494812, 0.18263523280620575, 0.06846115738153458, 0.32792046666145325, -0.187473326921463]\n",
      "Grand sum of 4 tensor sets is: [0.05345629155635834, 0.5237250328063965, 0.054620884358882904, 1.827793836593628, -0.6499686241149902]\n",
      "\n",
      "Instance 5 of approved.\n",
      "Looking for vocab token: approved\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "approved at index 5: [0.053949564695358276, 0.3826545476913452, -0.10893817245960236, 0.27703729271888733, -0.42938655614852905]\n",
      "Grand sum of 5 tensor sets is: [0.10740585625171661, 0.9063795804977417, -0.05431728810071945, 2.1048312187194824, -1.079355239868164]\n",
      "\n",
      "Instance 6 of approved.\n",
      "Looking for vocab token: approved\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "approved at index 5: [-0.08021722733974457, 0.059619173407554626, 0.04709672927856445, 0.4496241509914398, -0.25031498074531555]\n",
      "Grand sum of 6 tensor sets is: [0.027188628911972046, 0.9659987688064575, -0.007220558822154999, 2.554455280303955, -1.3296701908111572]\n",
      "Mean of tensors is: tensor([ 0.0045,  0.1610, -0.0012,  0.4257, -0.2216]) (768 features in tensor)\n",
      "Saved the embedding for approved.\n",
      "Saved the count of sentences used to create approved embedding\n",
      "Run time for approved was 0.38236521899989384 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "approving\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for approving.\n",
      "Saved the count of sentences used to create approving embedding\n",
      "Run time for approving was 0.031243435000078534 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "argue\n",
      "\n",
      "Instance 1 of argue.\n",
      "Looking for vocab token: argue\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "argue at index 7: [0.06884217262268066, 0.20509298145771027, 0.1924692839384079, 0.09119677543640137, -1.6444611549377441]\n",
      "Grand sum of 1 tensor sets is: [0.06884217262268066, 0.20509298145771027, 0.1924692839384079, 0.09119677543640137, -1.6444611549377441]\n",
      "\n",
      "Instance 2 of argue.\n",
      "Looking for vocab token: argue\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "argue at index 21: [-0.02402881532907486, 0.11744076013565063, 0.17665696144104004, 0.09680195152759552, -1.0717964172363281]\n",
      "Grand sum of 2 tensor sets is: [0.044813357293605804, 0.3225337266921997, 0.36912626028060913, 0.1879987269639969, -2.7162575721740723]\n",
      "\n",
      "Instance 3 of argue.\n",
      "Looking for vocab token: argue\n",
      "Indices are [6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "argue at index 6: [0.1798577755689621, 0.13027141988277435, 0.09999781847000122, 0.09113790094852448, -1.0571374893188477]\n",
      "Grand sum of 3 tensor sets is: [0.2246711254119873, 0.45280516147613525, 0.46912407875061035, 0.27913662791252136, -3.77339506149292]\n",
      "Mean of tensors is: tensor([ 0.0749,  0.1509,  0.1564,  0.0930, -1.2578]) (768 features in tensor)\n",
      "Saved the embedding for argue.\n",
      "Saved the count of sentences used to create argue embedding\n",
      "Run time for argue was 0.21144664999997076 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "argument\n",
      "ative\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for argumentative.\n",
      "Saved the count of sentences used to create argumentative embedding\n",
      "Run time for argumentative was 0.027810067000018535 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "aroused\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for aroused.\n",
      "Saved the count of sentences used to create aroused embedding\n",
      "Run time for aroused was 0.02486165200002688 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "arrogance\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for arrogance.\n",
      "Saved the count of sentences used to create arrogance embedding\n",
      "Run time for arrogance was 0.03193555000007109 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "arrogant\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for arrogant.\n",
      "Saved the count of sentences used to create arrogant embedding\n",
      "Run time for arrogant was 0.035574733000203196 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "arrog\n",
      "antly\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for arrogantly.\n",
      "Saved the count of sentences used to create arrogantly embedding\n",
      "Run time for arrogantly was 0.027652441000100225 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "artificial\n",
      "\n",
      "Instance 1 of artificial.\n",
      "Looking for vocab token: artificial\n",
      "Indices are [35]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "artificial at index 35: [0.07960290461778641, 0.034764569252729416, 0.10795908421278, -0.22299298644065857, 0.5744372606277466]\n",
      "Grand sum of 1 tensor sets is: [0.07960290461778641, 0.034764569252729416, 0.10795908421278, -0.22299298644065857, 0.5744372606277466]\n",
      "\n",
      "Instance 2 of artificial.\n",
      "Looking for vocab token: artificial\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "artificial at index 4: [0.08108662068843842, 0.3269450068473816, 0.11364691704511642, -0.15487828850746155, 0.7841124534606934]\n",
      "Grand sum of 2 tensor sets is: [0.16068953275680542, 0.3617095649242401, 0.22160600125789642, -0.3778712749481201, 1.35854971408844]\n",
      "Mean of tensors is: tensor([ 0.0803,  0.1809,  0.1108, -0.1889,  0.6793]) (768 features in tensor)\n",
      "Saved the embedding for artificial.\n",
      "Saved the count of sentences used to create artificial embedding\n",
      "Run time for artificial was 0.1514537250000103 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "ashamed\n",
      "\n",
      "Instance 1 of ashamed.\n",
      "Looking for vocab token: ashamed\n",
      "Indices are [66]\n",
      "Size of token embeddings is torch.Size([69, 13, 768])\n",
      "Shape of summed layers is: 69 x 768\n",
      "ashamed at index 66: [-0.36356493830680847, -0.25257885456085205, -0.1825815737247467, 0.00846082903444767, -0.2561294436454773]\n",
      "Grand sum of 1 tensor sets is: [-0.36356493830680847, -0.25257885456085205, -0.1825815737247467, 0.00846082903444767, -0.2561294436454773]\n",
      "\n",
      "Instance 2 of ashamed.\n",
      "Looking for vocab token: ashamed\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "ashamed at index 6: [0.013952415436506271, -0.13214603066444397, -2.1062791347503662e-05, -0.2353881299495697, 0.5075865983963013]\n",
      "Grand sum of 2 tensor sets is: [-0.3496125340461731, -0.384724885225296, -0.1826026439666748, -0.22692729532718658, 0.251457154750824]\n",
      "Mean of tensors is: tensor([-0.1748, -0.1924, -0.0913, -0.1135,  0.1257]) (768 features in tensor)\n",
      "Saved the embedding for ashamed.\n",
      "Saved the count of sentences used to create ashamed embedding\n",
      "Run time for ashamed was 0.1822516690001521 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "aspiring\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for aspiring.\n",
      "Saved the count of sentences used to create aspiring embedding\n",
      "Run time for aspiring was 0.028313931999946362 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "assert\n",
      "ive\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for assertive.\n",
      "Saved the count of sentences used to create assertive embedding\n",
      "Run time for assertive was 0.024832057000139685 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "assert\n",
      "ively\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for assertively.\n",
      "Saved the count of sentences used to create assertively embedding\n",
      "Run time for assertively was 0.028497427000047537 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "assessing\n",
      "\n",
      "Instance 1 of assessing.\n",
      "Looking for vocab token: assessing\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([72, 13, 768])\n",
      "Shape of summed layers is: 72 x 768\n",
      "assessing at index 2: [0.12681639194488525, 0.19607220590114594, -0.2301521897315979, 0.08991909772157669, 1.0132317543029785]\n",
      "Grand sum of 1 tensor sets is: [0.12681639194488525, 0.19607220590114594, -0.2301521897315979, 0.08991909772157669, 1.0132317543029785]\n",
      "Mean of tensors is: tensor([ 0.1268,  0.1961, -0.2302,  0.0899,  1.0132]) (768 features in tensor)\n",
      "Saved the embedding for assessing.\n",
      "Saved the count of sentences used to create assessing embedding\n",
      "Run time for assessing was 0.12990193199993882 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "assured\n",
      "\n",
      "Instance 1 of assured.\n",
      "Looking for vocab token: assured\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "assured at index 15: [0.06265281140804291, -0.2211502343416214, 0.024464672431349754, -0.18672098219394684, 0.8194430470466614]\n",
      "Grand sum of 1 tensor sets is: [0.06265281140804291, -0.2211502343416214, 0.024464672431349754, -0.18672098219394684, 0.8194430470466614]\n",
      "Mean of tensors is: tensor([ 0.0627, -0.2212,  0.0245, -0.1867,  0.8194]) (768 features in tensor)\n",
      "Saved the embedding for assured.\n",
      "Saved the count of sentences used to create assured embedding\n",
      "Run time for assured was 0.07732057299995176 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "astonished\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for astonished.\n",
      "Saved the count of sentences used to create astonished embedding\n",
      "Run time for astonished was 0.031360227999812196 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "aston\n",
      "ishment\n",
      "\n",
      "Instance 1 of astonishment.\n",
      "Looking for vocab token: aston\n",
      "Looking for vocab token: ishment\n",
      "Indices are [14, 15]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "aston at index 14: [0.17947961390018463, -0.04122838377952576, 0.2270057201385498, 0.2485000342130661, -0.8990238308906555]\n",
      "ishment at index 15: [-0.006226882338523865, -0.0732608288526535, -0.1315445899963379, 0.3091508448123932, 0.02471931278705597]\n",
      "Grand sum of 1 tensor sets is: [0.08662636578083038, -0.05724460631608963, 0.04773056507110596, 0.27882543206214905, -0.43715226650238037]\n",
      "\n",
      "Instance 2 of astonishment.\n",
      "Looking for vocab token: aston\n",
      "Looking for vocab token: ishment\n",
      "Indices are [23, 24]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "aston at index 23: [0.026200132444500923, 0.2283799648284912, 0.17866410315036774, 0.4976412057876587, -1.1627042293548584]\n",
      "ishment at index 24: [-0.013910967856645584, 0.15450137853622437, -0.13057099282741547, 0.5317883491516113, 0.20326219499111176]\n",
      "Grand sum of 2 tensor sets is: [0.09277094900608063, 0.13419607281684875, 0.07177712023258209, 0.7935402393341064, -0.9168732762336731]\n",
      "Mean of tensors is: tensor([ 0.0464,  0.0671,  0.0359,  0.3968, -0.4584]) (768 features in tensor)\n",
      "Saved the embedding for astonishment.\n",
      "Saved the count of sentences used to create astonishment embedding\n",
      "Run time for astonishment was 0.13828575899992757 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ast\n",
      "ounded\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for astounded.\n",
      "Saved the count of sentences used to create astounded embedding\n",
      "Run time for astounded was 0.028417586999921696 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "attempting\n",
      "\n",
      "Instance 1 of attempting.\n",
      "Looking for vocab token: attempting\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "attempting at index 2: [0.268297404050827, 0.11452382802963257, 0.2384369969367981, 0.08031939715147018, 0.20495353639125824]\n",
      "Grand sum of 1 tensor sets is: [0.268297404050827, 0.11452382802963257, 0.2384369969367981, 0.08031939715147018, 0.20495353639125824]\n",
      "\n",
      "Instance 2 of attempting.\n",
      "Looking for vocab token: attempting\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "attempting at index 13: [0.2750488817691803, 0.06962272524833679, 0.11182277649641037, 0.23127178847789764, 0.03039490431547165]\n",
      "Grand sum of 2 tensor sets is: [0.5433462858200073, 0.18414655327796936, 0.35025978088378906, 0.31159117817878723, 0.2353484332561493]\n",
      "\n",
      "Instance 3 of attempting.\n",
      "Looking for vocab token: attempting\n",
      "Indices are [11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "attempting at index 11: [-0.004661872982978821, -0.012229569256305695, 0.20318102836608887, 0.1792888194322586, 0.581027626991272]\n",
      "Grand sum of 3 tensor sets is: [0.5386844277381897, 0.17191699147224426, 0.5534408092498779, 0.49088001251220703, 0.8163760900497437]\n",
      "\n",
      "Instance 4 of attempting.\n",
      "Looking for vocab token: attempting\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "attempting at index 27: [0.11186040937900543, 0.13188433647155762, 0.07992716133594513, 0.07227587699890137, 0.47849926352500916]\n",
      "Grand sum of 4 tensor sets is: [0.6505448222160339, 0.3038013279438019, 0.6333679556846619, 0.5631558895111084, 1.2948753833770752]\n",
      "\n",
      "Instance 5 of attempting.\n",
      "Looking for vocab token: attempting\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "attempting at index 15: [0.2207575887441635, 0.07597984373569489, 0.10291691869497299, 0.04342744126915932, 0.2601925730705261]\n",
      "Grand sum of 5 tensor sets is: [0.8713024258613586, 0.37978118658065796, 0.7362848520278931, 0.6065833568572998, 1.555068016052246]\n",
      "\n",
      "Instance 6 of attempting.\n",
      "Looking for vocab token: attempting\n",
      "Indices are [68]\n",
      "Size of token embeddings is torch.Size([76, 13, 768])\n",
      "Shape of summed layers is: 76 x 768\n",
      "attempting at index 68: [0.15483221411705017, 0.19438421726226807, 0.07633611559867859, -0.1123928651213646, 0.7156228423118591]\n",
      "Grand sum of 6 tensor sets is: [1.0261346101760864, 0.574165403842926, 0.812620997428894, 0.4941904842853546, 2.27069091796875]\n",
      "\n",
      "Instance 7 of attempting.\n",
      "Looking for vocab token: attempting\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "attempting at index 17: [0.000399664044380188, 0.04867810383439064, 0.10045544058084488, 0.12199667096138, 0.29201218485832214]\n",
      "Grand sum of 7 tensor sets is: [1.0265343189239502, 0.6228435039520264, 0.9130764603614807, 0.6161871552467346, 2.5627031326293945]\n",
      "\n",
      "Instance 8 of attempting.\n",
      "Looking for vocab token: attempting\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "attempting at index 2: [0.13421282172203064, 0.13369691371917725, 0.13294987380504608, -0.07670453935861588, 0.3058012127876282]\n",
      "Grand sum of 8 tensor sets is: [1.1607471704483032, 0.7565404176712036, 1.046026349067688, 0.539482593536377, 2.868504285812378]\n",
      "\n",
      "Instance 9 of attempting.\n",
      "Looking for vocab token: attempting\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "attempting at index 10: [0.12288235872983932, 0.3090379238128662, 0.1295420229434967, 0.1721075475215912, 0.24271804094314575]\n",
      "Grand sum of 9 tensor sets is: [1.2836295366287231, 1.0655783414840698, 1.1755683422088623, 0.7115901708602905, 3.111222267150879]\n",
      "\n",
      "Instance 10 of attempting.\n",
      "Looking for vocab token: attempting\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "attempting at index 37: [-0.002199701964855194, 0.16664117574691772, 0.11146022379398346, 0.02229103073477745, 0.06106967478990555]\n",
      "Grand sum of 10 tensor sets is: [1.2814298868179321, 1.2322194576263428, 1.2870285511016846, 0.7338811755180359, 3.1722919940948486]\n",
      "Mean of tensors is: tensor([0.1281, 0.1232, 0.1287, 0.0734, 0.3172]) (768 features in tensor)\n",
      "Saved the embedding for attempting.\n",
      "Saved the count of sentences used to create attempting embedding\n",
      "Run time for attempting was 0.6466610840000158 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "attentive\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for attentive.\n",
      "Saved the count of sentences used to create attentive embedding\n",
      "Run time for attentive was 0.02815361800003302 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "attent\n",
      "iveness\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for attentiveness.\n",
      "Saved the count of sentences used to create attentiveness embedding\n",
      "Run time for attentiveness was 0.025241389000029812 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "attracted\n",
      "\n",
      "Instance 1 of attracted.\n",
      "Looking for vocab token: attracted\n",
      "Indices are [42]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "attracted at index 42: [0.054477714002132416, -0.0397087037563324, -0.116180419921875, 0.16626279056072235, 0.4719522297382355]\n",
      "Grand sum of 1 tensor sets is: [0.054477714002132416, -0.0397087037563324, -0.116180419921875, 0.16626279056072235, 0.4719522297382355]\n",
      "\n",
      "Instance 2 of attracted.\n",
      "Looking for vocab token: attracted\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "attracted at index 17: [0.045469559729099274, 0.09940913319587708, -0.018614551052451134, 0.2462480068206787, -0.012619927525520325]\n",
      "Grand sum of 2 tensor sets is: [0.09994727373123169, 0.05970042943954468, -0.13479496538639069, 0.41251081228256226, 0.45933228731155396]\n",
      "\n",
      "Instance 3 of attracted.\n",
      "Looking for vocab token: attracted\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "attracted at index 3: [0.2868107855319977, 0.043074529618024826, 0.037776365876197815, 0.07543650269508362, -0.056826233863830566]\n",
      "Grand sum of 3 tensor sets is: [0.38675805926322937, 0.1027749627828598, -0.09701859951019287, 0.4879473149776459, 0.4025060534477234]\n",
      "\n",
      "Instance 4 of attracted.\n",
      "Looking for vocab token: attracted\n",
      "Indices are [44]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "attracted at index 44: [0.335921972990036, 0.11685992777347565, 0.09310296922922134, -0.04570561647415161, 0.06674838811159134]\n",
      "Grand sum of 4 tensor sets is: [0.7226800322532654, 0.21963489055633545, -0.003915630280971527, 0.44224169850349426, 0.46925443410873413]\n",
      "\n",
      "Instance 5 of attracted.\n",
      "Looking for vocab token: attracted\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "attracted at index 3: [0.21531406044960022, 0.11360040307044983, -0.05642981827259064, -0.06978600472211838, 0.17136552929878235]\n",
      "Grand sum of 5 tensor sets is: [0.937994122505188, 0.3332352936267853, -0.060345448553562164, 0.3724556863307953, 0.6406199932098389]\n",
      "\n",
      "Instance 6 of attracted.\n",
      "Looking for vocab token: attracted\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "attracted at index 7: [0.12156382203102112, -0.3263832926750183, 0.053324032574892044, -0.032811351120471954, -0.8589804172515869]\n",
      "Grand sum of 6 tensor sets is: [1.0595579147338867, 0.006852000951766968, -0.00702141597867012, 0.33964434266090393, -0.21836042404174805]\n",
      "\n",
      "Instance 7 of attracted.\n",
      "Looking for vocab token: attracted\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "attracted at index 6: [0.3230738937854767, 0.021551162004470825, -0.16743266582489014, 0.23112425208091736, -0.023702040314674377]\n",
      "Grand sum of 7 tensor sets is: [1.382631778717041, 0.028403162956237793, -0.17445407807826996, 0.5707685947418213, -0.24206246435642242]\n",
      "\n",
      "Instance 8 of attracted.\n",
      "Looking for vocab token: attracted\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "attracted at index 6: [0.10536332428455353, -0.06269694864749908, -0.06131836771965027, 0.3149847984313965, 0.4225046634674072]\n",
      "Grand sum of 8 tensor sets is: [1.4879951477050781, -0.03429378569126129, -0.23577244579792023, 0.8857533931732178, 0.1804421991109848]\n",
      "Mean of tensors is: tensor([ 0.1860, -0.0043, -0.0295,  0.1107,  0.0226]) (768 features in tensor)\n",
      "Saved the embedding for attracted.\n",
      "Saved the count of sentences used to create attracted embedding\n",
      "Run time for attracted was 0.5106773540001086 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "aven\n",
      "ging\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for avenging.\n",
      "Saved the count of sentences used to create avenging embedding\n",
      "Run time for avenging was 0.029860515000109444 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "a\n",
      "verse\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for averse.\n",
      "Saved the count of sentences used to create averse embedding\n",
      "Run time for averse was 0.025254043000131787 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "aversion\n",
      "\n",
      "Instance 1 of aversion.\n",
      "Looking for vocab token: aversion\n",
      "Indices are [23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "aversion at index 23: [0.08816449344158173, 0.0816800445318222, 0.1390906423330307, 0.12265820056200027, -0.12753167748451233]\n",
      "Grand sum of 1 tensor sets is: [0.08816449344158173, 0.0816800445318222, 0.1390906423330307, 0.12265820056200027, -0.12753167748451233]\n",
      "Mean of tensors is: tensor([ 0.0882,  0.0817,  0.1391,  0.1227, -0.1275]) (768 features in tensor)\n",
      "Saved the embedding for aversion.\n",
      "Saved the count of sentences used to create aversion embedding\n",
      "Run time for aversion was 0.08849502299995038 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "a\n",
      "versive\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for aversive.\n",
      "Saved the count of sentences used to create aversive embedding\n",
      "Run time for aversive was 0.028951457999937702 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "avid\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for avid.\n",
      "Saved the count of sentences used to create avid embedding\n",
      "Run time for avid was 0.030056118000175047 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "avoiding\n",
      "\n",
      "Instance 1 of avoiding.\n",
      "Looking for vocab token: avoiding\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "avoiding at index 28: [0.2064252644777298, -0.0027953237295150757, -0.09289681166410446, -0.17962346971035004, 0.4430348873138428]\n",
      "Grand sum of 1 tensor sets is: [0.2064252644777298, -0.0027953237295150757, -0.09289681166410446, -0.17962346971035004, 0.4430348873138428]\n",
      "\n",
      "Instance 2 of avoiding.\n",
      "Looking for vocab token: avoiding\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "avoiding at index 16: [0.35135895013809204, 0.3332180976867676, 0.05663122236728668, -0.2134239226579666, 0.5697475671768188]\n",
      "Grand sum of 2 tensor sets is: [0.5577841997146606, 0.3304227590560913, -0.03626558929681778, -0.39304739236831665, 1.0127824544906616]\n",
      "Mean of tensors is: tensor([ 0.2789,  0.1652, -0.0181, -0.1965,  0.5064]) (768 features in tensor)\n",
      "Saved the embedding for avoiding.\n",
      "Saved the count of sentences used to create avoiding embedding\n",
      "Run time for avoiding was 0.1860313249999308 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "awaiting\n",
      "\n",
      "Instance 1 of awaiting.\n",
      "Looking for vocab token: awaiting\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "awaiting at index 2: [0.09370365738868713, 0.315437912940979, 0.05047857016324997, 0.2967710494995117, 0.6203904747962952]\n",
      "Grand sum of 1 tensor sets is: [0.09370365738868713, 0.315437912940979, 0.05047857016324997, 0.2967710494995117, 0.6203904747962952]\n",
      "\n",
      "Instance 2 of awaiting.\n",
      "Looking for vocab token: awaiting\n",
      "Indices are [39]\n",
      "Size of token embeddings is torch.Size([61, 13, 768])\n",
      "Shape of summed layers is: 61 x 768\n",
      "awaiting at index 39: [0.13929256796836853, 0.1864623725414276, 0.048998650163412094, 0.3123489022254944, 0.2744264006614685]\n",
      "Grand sum of 2 tensor sets is: [0.23299622535705566, 0.501900315284729, 0.09947721660137177, 0.6091199517250061, 0.8948168754577637]\n",
      "\n",
      "Instance 3 of awaiting.\n",
      "Looking for vocab token: awaiting\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "awaiting at index 18: [-0.09810610115528107, 0.28537309169769287, -0.0058890450745821, 0.46089544892311096, 0.2718583047389984]\n",
      "Grand sum of 3 tensor sets is: [0.1348901242017746, 0.7872734069824219, 0.09358817338943481, 1.0700154304504395, 1.1666752099990845]\n",
      "Mean of tensors is: tensor([0.0450, 0.2624, 0.0312, 0.3567, 0.3889]) (768 features in tensor)\n",
      "Saved the embedding for awaiting.\n",
      "Saved the count of sentences used to create awaiting embedding\n",
      "Run time for awaiting was 0.24555826699997851 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "awakened\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for awakened.\n",
      "Saved the count of sentences used to create awakened embedding\n",
      "Run time for awakened was 0.026990220000016052 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "aware\n",
      "\n",
      "Instance 1 of aware.\n",
      "Looking for vocab token: aware\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "aware at index 12: [0.06320196390151978, 0.01043689250946045, -0.12627558410167694, 0.1915077269077301, 0.5086623430252075]\n",
      "Grand sum of 1 tensor sets is: [0.06320196390151978, 0.01043689250946045, -0.12627558410167694, 0.1915077269077301, 0.5086623430252075]\n",
      "\n",
      "Instance 2 of aware.\n",
      "Looking for vocab token: aware\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "aware at index 9: [0.09955587983131409, -0.28351521492004395, -0.09196937829256058, -0.057791613042354584, -0.2638980746269226]\n",
      "Grand sum of 2 tensor sets is: [0.16275784373283386, -0.2730783224105835, -0.21824496984481812, 0.13371610641479492, 0.2447642683982849]\n",
      "\n",
      "Instance 3 of aware.\n",
      "Looking for vocab token: aware\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "aware at index 7: [0.04753277078270912, -0.1793387532234192, -0.07096859812736511, 0.000461921445094049, -0.19554373621940613]\n",
      "Grand sum of 3 tensor sets is: [0.21029061079025269, -0.4524170756340027, -0.2892135679721832, 0.134178027510643, 0.049220532178878784]\n",
      "\n",
      "Instance 4 of aware.\n",
      "Looking for vocab token: aware\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "aware at index 29: [0.2820882201194763, 0.1717272698879242, 0.015065348707139492, 0.1655404269695282, 0.20110273361206055]\n",
      "Grand sum of 4 tensor sets is: [0.492378830909729, -0.2806898057460785, -0.27414822578430176, 0.29971843957901, 0.25032326579093933]\n",
      "\n",
      "Instance 5 of aware.\n",
      "Looking for vocab token: aware\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "aware at index 4: [0.30891552567481995, -0.13771463930606842, -0.15519794821739197, 0.09748797863721848, 0.39901819825172424]\n",
      "Grand sum of 5 tensor sets is: [0.8012943267822266, -0.4184044599533081, -0.4293461740016937, 0.3972064256668091, 0.6493414640426636]\n",
      "\n",
      "Instance 6 of aware.\n",
      "Looking for vocab token: aware\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([72, 13, 768])\n",
      "Shape of summed layers is: 72 x 768\n",
      "aware at index 31: [0.2632296085357666, -0.287253737449646, -0.31184181571006775, -0.1163700744509697, 0.5469584465026855]\n",
      "Grand sum of 6 tensor sets is: [1.0645239353179932, -0.7056581974029541, -0.7411879897117615, 0.2808363437652588, 1.1962999105453491]\n",
      "\n",
      "Instance 7 of aware.\n",
      "Looking for vocab token: aware\n",
      "Indices are [44]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "aware at index 44: [0.3832920491695404, 0.030457448214292526, -0.029434071853756905, 0.4989261031150818, -0.5551372766494751]\n",
      "Grand sum of 7 tensor sets is: [1.447816014289856, -0.6752007603645325, -0.7706220746040344, 0.7797624468803406, 0.641162633895874]\n",
      "\n",
      "Instance 8 of aware.\n",
      "Looking for vocab token: aware\n",
      "\n",
      "Instance 9 of aware.\n",
      "Looking for vocab token: aware\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "aware at index 30: [0.25534212589263916, 0.2867928743362427, -0.1632080078125, 0.2671853005886078, 0.2932778596878052]\n",
      "Grand sum of 8 tensor sets is: [1.7031581401824951, -0.3884078860282898, -0.9338300824165344, 1.046947717666626, 0.9344404935836792]\n",
      "Mean of tensors is: tensor([ 0.2129, -0.0486, -0.1167,  0.1309,  0.1168]) (768 features in tensor)\n",
      "Saved the embedding for aware.\n",
      "Saved the count of sentences used to create aware embedding\n",
      "Run time for aware was 0.5576091890000043 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "awareness\n",
      "\n",
      "Instance 1 of awareness.\n",
      "Looking for vocab token: awareness\n",
      "\n",
      "Instance 2 of awareness.\n",
      "Looking for vocab token: awareness\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "awareness at index 8: [-0.2500919997692108, -0.0290285125374794, 0.11069278419017792, 0.09179925918579102, -0.02760794758796692]\n",
      "Grand sum of 1 tensor sets is: [-0.2500919997692108, -0.0290285125374794, 0.11069278419017792, 0.09179925918579102, -0.02760794758796692]\n",
      "Mean of tensors is: tensor([-0.2501, -0.0290,  0.1107,  0.0918, -0.0276]) (768 features in tensor)\n",
      "Saved the embedding for awareness.\n",
      "Saved the count of sentences used to create awareness embedding\n",
      "Run time for awareness was 0.08167844700005844 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "awe\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for awe.\n",
      "Saved the count of sentences used to create awe embedding\n",
      "Run time for awe was 0.029794437999953516 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "aw\n",
      "ed\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for awed.\n",
      "Saved the count of sentences used to create awed embedding\n",
      "Run time for awed was 0.026755150000099093 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "aw\n",
      "est\n",
      "ruck\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for awestruck.\n",
      "Saved the count of sentences used to create awestruck embedding\n",
      "Run time for awestruck was 0.027733165999961784 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "awful\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for awful.\n",
      "Saved the count of sentences used to create awful embedding\n",
      "Run time for awful was 0.032478116999982376 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "awkward\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for awkward.\n",
      "Saved the count of sentences used to create awkward embedding\n",
      "Run time for awkward was 0.027426341999898796 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "awkward\n",
      "ness\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for awkwardness.\n",
      "Saved the count of sentences used to create awkwardness embedding\n",
      "Run time for awkwardness was 0.025869841000030647 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ax\n",
      "ed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for axed.\n",
      "Saved the count of sentences used to create axed embedding\n",
      "Run time for axed was 0.028360937999877933 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "back\n",
      "handed\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for backhanded.\n",
      "Saved the count of sentences used to create backhanded embedding\n",
      "Run time for backhanded was 0.02738111700000445 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "badly\n",
      "\n",
      "Instance 1 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "badly at index 3: [-0.04932598024606705, 0.20432643592357635, 0.14524024724960327, 0.2621002197265625, 1.0760900974273682]\n",
      "Grand sum of 1 tensor sets is: [-0.04932598024606705, 0.20432643592357635, 0.14524024724960327, 0.2621002197265625, 1.0760900974273682]\n",
      "\n",
      "Instance 2 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "badly at index 7: [-0.14191602170467377, 0.14972223341464996, 0.05356631055474281, 0.3544180989265442, 0.6540783643722534]\n",
      "Grand sum of 2 tensor sets is: [-0.1912420094013214, 0.3540486693382263, 0.1988065540790558, 0.6165183186531067, 1.7301684617996216]\n",
      "\n",
      "Instance 3 of badly.\n",
      "Looking for vocab token: badly\n",
      "\n",
      "Instance 4 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([61, 13, 768])\n",
      "Shape of summed layers is: 61 x 768\n",
      "badly at index 36: [0.17451748251914978, 0.006813496351242065, 0.08840584754943848, -0.4612594544887543, 1.023608684539795]\n",
      "Grand sum of 3 tensor sets is: [-0.01672452688217163, 0.3608621656894684, 0.28721240162849426, 0.15525886416435242, 2.753777027130127]\n",
      "\n",
      "Instance 5 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "badly at index 27: [-0.02822815626859665, 0.13040566444396973, 0.0401155911386013, -0.31503725051879883, 0.9988101124763489]\n",
      "Grand sum of 4 tensor sets is: [-0.04495268315076828, 0.4912678301334381, 0.32732799649238586, -0.1597783863544464, 3.752587080001831]\n",
      "\n",
      "Instance 6 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "badly at index 23: [0.026280522346496582, 0.2800995111465454, 0.04932811111211777, -0.2359740436077118, 1.8147668838500977]\n",
      "Grand sum of 5 tensor sets is: [-0.018672160804271698, 0.7713673114776611, 0.37665611505508423, -0.3957524299621582, 5.567354202270508]\n",
      "\n",
      "Instance 7 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "badly at index 10: [0.09102112054824829, 0.26622334122657776, 0.04872165992856026, -0.06009083613753319, 1.7007012367248535]\n",
      "Grand sum of 6 tensor sets is: [0.07234895974397659, 1.0375906229019165, 0.4253777861595154, -0.4558432698249817, 7.268055438995361]\n",
      "\n",
      "Instance 8 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "badly at index 26: [0.08634793758392334, 0.21357418596744537, 0.07354362308979034, -0.09781914204359055, 1.094259262084961]\n",
      "Grand sum of 7 tensor sets is: [0.15869688987731934, 1.2511647939682007, 0.49892139434814453, -0.5536624193191528, 8.362314224243164]\n",
      "\n",
      "Instance 9 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "badly at index 2: [0.004996135830879211, 0.17452892661094666, 0.19005540013313293, 0.08308257162570953, 1.730299949645996]\n",
      "Grand sum of 8 tensor sets is: [0.16369302570819855, 1.4256937503814697, 0.6889767646789551, -0.4705798625946045, 10.09261417388916]\n",
      "\n",
      "Instance 10 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "badly at index 4: [0.06123572587966919, 0.16714262962341309, 0.08532989770174026, 0.26651373505592346, 1.234102725982666]\n",
      "Grand sum of 9 tensor sets is: [0.22492875158786774, 1.5928363800048828, 0.7743066549301147, -0.20406612753868103, 11.326717376708984]\n",
      "\n",
      "Instance 11 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "badly at index 25: [0.005772687494754791, 0.2867071330547333, 0.0031701549887657166, 0.2649258077144623, 1.2053329944610596]\n",
      "Grand sum of 10 tensor sets is: [0.23070144653320312, 1.8795435428619385, 0.7774767875671387, 0.06085968017578125, 12.532050132751465]\n",
      "\n",
      "Instance 12 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "badly at index 10: [0.04940548911690712, 0.2936899662017822, 0.12275620549917221, 0.4555037319660187, 1.0187597274780273]\n",
      "Grand sum of 11 tensor sets is: [0.28010693192481995, 2.1732335090637207, 0.9002329707145691, 0.5163633823394775, 13.550809860229492]\n",
      "\n",
      "Instance 13 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "badly at index 4: [0.0024076253175735474, 0.16427108645439148, 0.05563846975564957, 0.1699882596731186, 1.3828911781311035]\n",
      "Grand sum of 12 tensor sets is: [0.2825145721435547, 2.3375046253204346, 0.9558714628219604, 0.6863516569137573, 14.933700561523438]\n",
      "\n",
      "Instance 14 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [4, 10]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "badly at index 4: [-0.03750115633010864, -0.02017318457365036, 0.14083419740200043, -0.2616604268550873, 1.1273969411849976]\n",
      "badly at index 10: [0.01732657477259636, 0.06904539465904236, 0.16496728360652924, -0.18441979587078094, 1.0890846252441406]\n",
      "Grand sum of 13 tensor sets is: [0.2724272906780243, 2.361940622329712, 1.1087721586227417, 0.4633115530014038, 16.041940689086914]\n",
      "\n",
      "Instance 15 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "badly at index 4: [0.025189783424139023, 0.3244208097457886, -0.005362933501601219, -0.07355137169361115, 0.8606026768684387]\n",
      "Grand sum of 14 tensor sets is: [0.2976170778274536, 2.686361312866211, 1.1034091711044312, 0.38976019620895386, 16.902544021606445]\n",
      "\n",
      "Instance 16 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "badly at index 11: [-0.083777517080307, 0.31061992049217224, 0.12527114152908325, 0.0837322548031807, 1.392991542816162]\n",
      "Grand sum of 15 tensor sets is: [0.2138395607471466, 2.996981143951416, 1.2286803722381592, 0.47349244356155396, 18.295536041259766]\n",
      "Mean of tensors is: tensor([0.0143, 0.1998, 0.0819, 0.0316, 1.2197]) (768 features in tensor)\n",
      "Saved the embedding for badly.\n",
      "Saved the count of sentences used to create badly embedding\n",
      "Run time for badly was 0.8290645579998 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "baff\n",
      "le\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for baffle.\n",
      "Saved the count of sentences used to create baffle embedding\n",
      "Run time for baffle was 0.034220870000126524 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "baffled\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for baffled.\n",
      "Saved the count of sentences used to create baffled embedding\n",
      "Run time for baffled was 0.028001382999946145 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "baff\n",
      "ling\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for baffling.\n",
      "Saved the count of sentences used to create baffling embedding\n",
      "Run time for baffling was 0.029219163999869124 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "baked\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for baked.\n",
      "Saved the count of sentences used to create baked embedding\n",
      "Run time for baked was 0.027237022999997862 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ban\n",
      "al\n",
      "\n",
      "Instance 1 of banal.\n",
      "Looking for vocab token: ban\n",
      "Looking for vocab token: al\n",
      "Indices are [29, 30]\n",
      "Size of token embeddings is torch.Size([74, 13, 768])\n",
      "Shape of summed layers is: 74 x 768\n",
      "ban at index 29: [0.37564027309417725, 0.8007642030715942, 0.19039347767829895, 0.27616459131240845, -0.15786463022232056]\n",
      "al at index 30: [-0.1630508452653885, 0.42835676670074463, 0.037747547030448914, 0.03732722997665405, 1.1688928604125977]\n",
      "Grand sum of 1 tensor sets is: [0.10629471391439438, 0.6145604848861694, 0.11407051235437393, 0.15674591064453125, 0.5055141448974609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of tensors is: tensor([0.1063, 0.6146, 0.1141, 0.1567, 0.5055]) (768 features in tensor)\n",
      "Saved the embedding for banal.\n",
      "Saved the count of sentences used to create banal embedding\n",
      "Run time for banal was 0.1496934419999434 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "barking\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for barking.\n",
      "Saved the count of sentences used to create barking embedding\n",
      "Run time for barking was 0.03098377299988897 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bash\n",
      "ful\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bashful.\n",
      "Saved the count of sentences used to create bashful embedding\n",
      "Run time for bashful was 0.027605446000052325 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "be\n",
      "aming\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for beaming.\n",
      "Saved the count of sentences used to create beaming embedding\n",
      "Run time for beaming was 0.027661388000069564 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bear\n",
      "ish\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bearish.\n",
      "Saved the count of sentences used to create bearish embedding\n",
      "Run time for bearish was 0.027570115999878908 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "beat\n",
      "\n",
      "Instance 1 of beat.\n",
      "Looking for vocab token: beat\n",
      "\n",
      "Instance 2 of beat.\n",
      "Looking for vocab token: beat\n",
      "\n",
      "Instance 3 of beat.\n",
      "Looking for vocab token: beat\n",
      "\n",
      "Instance 4 of beat.\n",
      "Looking for vocab token: beat\n",
      "\n",
      "Instance 5 of beat.\n",
      "Looking for vocab token: beat\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "beat at index 4: [0.18232806026935577, -0.28301751613616943, -0.23293912410736084, 0.048649244010448456, -0.488630473613739]\n",
      "Grand sum of 1 tensor sets is: [0.18232806026935577, -0.28301751613616943, -0.23293912410736084, 0.048649244010448456, -0.488630473613739]\n",
      "\n",
      "Instance 6 of beat.\n",
      "Looking for vocab token: beat\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "beat at index 12: [0.18745851516723633, -0.06715984642505646, 0.08181329071521759, -0.5155776739120483, -0.7835223078727722]\n",
      "Grand sum of 2 tensor sets is: [0.3697865605354309, -0.3501773476600647, -0.15112583339214325, -0.4669284224510193, -1.2721527814865112]\n",
      "\n",
      "Instance 7 of beat.\n",
      "Looking for vocab token: beat\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "beat at index 18: [0.18999524414539337, -0.04766558110713959, -0.07772503793239594, -0.35929590463638306, -0.7232933044433594]\n",
      "Grand sum of 3 tensor sets is: [0.5597817897796631, -0.3978429436683655, -0.22885087132453918, -0.8262243270874023, -1.9954460859298706]\n",
      "\n",
      "Instance 8 of beat.\n",
      "Looking for vocab token: beat\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "beat at index 4: [0.22316795587539673, -0.15126532316207886, -0.03902377933263779, -0.278055340051651, -1.4533255100250244]\n",
      "Grand sum of 4 tensor sets is: [0.7829497456550598, -0.5491082668304443, -0.26787465810775757, -1.104279637336731, -3.4487714767456055]\n",
      "\n",
      "Instance 9 of beat.\n",
      "Looking for vocab token: beat\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "beat at index 3: [0.1869942992925644, 0.09900295734405518, -0.1742640882730484, -0.4189644753932953, -0.06966196000576019]\n",
      "Grand sum of 5 tensor sets is: [0.9699440598487854, -0.45010530948638916, -0.4421387314796448, -1.5232441425323486, -3.5184333324432373]\n",
      "\n",
      "Instance 10 of beat.\n",
      "Looking for vocab token: beat\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "beat at index 14: [0.1090816855430603, 0.06753363460302353, -0.49266180396080017, -0.3019265830516815, -0.13274100422859192]\n",
      "Grand sum of 6 tensor sets is: [1.0790257453918457, -0.38257166743278503, -0.9348005056381226, -1.8251707553863525, -3.651174306869507]\n",
      "\n",
      "Instance 11 of beat.\n",
      "Looking for vocab token: beat\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "beat at index 2: [0.018019884824752808, -0.022769883275032043, -0.3281351625919342, -0.2826218605041504, 0.15953904390335083]\n",
      "Grand sum of 7 tensor sets is: [1.097045660018921, -0.40534156560897827, -1.2629356384277344, -2.107792615890503, -3.491635322570801]\n",
      "\n",
      "Instance 12 of beat.\n",
      "Looking for vocab token: beat\n",
      "Indices are [19, 33]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "beat at index 19: [0.24958230555057526, 0.16294661164283752, -0.22214184701442719, -0.2210734784603119, -1.1659855842590332]\n",
      "beat at index 33: [0.24658863246440887, 0.15531721711158752, -0.25946545600891113, -0.4825374186038971, -1.278578758239746]\n",
      "Grand sum of 8 tensor sets is: [1.3451311588287354, -0.24620965123176575, -1.503739356994629, -2.4595980644226074, -4.7139177322387695]\n",
      "\n",
      "Instance 13 of beat.\n",
      "Looking for vocab token: beat\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "beat at index 22: [0.24204668402671814, -0.010289318859577179, -0.17888998985290527, -0.3407951295375824, -0.6598373651504517]\n",
      "Grand sum of 9 tensor sets is: [1.5871778726577759, -0.25649896264076233, -1.6826293468475342, -2.8003931045532227, -5.373754978179932]\n",
      "\n",
      "Instance 14 of beat.\n",
      "Looking for vocab token: beat\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "beat at index 11: [0.010382764041423798, 0.03301608934998512, -0.02156142331659794, -0.2697214186191559, -0.8073258399963379]\n",
      "Grand sum of 10 tensor sets is: [1.5975606441497803, -0.2234828770160675, -1.704190731048584, -3.0701146125793457, -6.1810808181762695]\n",
      "\n",
      "Instance 15 of beat.\n",
      "Looking for vocab token: beat\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "beat at index 14: [0.1288866400718689, -0.10030436515808105, 0.05099240690469742, -0.17634311318397522, -0.7278710603713989]\n",
      "Grand sum of 11 tensor sets is: [1.726447343826294, -0.32378724217414856, -1.6531983613967896, -3.246457815170288, -6.908951759338379]\n",
      "\n",
      "Instance 16 of beat.\n",
      "Looking for vocab token: beat\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "beat at index 14: [0.15994703769683838, -0.1538230925798416, -0.1925855129957199, 0.05143578723073006, -0.3928861618041992]\n",
      "Grand sum of 12 tensor sets is: [1.8863943815231323, -0.47761034965515137, -1.8457838296890259, -3.1950221061706543, -7.301837921142578]\n",
      "\n",
      "Instance 17 of beat.\n",
      "Looking for vocab token: beat\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "beat at index 24: [0.06507625430822372, -0.10517197847366333, 0.033558398485183716, -0.29775190353393555, -0.6926261782646179]\n",
      "Grand sum of 13 tensor sets is: [1.9514706134796143, -0.5827823281288147, -1.8122254610061646, -3.49277400970459, -7.994463920593262]\n",
      "\n",
      "Instance 18 of beat.\n",
      "Looking for vocab token: beat\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "beat at index 13: [0.03614579886198044, -0.13984794914722443, 0.06919234246015549, -0.2266642451286316, -0.6214209198951721]\n",
      "Grand sum of 14 tensor sets is: [1.9876164197921753, -0.7226302623748779, -1.7430331707000732, -3.719438314437866, -8.615884780883789]\n",
      "Mean of tensors is: tensor([ 0.1420, -0.0516, -0.1245, -0.2657, -0.6154]) (768 features in tensor)\n",
      "Saved the embedding for beat.\n",
      "Saved the count of sentences used to create beat embedding\n",
      "Run time for beat was 0.8545939870000439 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "beaten\n",
      "\n",
      "Instance 1 of beaten.\n",
      "Looking for vocab token: beaten\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "beaten at index 4: [-0.13692939281463623, -0.1129075288772583, -0.14185190200805664, -0.39873701333999634, -0.4848898649215698]\n",
      "Grand sum of 1 tensor sets is: [-0.13692939281463623, -0.1129075288772583, -0.14185190200805664, -0.39873701333999634, -0.4848898649215698]\n",
      "Mean of tensors is: tensor([-0.1369, -0.1129, -0.1419, -0.3987, -0.4849]) (768 features in tensor)\n",
      "Saved the embedding for beaten.\n",
      "Saved the count of sentences used to create beaten embedding\n",
      "Run time for beaten was 0.08615004099988255 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "bed\n",
      "ev\n",
      "iled\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bedeviled.\n",
      "Saved the count of sentences used to create bedeviled embedding\n",
      "Run time for bedeviled was 0.027159478999919884 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "be\n",
      "f\n",
      "uddled\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for befuddled.\n",
      "Saved the count of sentences used to create befuddled embedding\n",
      "Run time for befuddled was 0.026980937000189442 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "begging\n",
      "\n",
      "Instance 1 of begging.\n",
      "Looking for vocab token: begging\n",
      "Indices are [16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "begging at index 16: [-0.084084153175354, 0.19248074293136597, 0.0530625618994236, 0.0920068770647049, 1.6260147094726562]\n",
      "Grand sum of 1 tensor sets is: [-0.084084153175354, 0.19248074293136597, 0.0530625618994236, 0.0920068770647049, 1.6260147094726562]\n",
      "\n",
      "Instance 2 of begging.\n",
      "Looking for vocab token: begging\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "begging at index 4: [0.22923703491687775, -0.2411428540945053, -0.01716305874288082, 0.3000563979148865, 1.1454803943634033]\n",
      "Grand sum of 2 tensor sets is: [0.14515288174152374, -0.04866211116313934, 0.03589950501918793, 0.3920632600784302, 2.7714951038360596]\n",
      "Mean of tensors is: tensor([ 0.0726, -0.0243,  0.0179,  0.1960,  1.3857]) (768 features in tensor)\n",
      "Saved the embedding for begging.\n",
      "Saved the count of sentences used to create begging embedding\n",
      "Run time for begging was 0.1479367879999245 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "be\n",
      "gr\n",
      "udge\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for begrudge.\n",
      "Saved the count of sentences used to create begrudge embedding\n",
      "Run time for begrudge was 0.026557324999885168 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "be\n",
      "gr\n",
      "udging\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for begrudging.\n",
      "Saved the count of sentences used to create begrudging embedding\n",
      "Run time for begrudging was 0.029130246000022453 seconds.\n",
      "\n",
      "There are 4 tokens in tokenized vocabulary word:\n",
      "be\n",
      "gr\n",
      "udging\n",
      "ly\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for begrudgingly.\n",
      "Saved the count of sentences used to create begrudgingly embedding\n",
      "Run time for begrudgingly was 0.027508928000088417 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "beg\n",
      "u\n",
      "iled\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for beguiled.\n",
      "Saved the count of sentences used to create beguiled embedding\n",
      "Run time for beguiled was 0.03049988299994766 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bel\n",
      "ated\n",
      "\n",
      "Instance 1 of belated.\n",
      "Looking for vocab token: bel\n",
      "Looking for vocab token: ated\n",
      "Indices are [14, 15]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "bel at index 14: [0.021198946982622147, 0.04176769778132439, 0.026334745809435844, -0.025293787941336632, -0.19791942834854126]\n",
      "ated at index 15: [-0.07539894431829453, 0.08971923589706421, 0.05585380643606186, 0.25272104144096375, 0.45447394251823425]\n",
      "Grand sum of 1 tensor sets is: [-0.02709999866783619, 0.06574346870183945, 0.041094277054071426, 0.11371362954378128, 0.1282772570848465]\n",
      "Mean of tensors is: tensor([-0.0271,  0.0657,  0.0411,  0.1137,  0.1283]) (768 features in tensor)\n",
      "Saved the embedding for belated.\n",
      "Saved the count of sentences used to create belated embedding\n",
      "Run time for belated was 0.07256722700003593 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "bel\n",
      "itt\n",
      "ling\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for belittling.\n",
      "Saved the count of sentences used to create belittling embedding\n",
      "Run time for belittling was 0.034161716000198794 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bellig\n",
      "erence\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for belligerence.\n",
      "Saved the count of sentences used to create belligerence embedding\n",
      "Run time for belligerence was 0.027955196999982945 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bellig\n",
      "erent\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for belligerent.\n",
      "Saved the count of sentences used to create belligerent embedding\n",
      "Run time for belligerent was 0.028840956000067308 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "belonging\n",
      "\n",
      "Instance 1 of belonging.\n",
      "Looking for vocab token: belonging\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "belonging at index 20: [0.24389567971229553, 0.03324664384126663, -0.3973850607872009, 0.2695547938346863, 0.9879419803619385]\n",
      "Grand sum of 1 tensor sets is: [0.24389567971229553, 0.03324664384126663, -0.3973850607872009, 0.2695547938346863, 0.9879419803619385]\n",
      "\n",
      "Instance 2 of belonging.\n",
      "Looking for vocab token: belonging\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "belonging at index 3: [0.2925761938095093, 0.08313213288784027, -0.14643928408622742, 0.09320096671581268, 0.427541583776474]\n",
      "Grand sum of 2 tensor sets is: [0.5364718437194824, 0.1163787767291069, -0.543824315071106, 0.36275577545166016, 1.4154835939407349]\n",
      "Mean of tensors is: tensor([ 0.2682,  0.0582, -0.2719,  0.1814,  0.7077]) (768 features in tensor)\n",
      "Saved the embedding for belonging.\n",
      "Saved the count of sentences used to create belonging embedding\n",
      "Run time for belonging was 0.15919235400019716 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "be\n",
      "m\n",
      "used\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bemused.\n",
      "Saved the count of sentences used to create bemused embedding\n",
      "Run time for bemused was 0.02661434099991311 seconds.\n",
      "\n",
      "There are 4 tokens in tokenized vocabulary word:\n",
      "be\n",
      "m\n",
      "use\n",
      "ment\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bemusement.\n",
      "Saved the count of sentences used to create bemusement embedding\n",
      "Run time for bemusement was 0.026833867000050304 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "bene\n",
      "vol\n",
      "ence\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for benevolence.\n",
      "Saved the count of sentences used to create benevolence embedding\n",
      "Run time for benevolence was 0.031143866000093112 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "benevolent\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for benevolent.\n",
      "Saved the count of sentences used to create benevolent embedding\n",
      "Run time for benevolent was 0.029305189999831782 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "ben\n",
      "umb\n",
      "ed\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for benumbed.\n",
      "Saved the count of sentences used to create benumbed embedding\n",
      "Run time for benumbed was 0.02769924400013224 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ber\n",
      "ate\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for berate.\n",
      "Saved the count of sentences used to create berate embedding\n",
      "Run time for berate was 0.02471430200012037 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ber\n",
      "ating\n",
      "\n",
      "Instance 1 of berating.\n",
      "Looking for vocab token: ber\n",
      "Looking for vocab token: ating\n",
      "Indices are [11, 12]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "ber at index 11: [-0.05139554291963577, 0.11074674129486084, 0.012256844900548458, 0.41428881883621216, -0.5741423964500427]\n",
      "ating at index 12: [-0.008366528898477554, 0.17898835241794586, 0.08633004873991013, 0.8704319000244141, 0.8281299471855164]\n",
      "Grand sum of 1 tensor sets is: [-0.029881035909056664, 0.14486753940582275, 0.04929344728589058, 0.6423603296279907, 0.12699377536773682]\n",
      "Mean of tensors is: tensor([-0.0299,  0.1449,  0.0493,  0.6424,  0.1270]) (768 features in tensor)\n",
      "Saved the embedding for berating.\n",
      "Saved the count of sentences used to create berating embedding\n",
      "Run time for berating was 0.08535443800019493 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bere\n",
      "aved\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bereaved.\n",
      "Saved the count of sentences used to create bereaved embedding\n",
      "Run time for bereaved was 0.029139831000065897 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bere\n",
      "ft\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bereft.\n",
      "Saved the count of sentences used to create bereft embedding\n",
      "Run time for bereft was 0.029005861000086952 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "bes\n",
      "ee\n",
      "ching\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for beseeching.\n",
      "Saved the count of sentences used to create beseeching embedding\n",
      "Run time for beseeching was 0.027808585000002495 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "best\n",
      "ed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bested.\n",
      "Saved the count of sentences used to create bested embedding\n",
      "Run time for bested was 0.031252151999979105 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "betrayal\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for betrayal.\n",
      "Saved the count of sentences used to create betrayal embedding\n",
      "Run time for betrayal was 0.02735987699998077 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "betrayed\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for betrayed.\n",
      "Saved the count of sentences used to create betrayed embedding\n",
      "Run time for betrayed was 0.028704961000130425 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bewild\n",
      "ered\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bewildered.\n",
      "Saved the count of sentences used to create bewildered embedding\n",
      "Run time for bewildered was 0.02775903500014465 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "bewild\n",
      "er\n",
      "ment\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bewilderment.\n",
      "Saved the count of sentences used to create bewilderment embedding\n",
      "Run time for bewilderment was 0.024707339000087813 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bi\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bi.\n",
      "Saved the count of sentences used to create bi embedding\n",
      "Run time for bi was 0.024889119999897957 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bil\n",
      "ious\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bilious.\n",
      "Saved the count of sentences used to create bilious embedding\n",
      "Run time for bilious was 0.024965659999907075 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bit\n",
      "\n",
      "Instance 1 of bit.\n",
      "Looking for vocab token: bit\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "bit at index 22: [0.21086853742599487, 0.5196075439453125, 0.3275662958621979, 0.3908795118331909, 1.1037216186523438]\n",
      "Grand sum of 1 tensor sets is: [0.21086853742599487, 0.5196075439453125, 0.3275662958621979, 0.3908795118331909, 1.1037216186523438]\n",
      "\n",
      "Instance 2 of bit.\n",
      "Looking for vocab token: bit\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "bit at index 12: [0.24724599719047546, 0.42722558975219727, 0.332744836807251, 0.42216357588768005, 0.9602119326591492]\n",
      "Grand sum of 2 tensor sets is: [0.45811453461647034, 0.9468331336975098, 0.6603111028671265, 0.8130431175231934, 2.0639336109161377]\n",
      "\n",
      "Instance 3 of bit.\n",
      "Looking for vocab token: bit\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "bit at index 30: [0.017276005819439888, 0.2702726125717163, 0.29617807269096375, 0.4095306396484375, 0.7606801390647888]\n",
      "Grand sum of 3 tensor sets is: [0.47539055347442627, 1.217105746269226, 0.9564892053604126, 1.2225737571716309, 2.8246138095855713]\n",
      "\n",
      "Instance 4 of bit.\n",
      "Looking for vocab token: bit\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "bit at index 25: [0.024170154705643654, 0.33883804082870483, 0.44729408621788025, 0.1355164796113968, 0.9544157385826111]\n",
      "Grand sum of 4 tensor sets is: [0.49956071376800537, 1.5559437274932861, 1.4037833213806152, 1.3580902814865112, 3.779029607772827]\n",
      "\n",
      "Instance 5 of bit.\n",
      "Looking for vocab token: bit\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "bit at index 19: [0.22912615537643433, 0.4870338439941406, 0.26886531710624695, 0.6389355659484863, 1.3444633483886719]\n",
      "Grand sum of 5 tensor sets is: [0.7286868691444397, 2.0429775714874268, 1.6726486682891846, 1.9970258474349976, 5.123493194580078]\n",
      "\n",
      "Instance 6 of bit.\n",
      "Looking for vocab token: bit\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([64, 13, 768])\n",
      "Shape of summed layers is: 64 x 768\n",
      "bit at index 16: [0.17585279047489166, 0.4794130325317383, 0.03336039185523987, 0.4364612400531769, 1.3403780460357666]\n",
      "Grand sum of 6 tensor sets is: [0.9045396447181702, 2.522390604019165, 1.706009030342102, 2.4334871768951416, 6.463871002197266]\n",
      "Mean of tensors is: tensor([0.1508, 0.4204, 0.2843, 0.4056, 1.0773]) (768 features in tensor)\n",
      "Saved the embedding for bit.\n",
      "Saved the count of sentences used to create bit embedding\n",
      "Run time for bit was 0.4049985699998615 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "biting\n",
      "\n",
      "Instance 1 of biting.\n",
      "Looking for vocab token: biting\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "biting at index 26: [0.05008101835846901, 0.20077155530452728, -0.03700526803731918, -0.02508280612528324, 1.5657167434692383]\n",
      "Grand sum of 1 tensor sets is: [0.05008101835846901, 0.20077155530452728, -0.03700526803731918, -0.02508280612528324, 1.5657167434692383]\n",
      "Mean of tensors is: tensor([ 0.0501,  0.2008, -0.0370, -0.0251,  1.5657]) (768 features in tensor)\n",
      "Saved the embedding for biting.\n",
      "Saved the count of sentences used to create biting embedding\n",
      "Run time for biting was 0.08201377499995033 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bitter\n",
      "\n",
      "Instance 1 of bitter.\n",
      "Looking for vocab token: bitter\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "bitter at index 7: [-0.030010104179382324, 0.36923670768737793, -0.05473770946264267, 0.359480619430542, 0.3388366401195526]\n",
      "Grand sum of 1 tensor sets is: [-0.030010104179382324, 0.36923670768737793, -0.05473770946264267, 0.359480619430542, 0.3388366401195526]\n",
      "\n",
      "Instance 2 of bitter.\n",
      "Looking for vocab token: bitter\n",
      "\n",
      "Instance 3 of bitter.\n",
      "Looking for vocab token: bitter\n",
      "Indices are [61]\n",
      "Size of token embeddings is torch.Size([64, 13, 768])\n",
      "Shape of summed layers is: 64 x 768\n",
      "bitter at index 61: [0.12478653341531754, 0.13030201196670532, 0.1596059948205948, 0.3133370578289032, 1.4108741283416748]\n",
      "Grand sum of 2 tensor sets is: [0.09477642923593521, 0.49953871965408325, 0.10486828535795212, 0.6728177070617676, 1.7497107982635498]\n",
      "Mean of tensors is: tensor([0.0474, 0.2498, 0.0524, 0.3364, 0.8749]) (768 features in tensor)\n",
      "Saved the embedding for bitter.\n",
      "Saved the count of sentences used to create bitter embedding\n",
      "Run time for bitter was 0.19552638499999375 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bitters\n",
      "weet\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bittersweet.\n",
      "Saved the count of sentences used to create bittersweet embedding\n",
      "Run time for bittersweet was 0.03084265500001493 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "blaming\n",
      "\n",
      "Instance 1 of blaming.\n",
      "Looking for vocab token: blaming\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "blaming at index 12: [-0.02689886838197708, 0.07061503827571869, 0.1127564087510109, 0.3451955318450928, 1.050248146057129]\n",
      "Grand sum of 1 tensor sets is: [-0.02689886838197708, 0.07061503827571869, 0.1127564087510109, 0.3451955318450928, 1.050248146057129]\n",
      "Mean of tensors is: tensor([-0.0269,  0.0706,  0.1128,  0.3452,  1.0502]) (768 features in tensor)\n",
      "Saved the embedding for blaming.\n",
      "Saved the count of sentences used to create blaming embedding\n",
      "Run time for blaming was 0.09550196000009237 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bland\n",
      "\n",
      "Instance 1 of bland.\n",
      "Looking for vocab token: bland\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "bland at index 18: [-0.0815773755311966, 0.08304677903652191, 0.15926288068294525, 0.14481109380722046, 0.8064315319061279]\n",
      "Grand sum of 1 tensor sets is: [-0.0815773755311966, 0.08304677903652191, 0.15926288068294525, 0.14481109380722046, 0.8064315319061279]\n",
      "\n",
      "Instance 2 of bland.\n",
      "Looking for vocab token: bland\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "bland at index 7: [-0.0697711855173111, 0.15786075592041016, 0.01501396019011736, 0.00442313589155674, 0.20358231663703918]\n",
      "Grand sum of 2 tensor sets is: [-0.1513485610485077, 0.24090753495693207, 0.17427684366703033, 0.14923423528671265, 1.0100138187408447]\n",
      "Mean of tensors is: tensor([-0.0757,  0.1205,  0.0871,  0.0746,  0.5050]) (768 features in tensor)\n",
      "Saved the embedding for bland.\n",
      "Saved the count of sentences used to create bland embedding\n",
      "Run time for bland was 0.16326139499983583 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "blank\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for blank.\n",
      "Saved the count of sentences used to create blank embedding\n",
      "Run time for blank was 0.030252551000103267 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bl\n",
      "ase\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for blase.\n",
      "Saved the count of sentences used to create blase embedding\n",
      "Run time for blase was 0.02699986900006479 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bl\n",
      "azed\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for blazed.\n",
      "Saved the count of sentences used to create blazed embedding\n",
      "Run time for blazed was 0.027925640000148633 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bleak\n",
      "\n",
      "Instance 1 of bleak.\n",
      "Looking for vocab token: bleak\n",
      "Indices are [52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([56, 13, 768])\n",
      "Shape of summed layers is: 56 x 768\n",
      "bleak at index 52: [0.007422048598527908, 0.0860687643289566, 0.16305164992809296, -0.27366897463798523, 1.9297645092010498]\n",
      "Grand sum of 1 tensor sets is: [0.007422048598527908, 0.0860687643289566, 0.16305164992809296, -0.27366897463798523, 1.9297645092010498]\n",
      "\n",
      "Instance 2 of bleak.\n",
      "Looking for vocab token: bleak\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "bleak at index 24: [-0.14343465864658356, 0.23452793061733246, 0.2352324277162552, -0.3386574685573578, 1.7515597343444824]\n",
      "Grand sum of 2 tensor sets is: [-0.13601261377334595, 0.32059669494628906, 0.39828407764434814, -0.612326443195343, 3.6813242435455322]\n",
      "\n",
      "Instance 3 of bleak.\n",
      "Looking for vocab token: bleak\n",
      "Indices are [34]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "bleak at index 34: [-0.08131209015846252, 0.18792158365249634, 0.20335111021995544, -0.5511871576309204, 1.8677630424499512]\n",
      "Grand sum of 3 tensor sets is: [-0.21732470393180847, 0.5085182785987854, 0.601635217666626, -1.1635136604309082, 5.5490875244140625]\n",
      "\n",
      "Instance 4 of bleak.\n",
      "Looking for vocab token: bleak\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "bleak at index 29: [0.02195945754647255, 0.2804625630378723, 0.2103370875120163, -0.5825462341308594, 1.7538135051727295]\n",
      "Grand sum of 4 tensor sets is: [-0.19536525011062622, 0.7889808416366577, 0.8119723200798035, -1.7460598945617676, 7.302901268005371]\n",
      "Mean of tensors is: tensor([-0.0488,  0.1972,  0.2030, -0.4365,  1.8257]) (768 features in tensor)\n",
      "Saved the embedding for bleak.\n",
      "Saved the count of sentences used to create bleak embedding\n",
      "Run time for bleak was 0.31953730500003985 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "ble\n",
      "ary\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bleary.\n",
      "Saved the count of sentences used to create bleary embedding\n",
      "Run time for bleary was 0.027451097999801277 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "blessed\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for blessed.\n",
      "Saved the count of sentences used to create blessed embedding\n",
      "Run time for blessed was 0.026847760999999082 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "blew\n",
      "\n",
      "Instance 1 of blew.\n",
      "Looking for vocab token: blew\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "blew at index 11: [-0.07437489926815033, 0.44449490308761597, -0.028911622241139412, 0.22366955876350403, 0.41630491614341736]\n",
      "Grand sum of 1 tensor sets is: [-0.07437489926815033, 0.44449490308761597, -0.028911622241139412, 0.22366955876350403, 0.41630491614341736]\n",
      "\n",
      "Instance 2 of blew.\n",
      "Looking for vocab token: blew\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "blew at index 15: [-0.008666202425956726, 0.23253275454044342, 0.06094861775636673, -0.07413098216056824, 0.24536600708961487]\n",
      "Grand sum of 2 tensor sets is: [-0.08304110169410706, 0.6770276427268982, 0.03203699737787247, 0.1495385766029358, 0.6616709232330322]\n",
      "\n",
      "Instance 3 of blew.\n",
      "Looking for vocab token: blew\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "blew at index 18: [0.12807415425777435, 0.20435526967048645, 0.01333533227443695, -0.09176340699195862, -0.11317366361618042]\n",
      "Grand sum of 3 tensor sets is: [0.0450330525636673, 0.881382942199707, 0.04537232965230942, 0.05777516961097717, 0.5484972596168518]\n",
      "Mean of tensors is: tensor([0.0150, 0.2938, 0.0151, 0.0193, 0.1828]) (768 features in tensor)\n",
      "Saved the embedding for blew.\n",
      "Saved the count of sentences used to create blew embedding\n",
      "Run time for blew was 0.20070349499997064 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "blinded\n",
      "\n",
      "Instance 1 of blinded.\n",
      "Looking for vocab token: blinded\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "blinded at index 12: [0.01805512048304081, -0.18283189833164215, -0.1390848457813263, -0.42292219400405884, 1.0427896976470947]\n",
      "Grand sum of 1 tensor sets is: [0.01805512048304081, -0.18283189833164215, -0.1390848457813263, -0.42292219400405884, 1.0427896976470947]\n",
      "Mean of tensors is: tensor([ 0.0181, -0.1828, -0.1391, -0.4229,  1.0428]) (768 features in tensor)\n",
      "Saved the embedding for blinded.\n",
      "Saved the count of sentences used to create blinded embedding\n",
      "Run time for blinded was 0.09865315100000771 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "blind\n",
      "sided\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for blindsided.\n",
      "Saved the count of sentences used to create blindsided embedding\n",
      "Run time for blindsided was 0.0292247849999967 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bliss\n",
      "\n",
      "Instance 1 of bliss.\n",
      "Looking for vocab token: bliss\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "bliss at index 20: [0.15567883849143982, 0.4650304913520813, 0.18972469866275787, -0.1097821295261383, 0.6592400670051575]\n",
      "Grand sum of 1 tensor sets is: [0.15567883849143982, 0.4650304913520813, 0.18972469866275787, -0.1097821295261383, 0.6592400670051575]\n",
      "Mean of tensors is: tensor([ 0.1557,  0.4650,  0.1897, -0.1098,  0.6592]) (768 features in tensor)\n",
      "Saved the embedding for bliss.\n",
      "Saved the count of sentences used to create bliss embedding\n",
      "Run time for bliss was 0.11796230699997068 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bliss\n",
      "ful\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for blissful.\n",
      "Saved the count of sentences used to create blissful embedding\n",
      "Run time for blissful was 0.031044379999912053 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bliss\n",
      "fully\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for blissfully.\n",
      "Saved the count of sentences used to create blissfully embedding\n",
      "Run time for blissfully was 0.0319702500000858 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bl\n",
      "ithe\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for blithe.\n",
      "Saved the count of sentences used to create blithe embedding\n",
      "Run time for blithe was 0.028429632000097627 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "blown\n",
      "\n",
      "Instance 1 of blown.\n",
      "Looking for vocab token: blown\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "blown at index 31: [-0.07266641408205032, -0.046778544783592224, -0.046366751194000244, -0.23012025654315948, 0.6908119916915894]\n",
      "Grand sum of 1 tensor sets is: [-0.07266641408205032, -0.046778544783592224, -0.046366751194000244, -0.23012025654315948, 0.6908119916915894]\n",
      "\n",
      "Instance 2 of blown.\n",
      "Looking for vocab token: blown\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "blown at index 15: [0.003210049122571945, -0.07981877028942108, 0.1857372224330902, -0.1647617518901825, -0.17870229482650757]\n",
      "Grand sum of 2 tensor sets is: [-0.06945636868476868, -0.1265973150730133, 0.13937047123908997, -0.3948820233345032, 0.5121096968650818]\n",
      "\n",
      "Instance 3 of blown.\n",
      "Looking for vocab token: blown\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "blown at index 18: [0.06318545341491699, 0.08824600279331207, -0.01671622507274151, -0.24834167957305908, 0.4926970601081848]\n",
      "Grand sum of 3 tensor sets is: [-0.006270915269851685, -0.03835131227970123, 0.12265424430370331, -0.6432237029075623, 1.0048067569732666]\n",
      "\n",
      "Instance 4 of blown.\n",
      "Looking for vocab token: blown\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "blown at index 21: [0.06219024956226349, 0.12743771076202393, -0.014277515932917595, -0.04989302158355713, 0.5445766448974609]\n",
      "Grand sum of 4 tensor sets is: [0.055919334292411804, 0.08908639848232269, 0.10837672650814056, -0.6931167244911194, 1.5493834018707275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 5 of blown.\n",
      "Looking for vocab token: blown\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "blown at index 8: [0.22923651337623596, 0.20967547595500946, 0.1278967410326004, -0.33471596240997314, -0.14833146333694458]\n",
      "Grand sum of 5 tensor sets is: [0.2851558327674866, 0.29876187443733215, 0.23627346754074097, -1.0278327465057373, 1.4010519981384277]\n",
      "Mean of tensors is: tensor([ 0.0570,  0.0598,  0.0473, -0.2056,  0.2802]) (768 features in tensor)\n",
      "Saved the embedding for blown.\n",
      "Saved the count of sentences used to create blown embedding\n",
      "Run time for blown was 0.33448945600002844 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "blue\n",
      "\n",
      "Instance 1 of blue.\n",
      "Looking for vocab token: blue\n",
      "\n",
      "Instance 2 of blue.\n",
      "Looking for vocab token: blue\n",
      "\n",
      "Instance 3 of blue.\n",
      "Looking for vocab token: blue\n",
      "\n",
      "Instance 4 of blue.\n",
      "Looking for vocab token: blue\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "blue at index 8: [-0.0019573718309402466, 0.06493822485208511, -0.09682782739400864, 0.1734485775232315, 0.03414951264858246]\n",
      "Grand sum of 1 tensor sets is: [-0.0019573718309402466, 0.06493822485208511, -0.09682782739400864, 0.1734485775232315, 0.03414951264858246]\n",
      "\n",
      "Instance 5 of blue.\n",
      "Looking for vocab token: blue\n",
      "\n",
      "Instance 6 of blue.\n",
      "Looking for vocab token: blue\n",
      "\n",
      "Instance 7 of blue.\n",
      "Looking for vocab token: blue\n",
      "\n",
      "Instance 8 of blue.\n",
      "Looking for vocab token: blue\n",
      "Indices are [49]\n",
      "Size of token embeddings is torch.Size([69, 13, 768])\n",
      "Shape of summed layers is: 69 x 768\n",
      "blue at index 49: [-0.1302642673254013, -0.11087323725223541, -0.1189710944890976, 0.14774315059185028, 1.1140260696411133]\n",
      "Grand sum of 2 tensor sets is: [-0.13222163915634155, -0.0459350124001503, -0.21579891443252563, 0.3211917281150818, 1.148175597190857]\n",
      "Mean of tensors is: tensor([-0.0661, -0.0230, -0.1079,  0.1606,  0.5741]) (768 features in tensor)\n",
      "Saved the embedding for blue.\n",
      "Saved the count of sentences used to create blue embedding\n",
      "Run time for blue was 0.1816483799998423 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "blues\n",
      "\n",
      "Instance 1 of blues.\n",
      "Looking for vocab token: blues\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "blues at index 11: [-0.012859940528869629, 0.1337718963623047, 0.11931844800710678, -0.06124603748321533, 0.2385251224040985]\n",
      "Grand sum of 1 tensor sets is: [-0.012859940528869629, 0.1337718963623047, 0.11931844800710678, -0.06124603748321533, 0.2385251224040985]\n",
      "\n",
      "Instance 2 of blues.\n",
      "Looking for vocab token: blues\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "blues at index 9: [-0.13946081697940826, 0.2628227174282074, -0.031118428334593773, 0.23493970930576324, 0.6344351172447205]\n",
      "Grand sum of 2 tensor sets is: [-0.1523207575082779, 0.3965946137905121, 0.08820001780986786, 0.1736936718225479, 0.8729602098464966]\n",
      "Mean of tensors is: tensor([-0.0762,  0.1983,  0.0441,  0.0868,  0.4365]) (768 features in tensor)\n",
      "Saved the embedding for blues.\n",
      "Saved the count of sentences used to create blues embedding\n",
      "Run time for blues was 0.15699327499987703 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bluff\n",
      "ing\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bluffing.\n",
      "Saved the count of sentences used to create bluffing embedding\n",
      "Run time for bluffing was 0.02717078500018033 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "blunt\n",
      "\n",
      "Instance 1 of blunt.\n",
      "Looking for vocab token: blunt\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "blunt at index 10: [0.03945254534482956, 0.10176309198141098, 0.0032424796372652054, 0.18129919469356537, -0.1744300127029419]\n",
      "Grand sum of 1 tensor sets is: [0.03945254534482956, 0.10176309198141098, 0.0032424796372652054, 0.18129919469356537, -0.1744300127029419]\n",
      "\n",
      "Instance 2 of blunt.\n",
      "Looking for vocab token: blunt\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "blunt at index 36: [0.07376561313867569, -0.02279198169708252, 0.07784267514944077, 0.09228937327861786, 0.10606687515974045]\n",
      "Grand sum of 2 tensor sets is: [0.11321815848350525, 0.07897111028432846, 0.08108515292406082, 0.2735885679721832, -0.06836313754320145]\n",
      "Mean of tensors is: tensor([ 0.0566,  0.0395,  0.0405,  0.1368, -0.0342]) (768 features in tensor)\n",
      "Saved the embedding for blunt.\n",
      "Saved the count of sentences used to create blunt embedding\n",
      "Run time for blunt was 0.18837076199997682 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bl\n",
      "ushing\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for blushing.\n",
      "Saved the count of sentences used to create blushing embedding\n",
      "Run time for blushing was 0.03214986900002259 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "bl\n",
      "ust\n",
      "ering\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for blustering.\n",
      "Saved the count of sentences used to create blustering embedding\n",
      "Run time for blustering was 0.029293044000041846 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "boast\n",
      "ful\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for boastful.\n",
      "Saved the count of sentences used to create boastful embedding\n",
      "Run time for boastful was 0.03155985799980954 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "b\n",
      "ogg\n",
      "led\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for boggled.\n",
      "Saved the count of sentences used to create boggled embedding\n",
      "Run time for boggled was 0.0294043359999705 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "boiling\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for boiling.\n",
      "Saved the count of sentences used to create boiling embedding\n",
      "Run time for boiling was 0.02583992900008525 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "bo\n",
      "ister\n",
      "ous\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for boisterous.\n",
      "Saved the count of sentences used to create boisterous embedding\n",
      "Run time for boisterous was 0.026625737999893317 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bold\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bold.\n",
      "Saved the count of sentences used to create bold embedding\n",
      "Run time for bold was 0.028653894999933982 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bored\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bored.\n",
      "Saved the count of sentences used to create bored embedding\n",
      "Run time for bored was 0.02996851299985792 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "boredom\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for boredom.\n",
      "Saved the count of sentences used to create boredom embedding\n",
      "Run time for boredom was 0.030413338000016665 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "boring\n",
      "\n",
      "Instance 1 of boring.\n",
      "Looking for vocab token: boring\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([6, 13, 768])\n",
      "Shape of summed layers is: 6 x 768\n",
      "boring at index 4: [-0.12228512763977051, 0.15751805901527405, 0.011411692947149277, 0.23586781322956085, 0.6030279994010925]\n",
      "Grand sum of 1 tensor sets is: [-0.12228512763977051, 0.15751805901527405, 0.011411692947149277, 0.23586781322956085, 0.6030279994010925]\n",
      "\n",
      "Instance 2 of boring.\n",
      "Looking for vocab token: boring\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "boring at index 21: [-0.08383029699325562, 0.0653298944234848, 0.14163579046726227, 0.5134932398796082, 1.1484744548797607]\n",
      "Grand sum of 2 tensor sets is: [-0.20611542463302612, 0.22284795343875885, 0.15304748713970184, 0.7493610382080078, 1.751502513885498]\n",
      "\n",
      "Instance 3 of boring.\n",
      "Looking for vocab token: boring\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boring at index 15: [0.060351304709911346, 0.17118412256240845, 0.14410527050495148, 0.5044066309928894, 0.6101423501968384]\n",
      "Grand sum of 3 tensor sets is: [-0.14576411247253418, 0.3940320611000061, 0.2971527576446533, 1.253767728805542, 2.361644744873047]\n",
      "\n",
      "Instance 4 of boring.\n",
      "Looking for vocab token: boring\n",
      "Indices are [48]\n",
      "Size of token embeddings is torch.Size([66, 13, 768])\n",
      "Shape of summed layers is: 66 x 768\n",
      "boring at index 48: [0.18499086797237396, 0.16811926662921906, -0.08184072375297546, 0.5814017057418823, 1.1990149021148682]\n",
      "Grand sum of 4 tensor sets is: [0.03922675549983978, 0.562151312828064, 0.21531203389167786, 1.8351694345474243, 3.560659646987915]\n",
      "Mean of tensors is: tensor([0.0098, 0.1405, 0.0538, 0.4588, 0.8902]) (768 features in tensor)\n",
      "Saved the embedding for boring.\n",
      "Saved the count of sentences used to create boring embedding\n",
      "Run time for boring was 0.2755873319999864 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bothered\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bothered.\n",
      "Saved the count of sentences used to create bothered embedding\n",
      "Run time for bothered was 0.029551775999834717 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bound\n",
      "er\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bounder.\n",
      "Saved the count of sentences used to create bounder embedding\n",
      "Run time for bounder was 0.028728712999964046 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "br\n",
      "ash\n",
      "ness\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for brashness.\n",
      "Saved the count of sentences used to create brashness embedding\n",
      "Run time for brashness was 0.027612342000111312 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "br\n",
      "at\n",
      "ty\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bratty.\n",
      "Saved the count of sentences used to create bratty embedding\n",
      "Run time for bratty was 0.02885050399981992 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "brave\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for brave.\n",
      "Saved the count of sentences used to create brave embedding\n",
      "Run time for brave was 0.028843651999977737 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bright\n",
      "\n",
      "Instance 1 of bright.\n",
      "Looking for vocab token: bright\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([62, 13, 768])\n",
      "Shape of summed layers is: 62 x 768\n",
      "bright at index 27: [-0.022536270320415497, 0.24759125709533691, 0.07956890761852264, -0.10571835190057755, 1.3709838390350342]\n",
      "Grand sum of 1 tensor sets is: [-0.022536270320415497, 0.24759125709533691, 0.07956890761852264, -0.10571835190057755, 1.3709838390350342]\n",
      "\n",
      "Instance 2 of bright.\n",
      "Looking for vocab token: bright\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([53, 13, 768])\n",
      "Shape of summed layers is: 53 x 768\n",
      "bright at index 26: [0.033837828785181046, 0.02026544138789177, 0.0736267939209938, 0.04258468374609947, 1.015838623046875]\n",
      "Grand sum of 2 tensor sets is: [0.011301558464765549, 0.2678566873073578, 0.15319570899009705, -0.06313367187976837, 2.386822462081909]\n",
      "\n",
      "Instance 3 of bright.\n",
      "Looking for vocab token: bright\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "bright at index 20: [-0.03613238036632538, 0.20857055485248566, 0.012602219358086586, -0.2327261120080948, 0.8028964996337891]\n",
      "Grand sum of 3 tensor sets is: [-0.02483082190155983, 0.47642725706100464, 0.16579793393611908, -0.29585978388786316, 3.1897189617156982]\n",
      "Mean of tensors is: tensor([-0.0083,  0.1588,  0.0553, -0.0986,  1.0632]) (768 features in tensor)\n",
      "Saved the embedding for bright.\n",
      "Saved the count of sentences used to create bright embedding\n",
      "Run time for bright was 0.27596889999995255 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "brist\n",
      "ling\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bristling.\n",
      "Saved the count of sentences used to create bristling embedding\n",
      "Run time for bristling was 0.030041701000072862 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "broken\n",
      "\n",
      "Instance 1 of broken.\n",
      "Looking for vocab token: broken\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "broken at index 32: [-0.2137746512889862, -0.1756058931350708, 0.014334514737129211, -0.26710745692253113, 0.4574378430843353]\n",
      "Grand sum of 1 tensor sets is: [-0.2137746512889862, -0.1756058931350708, 0.014334514737129211, -0.26710745692253113, 0.4574378430843353]\n",
      "\n",
      "Instance 2 of broken.\n",
      "Looking for vocab token: broken\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "broken at index 18: [-0.0535789430141449, 0.18640366196632385, 0.14584055542945862, 0.09627316892147064, 0.7576018571853638]\n",
      "Grand sum of 2 tensor sets is: [-0.2673535943031311, 0.010797768831253052, 0.16017507016658783, -0.17083428800106049, 1.2150397300720215]\n",
      "\n",
      "Instance 3 of broken.\n",
      "Looking for vocab token: broken\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "broken at index 15: [0.03398871794342995, 0.12251338362693787, -0.1088220477104187, 0.022681929171085358, 0.7945266366004944]\n",
      "Grand sum of 3 tensor sets is: [-0.23336488008499146, 0.13331115245819092, 0.05135302245616913, -0.14815235137939453, 2.009566307067871]\n",
      "\n",
      "Instance 4 of broken.\n",
      "Looking for vocab token: broken\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([69, 13, 768])\n",
      "Shape of summed layers is: 69 x 768\n",
      "broken at index 8: [0.1401904821395874, -0.052392154932022095, -0.05744657665491104, -0.10301835834980011, 0.7515886425971985]\n",
      "Grand sum of 4 tensor sets is: [-0.09317439794540405, 0.08091899752616882, -0.006093554198741913, -0.25117069482803345, 2.761154890060425]\n",
      "\n",
      "Instance 5 of broken.\n",
      "Looking for vocab token: broken\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "broken at index 4: [-0.040617436170578, 0.046421486884355545, -0.04463688284158707, 0.03649985417723656, 1.3673996925354004]\n",
      "Grand sum of 5 tensor sets is: [-0.13379183411598206, 0.12734048068523407, -0.05073043704032898, -0.2146708369255066, 4.128554344177246]\n",
      "\n",
      "Instance 6 of broken.\n",
      "Looking for vocab token: broken\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "broken at index 3: [-0.07285919785499573, 0.07772047817707062, -0.01281028799712658, -0.16113552451133728, 0.7422123551368713]\n",
      "Grand sum of 6 tensor sets is: [-0.20665103197097778, 0.2050609588623047, -0.06354072690010071, -0.37580636143684387, 4.870766639709473]\n",
      "\n",
      "Instance 7 of broken.\n",
      "Looking for vocab token: broken\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "broken at index 20: [0.0433005765080452, 0.10111697018146515, 0.0266824122518301, 0.3578331172466278, 0.4834384322166443]\n",
      "Grand sum of 7 tensor sets is: [-0.16335046291351318, 0.30617791414260864, -0.03685831278562546, -0.017973244190216064, 5.354205131530762]\n",
      "\n",
      "Instance 8 of broken.\n",
      "Looking for vocab token: broken\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "broken at index 6: [-0.0714183822274208, 0.04572702944278717, -0.0707927793264389, -0.08144207298755646, 0.011225782334804535]\n",
      "Grand sum of 8 tensor sets is: [-0.2347688376903534, 0.3519049286842346, -0.10765109211206436, -0.09941531717777252, 5.36543083190918]\n",
      "\n",
      "Instance 9 of broken.\n",
      "Looking for vocab token: broken\n",
      "Indices are [44]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "broken at index 44: [0.19596587121486664, -0.20377011597156525, 0.2386152148246765, -0.28763699531555176, 0.8926362991333008]\n",
      "Grand sum of 9 tensor sets is: [-0.038802966475486755, 0.14813481271266937, 0.13096413016319275, -0.3870522975921631, 6.2580671310424805]\n",
      "\n",
      "Instance 10 of broken.\n",
      "Looking for vocab token: broken\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "broken at index 19: [0.0008238591253757477, 0.12114391475915909, 0.23993952572345734, -0.08303762972354889, 0.5951855778694153]\n",
      "Grand sum of 10 tensor sets is: [-0.03797910735011101, 0.26927873492240906, 0.3709036707878113, -0.4700899124145508, 6.85325288772583]\n",
      "\n",
      "Instance 11 of broken.\n",
      "Looking for vocab token: broken\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([61, 13, 768])\n",
      "Shape of summed layers is: 61 x 768\n",
      "broken at index 6: [0.09419779479503632, -0.06433404982089996, -0.11210036277770996, -0.20582371950149536, 0.8211214542388916]\n",
      "Grand sum of 11 tensor sets is: [0.05621868744492531, 0.2049446851015091, 0.2588033080101013, -0.6759136319160461, 7.674374580383301]\n",
      "\n",
      "Instance 12 of broken.\n",
      "Looking for vocab token: broken\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broken at index 4: [0.0061073265969753265, 0.12184877693653107, -0.06926628202199936, 0.059637293219566345, 0.47531670331954956]\n",
      "Grand sum of 12 tensor sets is: [0.062326014041900635, 0.32679346203804016, 0.18953701853752136, -0.6162763237953186, 8.149691581726074]\n",
      "\n",
      "Instance 13 of broken.\n",
      "Looking for vocab token: broken\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "broken at index 9: [-0.022339805960655212, -0.021274246275424957, -0.06735842674970627, 0.007990519516170025, 1.0947480201721191]\n",
      "Grand sum of 13 tensor sets is: [0.03998620808124542, 0.3055192232131958, 0.1221785917878151, -0.6082857847213745, 9.244440078735352]\n",
      "\n",
      "Instance 14 of broken.\n",
      "Looking for vocab token: broken\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "broken at index 16: [-0.031853094696998596, 0.20837463438510895, 0.13104000687599182, -0.20576705038547516, 0.6488054990768433]\n",
      "Grand sum of 14 tensor sets is: [0.008133113384246826, 0.5138938426971436, 0.2532185912132263, -0.8140528202056885, 9.893245697021484]\n",
      "\n",
      "Instance 15 of broken.\n",
      "Looking for vocab token: broken\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "broken at index 12: [0.07336003333330154, 0.10084475576877594, -0.039505332708358765, 0.22226324677467346, 0.8781154155731201]\n",
      "Grand sum of 15 tensor sets is: [0.08149314671754837, 0.6147385835647583, 0.21371325850486755, -0.5917896032333374, 10.771361351013184]\n",
      "\n",
      "Instance 16 of broken.\n",
      "Looking for vocab token: broken\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "broken at index 24: [-0.20015838742256165, 0.0403577983379364, -0.009840687736868858, -0.08820826560258865, 0.21668562293052673]\n",
      "Grand sum of 16 tensor sets is: [-0.11866524070501328, 0.6550964117050171, 0.20387257635593414, -0.6799978613853455, 10.988046646118164]\n",
      "\n",
      "Instance 17 of broken.\n",
      "Looking for vocab token: broken\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "broken at index 37: [0.11891201883554459, 0.10373497009277344, 0.25086140632629395, -0.016955405473709106, 0.4028061330318451]\n",
      "Grand sum of 17 tensor sets is: [0.00024677813053131104, 0.7588313817977905, 0.4547339677810669, -0.696953296661377, 11.390852928161621]\n",
      "\n",
      "Instance 18 of broken.\n",
      "Looking for vocab token: broken\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "broken at index 4: [-0.16907012462615967, -0.09779305756092072, 0.11858765035867691, 0.23258990049362183, 0.7960646152496338]\n",
      "Grand sum of 18 tensor sets is: [-0.16882334649562836, 0.661038339138031, 0.5733216404914856, -0.4643633961677551, 12.186917304992676]\n",
      "Mean of tensors is: tensor([-0.0094,  0.0367,  0.0319, -0.0258,  0.6771]) (768 features in tensor)\n",
      "Saved the embedding for broken.\n",
      "Saved the count of sentences used to create broken embedding\n",
      "Run time for broken was 1.1246211830000448 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "broken\n",
      "hearted\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for brokenhearted.\n",
      "Saved the count of sentences used to create brokenhearted embedding\n",
      "Run time for brokenhearted was 0.029759959999864805 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "broken\n",
      "heartedly\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for brokenheartedly.\n",
      "Saved the count of sentences used to create brokenheartedly embedding\n",
      "Run time for brokenheartedly was 0.02687647800007653 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bro\n",
      "oding\n",
      "\n",
      "Instance 1 of brooding.\n",
      "Looking for vocab token: bro\n",
      "Looking for vocab token: oding\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "bro at index 2: [-0.06792990863323212, 0.24359217286109924, -0.184430330991745, 0.08127739280462265, 1.0099856853485107]\n",
      "Grand sum of 1 tensor sets is: [-0.06792990863323212, 0.24359217286109924, -0.184430330991745, 0.08127739280462265, 1.0099856853485107]\n",
      "\n",
      "Instance 2 of brooding.\n",
      "Looking for vocab token: bro\n",
      "Looking for vocab token: oding\n",
      "Indices are [17, 18]\n",
      "Size of token embeddings is torch.Size([80, 13, 768])\n",
      "Shape of summed layers is: 80 x 768\n",
      "bro at index 17: [0.042223989963531494, 0.6393243670463562, 0.1505535989999771, 0.4161645174026489, 0.21705973148345947]\n",
      "oding at index 18: [0.04797876998782158, 0.2625999450683594, 0.1343749612569809, 0.3711760640144348, 0.9576461315155029]\n",
      "Grand sum of 2 tensor sets is: [-0.02282852679491043, 0.694554328918457, -0.04196605086326599, 0.4749476909637451, 1.5973386764526367]\n",
      "Mean of tensors is: tensor([-0.0114,  0.3473, -0.0210,  0.2375,  0.7987]) (768 features in tensor)\n",
      "Saved the embedding for brooding.\n",
      "Saved the count of sentences used to create brooding embedding\n",
      "Run time for brooding was 0.21286826599998676 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bro\n",
      "ody\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for broody.\n",
      "Saved the count of sentences used to create broody embedding\n",
      "Run time for broody was 0.029916950999904657 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bruised\n",
      "\n",
      "Instance 1 of bruised.\n",
      "Looking for vocab token: bruised\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "bruised at index 26: [-0.07277945429086685, -0.016109921038150787, 0.17099255323410034, 0.04874351620674133, 0.6105177402496338]\n",
      "Grand sum of 1 tensor sets is: [-0.07277945429086685, -0.016109921038150787, 0.17099255323410034, 0.04874351620674133, 0.6105177402496338]\n",
      "Mean of tensors is: tensor([-0.0728, -0.0161,  0.1710,  0.0487,  0.6105]) (768 features in tensor)\n",
      "Saved the embedding for bruised.\n",
      "Saved the count of sentences used to create bruised embedding\n",
      "Run time for bruised was 0.10130174699997951 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "br\n",
      "us\n",
      "que\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for brusque.\n",
      "Saved the count of sentences used to create brusque embedding\n",
      "Run time for brusque was 0.029914304999920205 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bug\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bug.\n",
      "Saved the count of sentences used to create bug embedding\n",
      "Run time for bug was 0.032480703999908656 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bul\n",
      "ging\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bulging.\n",
      "Saved the count of sentences used to create bulging embedding\n",
      "Run time for bulging was 0.03114009699993403 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bully\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bully.\n",
      "Saved the count of sentences used to create bully embedding\n",
      "Run time for bully was 0.03063341799997943 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bullying\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bullying.\n",
      "Saved the count of sentences used to create bullying embedding\n",
      "Run time for bullying was 0.029916720000073838 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bum\n",
      "med\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bummed.\n",
      "Saved the count of sentences used to create bummed embedding\n",
      "Run time for bummed was 0.026741181000033976 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "buoy\n",
      "ant\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for buoyant.\n",
      "Saved the count of sentences used to create buoyant embedding\n",
      "Run time for buoyant was 0.026223625999818978 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "burd\n",
      "ened\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for burdened.\n",
      "Saved the count of sentences used to create burdened embedding\n",
      "Run time for burdened was 0.02479732699998749 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "burn\n",
      "\n",
      "Instance 1 of burn.\n",
      "Looking for vocab token: burn\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for burn.\n",
      "Saved the count of sentences used to create burn embedding\n",
      "Run time for burn was 0.026330366999900434 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "bursting\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bursting.\n",
      "Saved the count of sentences used to create bursting embedding\n",
      "Run time for bursting was 0.025080030000026454 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "bus\n",
      "hed\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bushed.\n",
      "Saved the count of sentences used to create bushed embedding\n",
      "Run time for bushed was 0.02521979299990562 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "cage\n",
      "y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for cagey.\n",
      "Saved the count of sentences used to create cagey embedding\n",
      "Run time for cagey was 0.025913127999956487 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "c\n",
      "agy\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for cagy.\n",
      "Saved the count of sentences used to create cagy embedding\n",
      "Run time for cagy was 0.02729970699988371 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "calculating\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for calculating.\n",
      "Saved the count of sentences used to create calculating embedding\n",
      "Run time for calculating was 0.026354245999982595 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "call\n",
      "ous\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for callous.\n",
      "Saved the count of sentences used to create callous embedding\n",
      "Run time for callous was 0.026355504000093788 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "call\n",
      "used\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for callused.\n",
      "Saved the count of sentences used to create callused embedding\n",
      "Run time for callused was 0.02858758200000011 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "calm\n",
      "\n",
      "Instance 1 of calm.\n",
      "Looking for vocab token: calm\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "calm at index 6: [-0.25279855728149414, 0.41302788257598877, 0.2845711410045624, -0.017564674839377403, 1.038440465927124]\n",
      "Grand sum of 1 tensor sets is: [-0.25279855728149414, 0.41302788257598877, 0.2845711410045624, -0.017564674839377403, 1.038440465927124]\n",
      "\n",
      "Instance 2 of calm.\n",
      "Looking for vocab token: calm\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "calm at index 12: [-0.20844048261642456, 0.1337064504623413, 0.25260129570961, 0.01999104768037796, 0.3042166829109192]\n",
      "Grand sum of 2 tensor sets is: [-0.4612390398979187, 0.5467343330383301, 0.5371724367141724, 0.002426372841000557, 1.3426570892333984]\n",
      "\n",
      "Instance 3 of calm.\n",
      "Looking for vocab token: calm\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "calm at index 18: [-0.43033263087272644, 0.22903087735176086, 0.35628610849380493, 0.11011819541454315, 0.9177219271659851]\n",
      "Grand sum of 3 tensor sets is: [-0.8915716409683228, 0.7757651805877686, 0.8934585452079773, 0.11254456639289856, 2.2603790760040283]\n",
      "\n",
      "Instance 4 of calm.\n",
      "Looking for vocab token: calm\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "calm at index 7: [-0.08112815022468567, 0.2406127005815506, 0.1721675544977188, -0.13935019075870514, 0.5595735311508179]\n",
      "Grand sum of 4 tensor sets is: [-0.972699761390686, 1.0163779258728027, 1.0656261444091797, -0.02680562436580658, 2.8199524879455566]\n",
      "\n",
      "Instance 5 of calm.\n",
      "Looking for vocab token: calm\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "calm at index 24: [-0.22109413146972656, 0.16821953654289246, 0.3865342140197754, -0.0761929452419281, 0.5468295812606812]\n",
      "Grand sum of 5 tensor sets is: [-1.1937938928604126, 1.1845974922180176, 1.452160358428955, -0.10299856960773468, 3.3667821884155273]\n",
      "\n",
      "Instance 6 of calm.\n",
      "Looking for vocab token: calm\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "calm at index 2: [-0.0001698806881904602, 0.10787945985794067, 0.24668383598327637, 0.15606491267681122, 0.7723456621170044]\n",
      "Grand sum of 6 tensor sets is: [-1.1939637660980225, 1.2924768924713135, 1.6988441944122314, 0.05306634306907654, 4.139127731323242]\n",
      "Mean of tensors is: tensor([-0.1990,  0.2154,  0.2831,  0.0088,  0.6899]) (768 features in tensor)\n",
      "Saved the embedding for calm.\n",
      "Saved the count of sentences used to create calm embedding\n",
      "Run time for calm was 0.36131546900014655 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "calming\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for calming.\n",
      "Saved the count of sentences used to create calming embedding\n",
      "Run time for calming was 0.02972104200011927 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "calm\n",
      "ness\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for calmness.\n",
      "Saved the count of sentences used to create calmness embedding\n",
      "Run time for calmness was 0.026529213999992862 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "can\n",
      "ny\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for canny.\n",
      "Saved the count of sentences used to create canny embedding\n",
      "Run time for canny was 0.03263222800001131 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "cant\n",
      "ank\n",
      "erous\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for cantankerous.\n",
      "Saved the count of sentences used to create cantankerous embedding\n",
      "Run time for cantankerous was 0.03291867699999784 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "capable\n",
      "\n",
      "Instance 1 of capable.\n",
      "Looking for vocab token: capable\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([63, 13, 768])\n",
      "Shape of summed layers is: 63 x 768\n",
      "capable at index 29: [-0.25112947821617126, 0.1202336847782135, 0.013040161691606045, 0.5691428184509277, 0.7113959789276123]\n",
      "Grand sum of 1 tensor sets is: [-0.25112947821617126, 0.1202336847782135, 0.013040161691606045, 0.5691428184509277, 0.7113959789276123]\n",
      "\n",
      "Instance 2 of capable.\n",
      "Looking for vocab token: capable\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "capable at index 33: [-0.1854725480079651, 0.1265634000301361, 0.21540480852127075, -0.000995703274384141, 0.5745638608932495]\n",
      "Grand sum of 2 tensor sets is: [-0.43660202622413635, 0.2467970848083496, 0.22844496369361877, 0.5681471228599548, 1.2859598398208618]\n",
      "\n",
      "Instance 3 of capable.\n",
      "Looking for vocab token: capable\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([57, 13, 768])\n",
      "Shape of summed layers is: 57 x 768\n",
      "capable at index 18: [-0.24721315503120422, 0.11370512843132019, 0.1430843025445938, 0.29236677289009094, 0.44813328981399536]\n",
      "Grand sum of 3 tensor sets is: [-0.6838151812553406, 0.3605022132396698, 0.3715292811393738, 0.8605139255523682, 1.734093189239502]\n",
      "\n",
      "Instance 4 of capable.\n",
      "Looking for vocab token: capable\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "capable at index 19: [-0.11645032465457916, 0.24879150092601776, 0.32486793398857117, 0.3581140637397766, 0.7443939447402954]\n",
      "Grand sum of 4 tensor sets is: [-0.8002654910087585, 0.6092936992645264, 0.6963971853256226, 1.2186279296875, 2.478487014770508]\n",
      "\n",
      "Instance 5 of capable.\n",
      "Looking for vocab token: capable\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "capable at index 22: [-0.14688166975975037, 0.18578529357910156, 0.3028198480606079, 0.42760616540908813, 0.40149009227752686]\n",
      "Grand sum of 5 tensor sets is: [-0.9471471309661865, 0.7950789928436279, 0.9992170333862305, 1.6462340354919434, 2.879977226257324]\n",
      "\n",
      "Instance 6 of capable.\n",
      "Looking for vocab token: capable\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "capable at index 31: [-0.16031433641910553, 0.3544237017631531, 0.11167184263467789, 0.2531237006187439, 0.8760644793510437]\n",
      "Grand sum of 6 tensor sets is: [-1.1074614524841309, 1.1495027542114258, 1.1108888387680054, 1.899357795715332, 3.7560417652130127]\n",
      "\n",
      "Instance 7 of capable.\n",
      "Looking for vocab token: capable\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "capable at index 10: [-0.24062272906303406, -0.08720362186431885, 0.07831976562738419, 0.22733768820762634, 0.6724991798400879]\n",
      "Grand sum of 7 tensor sets is: [-1.3480842113494873, 1.062299132347107, 1.1892086267471313, 2.126695394515991, 4.42854118347168]\n",
      "\n",
      "Instance 8 of capable.\n",
      "Looking for vocab token: capable\n",
      "Indices are [25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([78, 13, 768])\n",
      "Shape of summed layers is: 78 x 768\n",
      "capable at index 25: [-0.14691928029060364, 0.198392853140831, 0.10621216148138046, 0.4685290455818176, 0.49979764223098755]\n",
      "Grand sum of 8 tensor sets is: [-1.4950034618377686, 1.2606920003890991, 1.29542076587677, 2.595224380493164, 4.928339004516602]\n",
      "\n",
      "Instance 9 of capable.\n",
      "Looking for vocab token: capable\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "capable at index 18: [-0.06258904933929443, 0.27273035049438477, 0.1656576693058014, 0.29195722937583923, 0.5255934000015259]\n",
      "Grand sum of 9 tensor sets is: [-1.557592511177063, 1.5334223508834839, 1.461078405380249, 2.887181520462036, 5.453932285308838]\n",
      "\n",
      "Instance 10 of capable.\n",
      "Looking for vocab token: capable\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "capable at index 40: [0.051228754222393036, 0.29192084074020386, 0.2403024137020111, -0.0815231055021286, 0.05432485044002533]\n",
      "Grand sum of 10 tensor sets is: [-1.5063637495040894, 1.825343132019043, 1.7013808488845825, 2.8056583404541016, 5.508256912231445]\n",
      "\n",
      "Instance 11 of capable.\n",
      "Looking for vocab token: capable\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "capable at index 12: [-0.04300103336572647, 0.05494571849703789, 0.12104368954896927, -0.029872305691242218, 0.07466783374547958]\n",
      "Grand sum of 11 tensor sets is: [-1.5493648052215576, 1.88028883934021, 1.8224245309829712, 2.7757859230041504, 5.582924842834473]\n",
      "\n",
      "Instance 12 of capable.\n",
      "Looking for vocab token: capable\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "capable at index 24: [-0.19885237514972687, 0.1375381499528885, 0.15328435599803925, 0.09487111121416092, 0.7529120445251465]\n",
      "Grand sum of 12 tensor sets is: [-1.748217225074768, 2.017827033996582, 1.9757088422775269, 2.870656967163086, 6.335836887359619]\n",
      "\n",
      "Instance 13 of capable.\n",
      "Looking for vocab token: capable\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "capable at index 37: [-0.3250466287136078, 0.28478074073791504, 0.11593908816576004, 0.04716505855321884, 0.8706523776054382]\n",
      "Grand sum of 13 tensor sets is: [-2.0732638835906982, 2.302607774734497, 2.0916478633880615, 2.9178221225738525, 7.206489086151123]\n",
      "\n",
      "Instance 14 of capable.\n",
      "Looking for vocab token: capable\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "capable at index 25: [-0.1468985676765442, 0.1549660861492157, 0.17966444790363312, 0.30876654386520386, 0.45618271827697754]\n",
      "Grand sum of 14 tensor sets is: [-2.2201623916625977, 2.457573890686035, 2.2713122367858887, 3.226588726043701, 7.66267204284668]\n",
      "\n",
      "Instance 15 of capable.\n",
      "Looking for vocab token: capable\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "capable at index 6: [-0.2995275557041168, -0.07658019661903381, 0.22137224674224854, 0.2229296863079071, 0.6789380311965942]\n",
      "Grand sum of 15 tensor sets is: [-2.5196900367736816, 2.380993604660034, 2.4926843643188477, 3.4495184421539307, 8.341609954833984]\n",
      "Mean of tensors is: tensor([-0.1680,  0.1587,  0.1662,  0.2300,  0.5561]) (768 features in tensor)\n",
      "Saved the embedding for capable.\n",
      "Saved the count of sentences used to create capable embedding\n",
      "Run time for capable was 1.0632341869998072 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "cap\n",
      "ric\n",
      "ious\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for capricious.\n",
      "Saved the count of sentences used to create capricious embedding\n",
      "Run time for capricious was 0.03713104600001316 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "capt\n",
      "ivated\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for captivated.\n",
      "Saved the count of sentences used to create captivated embedding\n",
      "Run time for captivated was 0.02767225000002327 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "captive\n",
      "\n",
      "Instance 1 of captive.\n",
      "Looking for vocab token: captive\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "captive at index 19: [0.0797126442193985, 0.03616652637720108, -0.015198448672890663, -0.034709617495536804, 0.43411943316459656]\n",
      "Grand sum of 1 tensor sets is: [0.0797126442193985, 0.03616652637720108, -0.015198448672890663, -0.034709617495536804, 0.43411943316459656]\n",
      "\n",
      "Instance 2 of captive.\n",
      "Looking for vocab token: captive\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([84, 13, 768])\n",
      "Shape of summed layers is: 84 x 768\n",
      "captive at index 28: [-0.07171614468097687, -0.021257519721984863, 0.0052017103880643845, -0.09671317785978317, 0.4924357533454895]\n",
      "Grand sum of 2 tensor sets is: [0.00799649953842163, 0.014909006655216217, -0.009996738284826279, -0.13142278790473938, 0.9265551567077637]\n",
      "Mean of tensors is: tensor([ 0.0040,  0.0075, -0.0050, -0.0657,  0.4633]) (768 features in tensor)\n",
      "Saved the embedding for captive.\n",
      "Saved the count of sentences used to create captive embedding\n",
      "Run time for captive was 0.22019889900002454 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "care\n",
      "free\n",
      "\n",
      "Instance 1 of carefree.\n",
      "Looking for vocab token: care\n",
      "Looking for vocab token: free\n",
      "Indices are [22, 23]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "care at index 22: [0.2667367160320282, 0.8426485657691956, 0.2914060950279236, 0.5788156986236572, -0.2798318564891815]\n",
      "free at index 23: [0.06891933083534241, 0.44731569290161133, -0.11474056541919708, 0.20944780111312866, 1.0052837133407593]\n",
      "Grand sum of 1 tensor sets is: [0.1678280234336853, 0.644982099533081, 0.08833276480436325, 0.39413174986839294, 0.3627259135246277]\n",
      "\n",
      "Instance 2 of carefree.\n",
      "Looking for vocab token: care\n",
      "Looking for vocab token: free\n",
      "Indices are [21, 22]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "care at index 21: [0.3487991392612457, 0.5963505506515503, 0.38417020440101624, 0.8982100486755371, -0.013673409819602966]\n",
      "free at index 22: [0.11221563816070557, 0.10966822504997253, 0.04888328164815903, -0.03342262655496597, 1.4527394771575928]\n",
      "Grand sum of 2 tensor sets is: [0.39833539724349976, 0.9979915022850037, 0.3048595190048218, 0.8265254497528076, 1.082258939743042]\n",
      "Mean of tensors is: tensor([0.1992, 0.4990, 0.1524, 0.4133, 0.5411]) (768 features in tensor)\n",
      "Saved the embedding for carefree.\n",
      "Saved the count of sentences used to create carefree embedding\n",
      "Run time for carefree was 0.16884339399985038 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "careful\n",
      "\n",
      "Instance 1 of careful.\n",
      "Looking for vocab token: careful\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "careful at index 6: [-0.02814478427171707, -0.09562359750270844, -0.06201883405447006, 0.5605047941207886, 0.8142984509468079]\n",
      "Grand sum of 1 tensor sets is: [-0.02814478427171707, -0.09562359750270844, -0.06201883405447006, 0.5605047941207886, 0.8142984509468079]\n",
      "\n",
      "Instance 2 of careful.\n",
      "Looking for vocab token: careful\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "careful at index 23: [-0.032322950661182404, 0.14917027950286865, 0.1564461588859558, 0.3692052364349365, 0.5934814810752869]\n",
      "Grand sum of 2 tensor sets is: [-0.060467734932899475, 0.05354668200016022, 0.09442732483148575, 0.9297100305557251, 1.4077799320220947]\n",
      "\n",
      "Instance 3 of careful.\n",
      "Looking for vocab token: careful\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "careful at index 7: [0.4356710910797119, -0.0032262951135635376, 0.07549695670604706, 0.5789555311203003, 0.768084704875946]\n",
      "Grand sum of 3 tensor sets is: [0.37520337104797363, 0.05032038688659668, 0.1699242889881134, 1.5086655616760254, 2.1758646965026855]\n",
      "Mean of tensors is: tensor([0.1251, 0.0168, 0.0566, 0.5029, 0.7253]) (768 features in tensor)\n",
      "Saved the embedding for careful.\n",
      "Saved the count of sentences used to create careful embedding\n",
      "Run time for careful was 0.20462463400008346 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "careless\n",
      "\n",
      "Instance 1 of careless.\n",
      "Looking for vocab token: careless\n",
      "Indices are [32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "careless at index 32: [0.06383327394723892, 0.40585237741470337, -0.0006874389946460724, -0.04816702380776405, 0.45477619767189026]\n",
      "Grand sum of 1 tensor sets is: [0.06383327394723892, 0.40585237741470337, -0.0006874389946460724, -0.04816702380776405, 0.45477619767189026]\n",
      "Mean of tensors is: tensor([ 0.0638,  0.4059, -0.0007, -0.0482,  0.4548]) (768 features in tensor)\n",
      "Saved the embedding for careless.\n",
      "Saved the count of sentences used to create careless embedding\n",
      "Run time for careless was 0.11446805099990343 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "caring\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for caring.\n",
      "Saved the count of sentences used to create caring embedding\n",
      "Run time for caring was 0.03808503599998403 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "cat\n",
      "ty\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for catty.\n",
      "Saved the count of sentences used to create catty embedding\n",
      "Run time for catty was 0.03941043099985109 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized vocabulary word:\n",
      "ca\n",
      "ust\n",
      "ic\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for caustic.\n",
      "Saved the count of sentences used to create caustic embedding\n",
      "Run time for caustic was 0.031140754000034576 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "caution\n",
      "ary\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for cautionary.\n",
      "Saved the count of sentences used to create cautionary embedding\n",
      "Run time for cautionary was 0.02628552100009074 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "cautious\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for cautious.\n",
      "Saved the count of sentences used to create cautious embedding\n",
      "Run time for cautious was 0.028139250999856813 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized vocabulary word:\n",
      "caval\n",
      "ier\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for cavalier.\n",
      "Saved the count of sentences used to create cavalier embedding\n",
      "Run time for cavalier was 0.0286561249999977 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "celebrating\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for celebrating.\n",
      "Saved the count of sentences used to create celebrating embedding\n",
      "Run time for celebrating was 0.027709798000159935 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized vocabulary word:\n",
      "celebration\n",
      "\n",
      "Instance 1 of celebration.\n",
      "Looking for vocab token: celebration\n",
      "Indices are [45]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "celebration at index 45: [0.16741585731506348, 0.13570436835289001, -0.15651634335517883, -0.2610800266265869, 0.27943354845046997]\n",
      "Grand sum of 1 tensor sets is: [0.16741585731506348, 0.13570436835289001, -0.15651634335517883, -0.2610800266265869, 0.27943354845046997]\n",
      "\n",
      "Instance 2 of celebration.\n",
      "Looking for vocab token: celebration\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "celebration at index 8: [0.26520439982414246, -0.13182613253593445, -0.05765427649021149, 0.3267160952091217, 0.142203688621521]\n",
      "Grand sum of 2 tensor sets is: [0.43262025713920593, 0.0038782358169555664, -0.21417061984539032, 0.06563606858253479, 0.42163723707199097]\n",
      "\n",
      "Instance 3 of celebration.\n",
      "Looking for vocab token: celebration\n",
      "\n",
      "Instance 4 of celebration.\n",
      "Looking for vocab token: celebration\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "celebration at index 5: [0.22875411808490753, -0.11657850444316864, 0.006071886047720909, 0.28234779834747314, 0.06990193575620651]\n",
      "Grand sum of 3 tensor sets is: [0.6613743901252747, -0.11270026862621307, -0.20809873938560486, 0.34798386693000793, 0.4915391802787781]\n",
      "\n",
      "Instance 5 of celebration.\n",
      "Looking for vocab token: celebration\n",
      "Indices are [40]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-8de5e927ddf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Indices are {indices}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0;31m# Get the feature vectors for all tokens in the line/sentence.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                     \u001b[0mtoken_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_token_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                     \u001b[0;31m# Sum the last four layers to get embeddings for the line/sentence.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m#                         for t in v_tokens[1:-1]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-70eaa1f8a020>\u001b[0m in \u001b[0;36mcreate_token_embeddings\u001b[0;34m(tokenized_text)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Batch size 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_lm_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mencoded_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtoken_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/venv/lib/python3.7/site-packages/transformers/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, masked_lm_labels)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         )\n\u001b[1;32m    234\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/venv/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m         )\n\u001b[1;32m    738\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/venv/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             layer_outputs = layer_module(\n\u001b[0;32m--> 407\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             )\n\u001b[1;32m    409\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/venv/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     ):\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0mself_attention_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add self attentions if we output attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/venv/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    312\u001b[0m     ):\n\u001b[1;32m    313\u001b[0m         self_outputs = self.self(\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         )\n\u001b[1;32m    316\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/venv/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Process vocabulary words in the outer loop.\n",
    "for v in vocab:\n",
    "    start = timer()\n",
    "    with open(context_file, 'r') as lines:\n",
    "        v_sum = torch.zeros([1, 768])\n",
    "        v_tokens = tokenizer.encode(v)\n",
    "        print(f'\\nThere are {len(v_tokens) - 2} tokens in tokenized vocabulary word:')\n",
    "        for t in v_tokens[1:-1]:\n",
    "            print(tokenizer.decode(t).strip())\n",
    "        count_sentence = 0\n",
    "        count_tensor = 0\n",
    "        \n",
    "        # Process all lines in the context file in the inner loop.\n",
    "        for line in lines:\n",
    "            # Check for this vocab word in this line; if found, split the line into individual sentences.\n",
    "            if v in line.lower().split():\n",
    "                for sentence in line.split('.'):\n",
    "                    if v in sentence.lower():\n",
    "                        line = sentence\n",
    "                        count_sentence += 1\n",
    "                        print(f'\\nInstance {count_sentence} of {tokenizer.decode(v_tokens[1:-1]).strip()}.')\n",
    "                        break\n",
    "                # Split the new sentence-based line into tokens.\n",
    "                # Use max_length to avoid overflowing the maximum sequence length for the model.\n",
    "                tokenized_text = tokenizer.encode(line, add_special_tokens=True, max_length=512)\n",
    "#                 print(f'The decoded sentence has {len(tokenized_text)} tokens and is: {tokenizer.decode(tokenized_text)}')\n",
    "                indices = []              \n",
    "\n",
    "                # Check to see whether the vocab word is found in this particular line.\n",
    "                # Initially, some lines may have comprised multiple sentences, which were\n",
    "                # broken out individually above.\n",
    "                for t in v_tokens[1:-1]:\n",
    "                    print(f'Looking for vocab token: {tokenizer.decode(t).strip()}')\n",
    "                    for i, token_str in enumerate(tokenized_text):\n",
    "#                         print(f'Next sentence token: {tokenizer.decode(token_str).strip()}')\n",
    "#                         print(tokenizer.decode(token_str).strip() == tokenizer.decode(t).strip())\n",
    "                        if tokenizer.decode(token_str).strip() == tokenizer.decode(t).strip():\n",
    "                            indices.append(i)               \n",
    "\n",
    "                ###################################################################################\n",
    "                # If the vocabulary word was found, process the containing line.\n",
    "                if indices:\n",
    "\n",
    "                    # The vocab word was found in this line/sentence, at the locations in indices.\n",
    "                    print(f'Indices are {indices}')\n",
    "                    # Get the feature vectors for all tokens in the line/sentence.\n",
    "                    token_embeddings = create_token_embeddings(tokenized_text)\n",
    "                    # Sum the last four layers to get embeddings for the line/sentence.\n",
    "#                         for t in v_tokens[1:-1]:\n",
    "#                             for i, token_str in enumerate(tokenized_text):\n",
    "#                                 if (tokenizer.decode(token_str).strip() == tokenizer.decode(t).strip()):\n",
    "#                                     print(f'{tokenizer.decode(token_str).strip()} is index {i} in the sentence and {token_str} in the vocabulary.')\n",
    "                    token_vecs_layer = get_layer_token_vecs(token_embeddings, 12)\n",
    "\n",
    "                    # Get the vocab word's contextual embedding for this line.\n",
    "                    tensor_layer = torch.zeros([1, 768])\n",
    "                    for i in range(len(indices)):\n",
    "                        v_index = i % len(v_tokens[1:-1])\n",
    "                        print(f'{tokenizer.decode(v_tokens[v_index + 1]).strip()} at index {indices[i]}: {token_vecs_layer[indices[i]][:5].tolist()}')\n",
    "                        tensor_layer += token_vecs_layer[indices[i]]\n",
    "#                         print(f'Sum of tensors is: {tensor_layer[0][:5].tolist()} before taking the mean.')\n",
    "\n",
    "                    # If our vocab word is broken into more than one token, we need to get the mean of the token embeddings.\n",
    "                    tensor_layer /= len(indices)\n",
    "#                     print(f'Sum of tensors is: {tensor_layer[0][:5].tolist()} after taking the mean.')\n",
    "\n",
    "                    # Add the embedding distilled from this line to the sum of embeddings for all lines.\n",
    "                    v_sum += tensor_layer\n",
    "                    count_tensor += 1\n",
    "                    print(f'Grand sum of {count_tensor} tensor sets is: {v_sum[0][:5].tolist()}')\n",
    "                ###################################################################################\n",
    "            # Stop processing lines once we've found 2000 instances of our vocab word.\n",
    "            if count_tensor >= 2000:\n",
    "                break\n",
    "        \n",
    "        # We're done processing all lines of 512 tokens or less containing our vocab word.\n",
    "        # Get the mean embedding for the word.\n",
    "        v_mean = v_sum / count_tensor\n",
    "        print(f'Mean of tensors is: {v_mean[0][:5]} ({len(v_mean[0])} features in tensor)')\n",
    "        write_embedding(output_file, v, v_mean)\n",
    "        try:\n",
    "            with open(count_file, 'a') as counts:\n",
    "                counts.write(v + ', ' + str(count_tensor) + '\\n')\n",
    "            print(f'Saved the count of sentences used to create {v} embedding')\n",
    "        except:\n",
    "            print('Wha?! Could not write the sentence count.')\n",
    "    end = timer()\n",
    "    print(f'Run time for {v} was {end - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(vocab_file):\n",
    "    vocab = []\n",
    "    with open(vocab_file, 'r') as v:\n",
    "        vocab = v.read().splitlines()\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_token_embeddings(tokenized_text):\n",
    "    input_ids = torch.tensor(tokenized_text).unsqueeze(0)  # Batch size 1\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, masked_lm_labels=input_ids)\n",
    "        encoded_layers = outputs[2]\n",
    "        token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        token_embeddings = token_embeddings.permute(1,0,2)\n",
    "        print(f'Size of token embeddings is {token_embeddings.size()}')\n",
    "        return token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the last 4 layers' features\n",
    "def sum_last_four_token_vecs(token_embeddings):\n",
    "    token_vecs_sum_last_four = []\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "        # `token` is a [13 x 768] tensor\n",
    "        # Sum the vectors from the last 4 layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum_last_four.append(sum_vec)\n",
    "\n",
    "    print ('Shape of summed layers is: %d x %d' % (len(token_vecs_sum_last_four), len(token_vecs_sum_last_four[0])))\n",
    "    # Shape is: <token count> x 768\n",
    "    return token_vecs_sum_last_four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a single layer of the model.\n",
    "def get_layer_token_vecs(token_embeddings, layer_number):\n",
    "    token_vecs_layer = []\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "        # `token` is a [13 x 768] tensor\n",
    "        # Sum the vectors from the last 4 layers.\n",
    "        layer_vec = token[layer_number]\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_layer.append(layer_vec)\n",
    "\n",
    "    print ('Shape of summed layers is: %d x %d' % (len(token_vecs_layer), len(token_vecs_layer[0])))\n",
    "    # Shape is: <token count> x 768\n",
    "    return token_vecs_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_embedding(embeddings_file, vocab_word, contextual_embedding):\n",
    "    try:\n",
    "        with open(embeddings_file, 'a') as f:\n",
    "            f.write(vocab_word)\n",
    "            for value in contextual_embedding[0]:\n",
    "                f.write(' ' + str(value.item()))\n",
    "            f.write('\\n')\n",
    "        print(f'Saved the embedding for {vocab_word}.')\n",
    "    except:\n",
    "        print('Oh no! Unable to write to the embeddings file.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
