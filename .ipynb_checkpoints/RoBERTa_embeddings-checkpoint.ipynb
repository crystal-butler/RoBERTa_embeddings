{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaForMaskedLM, RobertaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/crystal.butler/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure we're in the transformers directory with fine-tuned model output.\n",
    "os.chdir('/Users/crystal.butler/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from the tutorial at https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
    "# and Transformers documentation: https://huggingface.co/transformers/model_doc/roberta.html#robertaformaskedlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('/Users/crystal.butler/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/data/output_wiki-103_filtered')\n",
    "config = RobertaConfig.from_pretrained('/Users/crystal.butler/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/data/output_wiki-103_filtered')\n",
    "model = RobertaForMaskedLM.from_pretrained('/Users/crystal.butler/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/data/output_wiki-103_filtered', config=config)\n",
    "model.eval()\n",
    "\n",
    "context_file = \"/Users/crystal.butler/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/data/wiki.test.raw.out\"\n",
    "output_file = '/Users/crystal.butler/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/data/roberta_test.txt'\n",
    "count_file = '/Users/crystal.butler/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/data/roberta_test_counts.txt'\n",
    "vocab_file = '/Users/crystal.butler/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/data/vocab_checked.txt'\n",
    "vocab = make_vocab(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "aback\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for aback.\n",
      "Saved the count of sentences used to create aback embedding\n",
      "Run time for aback was 0.04021829099974639 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "ab\n",
      "ashed\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for abashed.\n",
      "Saved the count of sentences used to create abashed embedding\n",
      "Run time for abashed was 0.029923640999641066 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "abhor\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for abhor.\n",
      "Saved the count of sentences used to create abhor embedding\n",
      "Run time for abhor was 0.027299575999677472 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "abhor\n",
      "red\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for abhorred.\n",
      "Saved the count of sentences used to create abhorred embedding\n",
      "Run time for abhorred was 0.024258145000203513 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "abhor\n",
      "rence\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for abhorrence.\n",
      "Saved the count of sentences used to create abhorrence embedding\n",
      "Run time for abhorrence was 0.0241971730001751 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "abhor\n",
      "rent\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for abhorrent.\n",
      "Saved the count of sentences used to create abhorrent embedding\n",
      "Run time for abhorrent was 0.02417406300037328 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized text:\n",
      "ab\n",
      "omin\n",
      "able\n",
      "\n",
      "Instance 1 of abominable.\n",
      "Indices are [15, 16]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "ab at index 15: [0.06792555004358292, 0.28369536995887756, 0.05051126331090927, 0.46096566319465637, -0.4600364565849304]\n",
      "omin at index 16: [-0.016813814640045166, 0.19277450442314148, 0.06668046861886978, -0.04752948507666588, 0.9479538202285767]\n",
      "Grand sum of 1 tensor sets is: [0.025555867701768875, 0.23823493719100952, 0.058595865964889526, 0.2067180871963501, 0.24395868182182312]\n",
      "Mean of tensors is: tensor([0.0256, 0.2382, 0.0586, 0.2067, 0.2440]) (768 features in tensor)\n",
      "Saved the embedding for abominable.\n",
      "Saved the count of sentences used to create abominable embedding\n",
      "Run time for abominable was 0.12607579799987434 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "abound\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for abound.\n",
      "Saved the count of sentences used to create abound embedding\n",
      "Run time for abound was 0.028739343999859557 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "absent\n",
      "\n",
      "Instance 1 of absent.\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "absent at index 31: [0.14843827486038208, 0.04582355171442032, 0.17491157352924347, -0.08133866637945175, 0.2885882556438446]\n",
      "Grand sum of 1 tensor sets is: [0.14843827486038208, 0.04582355171442032, 0.17491157352924347, -0.08133866637945175, 0.2885882556438446]\n",
      "\n",
      "Instance 2 of absent.\n",
      "Indices are [16, 37]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "absent at index 16: [-0.02350948005914688, 0.16972917318344116, 0.1736392080783844, -0.15934348106384277, -0.31368789076805115]\n",
      "absent at index 37: [0.01831918954849243, 0.13984918594360352, 0.2514174282550812, -0.13553345203399658, -0.03203746676445007]\n",
      "Grand sum of 2 tensor sets is: [0.14584313333034515, 0.20061272382736206, 0.38743990659713745, -0.22877714037895203, 0.115725576877594]\n",
      "\n",
      "Instance 3 of absent.\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "absent at index 23: [0.07817971706390381, 0.16546979546546936, 0.11813221126794815, -0.15412814915180206, 0.39116743206977844]\n",
      "Grand sum of 3 tensor sets is: [0.22402285039424896, 0.3660825192928314, 0.5055721402168274, -0.3829053044319153, 0.5068930387496948]\n",
      "\n",
      "Instance 4 of absent.\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "absent at index 17: [0.0689462199807167, -0.024369552731513977, 0.09811405092477798, -0.12455013394355774, 0.019996173679828644]\n",
      "Grand sum of 4 tensor sets is: [0.29296907782554626, 0.34171295166015625, 0.6036862134933472, -0.5074554681777954, 0.5268892049789429]\n",
      "Mean of tensors is: tensor([ 0.0732,  0.0854,  0.1509, -0.1269,  0.1317]) (768 features in tensor)\n",
      "Saved the embedding for absent.\n",
      "Saved the count of sentences used to create absent embedding\n",
      "Run time for absent was 0.2749125139998796 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "absorbed\n",
      "\n",
      "Instance 1 of absorbed.\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "absorbed at index 38: [0.16908174753189087, 0.10756009817123413, -0.008513839915394783, 0.27851057052612305, 0.3770231008529663]\n",
      "Grand sum of 1 tensor sets is: [0.16908174753189087, 0.10756009817123413, -0.008513839915394783, 0.27851057052612305, 0.3770231008529663]\n",
      "\n",
      "Instance 2 of absorbed.\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "absorbed at index 30: [0.16352909803390503, 0.09212237596511841, -0.13305869698524475, -0.048667483031749725, 0.7765839695930481]\n",
      "Grand sum of 2 tensor sets is: [0.3326108455657959, 0.19968247413635254, -0.14157253503799438, 0.22984308004379272, 1.1536071300506592]\n",
      "\n",
      "Instance 3 of absorbed.\n",
      "Indices are [45]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "absorbed at index 45: [-0.032996103167533875, 0.3596811294555664, -0.02506312169134617, -0.026045866310596466, -0.07001574337482452]\n",
      "Grand sum of 3 tensor sets is: [0.29961472749710083, 0.559363603591919, -0.166635662317276, 0.20379722118377686, 1.083591341972351]\n",
      "Mean of tensors is: tensor([ 0.0999,  0.1865, -0.0555,  0.0679,  0.3612]) (768 features in tensor)\n",
      "Saved the embedding for absorbed.\n",
      "Saved the count of sentences used to create absorbed embedding\n",
      "Run time for absorbed was 0.25848295100013274 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "acceptance\n",
      "\n",
      "Instance 1 of acceptance.\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "acceptance at index 3: [0.17614762485027313, -0.11515526473522186, 0.18490572273731232, -0.030470168218016624, 0.3363168239593506]\n",
      "Grand sum of 1 tensor sets is: [0.17614762485027313, -0.11515526473522186, 0.18490572273731232, -0.030470168218016624, 0.3363168239593506]\n",
      "\n",
      "Instance 2 of acceptance.\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "acceptance at index 14: [-0.034419864416122437, 0.1038825511932373, 0.1136576384305954, 0.16635248064994812, -0.1867552101612091]\n",
      "Grand sum of 2 tensor sets is: [0.1417277604341507, -0.011272713541984558, 0.2985633611679077, 0.13588231801986694, 0.14956161379814148]\n",
      "Mean of tensors is: tensor([ 0.0709, -0.0056,  0.1493,  0.0679,  0.0748]) (768 features in tensor)\n",
      "Saved the embedding for acceptance.\n",
      "Saved the count of sentences used to create acceptance embedding\n",
      "Run time for acceptance was 0.20949915899973348 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "accepted\n",
      "\n",
      "Instance 1 of accepted.\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "accepted at index 3: [0.31527239084243774, 0.3824237585067749, 0.03503768518567085, 0.3566093444824219, 0.13334782421588898]\n",
      "Grand sum of 1 tensor sets is: [0.31527239084243774, 0.3824237585067749, 0.03503768518567085, 0.3566093444824219, 0.13334782421588898]\n",
      "\n",
      "Instance 2 of accepted.\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "accepted at index 3: [0.07364761084318161, 0.03981919214129448, 0.026067452505230904, 0.1746038794517517, 0.8003053069114685]\n",
      "Grand sum of 2 tensor sets is: [0.38892000913619995, 0.4222429394721985, 0.061105139553546906, 0.5312132239341736, 0.9336531162261963]\n",
      "\n",
      "Instance 3 of accepted.\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "accepted at index 27: [-0.11929482221603394, 0.02767271175980568, -0.13306935131549835, 0.4885920286178589, 0.16952836513519287]\n",
      "Grand sum of 3 tensor sets is: [0.269625186920166, 0.44991564750671387, -0.07196421176195145, 1.0198051929473877, 1.1031814813613892]\n",
      "\n",
      "Instance 4 of accepted.\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "accepted at index 26: [0.1258816421031952, -0.08449521660804749, -0.12492911517620087, 0.5103059411048889, 0.30571746826171875]\n",
      "Grand sum of 4 tensor sets is: [0.3955068290233612, 0.3654204308986664, -0.1968933343887329, 1.5301110744476318, 1.408898949623108]\n",
      "\n",
      "Instance 5 of accepted.\n",
      "Indices are [22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "accepted at index 22: [-0.005839262157678604, 0.0065529122948646545, 0.007053704932332039, 0.2620042860507965, 0.30201423168182373]\n",
      "Grand sum of 5 tensor sets is: [0.3896675705909729, 0.37197333574295044, -0.18983963131904602, 1.792115330696106, 1.7109131813049316]\n",
      "\n",
      "Instance 6 of accepted.\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "accepted at index 19: [0.055956169962882996, 0.10654482245445251, -0.1882839798927307, 0.127366304397583, 0.40761083364486694]\n",
      "Grand sum of 6 tensor sets is: [0.4456237554550171, 0.47851815819740295, -0.37812361121177673, 1.919481635093689, 2.1185240745544434]\n",
      "\n",
      "Instance 7 of accepted.\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "accepted at index 3: [0.04771412909030914, -0.06445986032485962, -0.07482258975505829, 0.20206670463085175, 0.9173025488853455]\n",
      "Grand sum of 7 tensor sets is: [0.49333786964416504, 0.41405829787254333, -0.45294618606567383, 2.1215484142303467, 3.0358266830444336]\n",
      "\n",
      "Instance 8 of accepted.\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "accepted at index 24: [0.15371015667915344, 0.27520525455474854, -0.004592472687363625, -0.0031294722575694323, 0.3256704807281494]\n",
      "Grand sum of 8 tensor sets is: [0.6470479965209961, 0.6892635822296143, -0.4575386643409729, 2.1184189319610596, 3.361497163772583]\n",
      "\n",
      "Instance 9 of accepted.\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "accepted at index 14: [0.19017377495765686, 0.11428150534629822, 0.010414614342153072, 0.42573899030685425, 0.11101701855659485]\n",
      "Grand sum of 9 tensor sets is: [0.8372217416763306, 0.8035451173782349, -0.44712406396865845, 2.5441579818725586, 3.4725141525268555]\n",
      "\n",
      "Instance 10 of accepted.\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "accepted at index 11: [-0.07164224237203598, 0.10343874990940094, -0.07630510628223419, 0.40019911527633667, 0.31786173582077026]\n",
      "Grand sum of 10 tensor sets is: [0.7655795216560364, 0.9069838523864746, -0.5234291553497314, 2.94435715675354, 3.7903759479522705]\n",
      "\n",
      "Instance 11 of accepted.\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "accepted at index 36: [0.25432631373405457, 0.10714972019195557, 0.014403698034584522, 0.1570647805929184, -0.3822193741798401]\n",
      "Grand sum of 11 tensor sets is: [1.0199058055877686, 1.0141335725784302, -0.5090254545211792, 3.10142183303833, 3.408156633377075]\n",
      "\n",
      "Instance 12 of accepted.\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "accepted at index 9: [0.043595749884843826, -0.005929537117481232, -0.12919555604457855, 0.21360057592391968, 0.6784359812736511]\n",
      "Grand sum of 12 tensor sets is: [1.0635015964508057, 1.0082039833068848, -0.638221025466919, 3.3150224685668945, 4.086592674255371]\n",
      "\n",
      "Instance 13 of accepted.\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "accepted at index 23: [0.12998247146606445, -0.027080610394477844, 0.08765701204538345, 0.48577961325645447, -0.010006099939346313]\n",
      "Grand sum of 13 tensor sets is: [1.1934840679168701, 0.9811233878135681, -0.5505639910697937, 3.800801992416382, 4.076586723327637]\n",
      "\n",
      "Instance 14 of accepted.\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "accepted at index 5: [0.05722447484731674, 0.3825082778930664, -0.005771718919277191, 0.2277694195508957, 0.012131556868553162]\n",
      "Grand sum of 14 tensor sets is: [1.2507085800170898, 1.3636317253112793, -0.5563356876373291, 4.028571605682373, 4.088718414306641]\n",
      "\n",
      "Instance 15 of accepted.\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "accepted at index 25: [0.12250278145074844, 0.034740619361400604, 0.027965279296040535, 0.35419175028800964, -0.344702810049057]\n",
      "Grand sum of 15 tensor sets is: [1.37321138381958, 1.3983722925186157, -0.5283703804016113, 4.382763385772705, 3.744015693664551]\n",
      "\n",
      "Instance 16 of accepted.\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "accepted at index 4: [0.030197689309716225, -0.021172717213630676, 0.008647509850561619, 0.5794013142585754, 0.21002572774887085]\n",
      "Grand sum of 16 tensor sets is: [1.4034091234207153, 1.3771995306015015, -0.5197228789329529, 4.962164878845215, 3.9540414810180664]\n",
      "\n",
      "Instance 17 of accepted.\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "accepted at index 28: [-0.02572029083967209, -0.17438963055610657, -0.14242656528949738, 0.2915196418762207, -0.08960045874118805]\n",
      "Grand sum of 17 tensor sets is: [1.3776888847351074, 1.2028099298477173, -0.6621494293212891, 5.2536845207214355, 3.86444091796875]\n",
      "\n",
      "Instance 18 of accepted.\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "accepted at index 12: [0.08536431938409805, 0.05067424476146698, -0.15328656136989594, 0.25065332651138306, 0.39228034019470215]\n",
      "Grand sum of 18 tensor sets is: [1.4630532264709473, 1.2534841299057007, -0.8154360055923462, 5.504337787628174, 4.256721496582031]\n",
      "\n",
      "Instance 19 of accepted.\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "accepted at index 33: [-0.05929044634103775, 0.11495724320411682, -0.11384306848049164, 0.3998710811138153, 0.2771539092063904]\n",
      "Grand sum of 19 tensor sets is: [1.4037628173828125, 1.3684413433074951, -0.929279088973999, 5.904208660125732, 4.533875465393066]\n",
      "Mean of tensors is: tensor([ 0.0739,  0.0720, -0.0489,  0.3107,  0.2386]) (768 features in tensor)\n",
      "Saved the embedding for accepted.\n",
      "Saved the count of sentences used to create accepted embedding\n",
      "Run time for accepted was 1.6501293110000006 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "accepting\n",
      "\n",
      "Instance 1 of accepting.\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "accepting at index 10: [0.24595578014850616, 0.07148412615060806, -0.04664992541074753, 0.30898892879486084, 0.4655201733112335]\n",
      "Grand sum of 1 tensor sets is: [0.24595578014850616, 0.07148412615060806, -0.04664992541074753, 0.30898892879486084, 0.4655201733112335]\n",
      "\n",
      "Instance 2 of accepting.\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "accepting at index 11: [0.02351229265332222, 0.19728612899780273, 0.026657521724700928, 0.20548617839813232, 0.7040669918060303]\n",
      "Grand sum of 2 tensor sets is: [0.2694680690765381, 0.2687702476978302, -0.0199924036860466, 0.5144751071929932, 1.1695871353149414]\n",
      "\n",
      "Instance 3 of accepting.\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "accepting at index 33: [-0.07259106636047363, 0.03504542261362076, -0.028884420171380043, -0.03840100020170212, 0.3502596914768219]\n",
      "Grand sum of 3 tensor sets is: [0.19687700271606445, 0.30381566286087036, -0.048876821994781494, 0.47607409954071045, 1.519846796989441]\n",
      "Mean of tensors is: tensor([ 0.0656,  0.1013, -0.0163,  0.1587,  0.5066]) (768 features in tensor)\n",
      "Saved the embedding for accepting.\n",
      "Saved the count of sentences used to create accepting embedding\n",
      "Run time for accepting was 0.2933862939999017 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "accommodating\n",
      "\n",
      "Instance 1 of accommodating.\n",
      "Indices are [40]\n",
      "Size of token embeddings is torch.Size([74, 13, 768])\n",
      "Shape of summed layers is: 74 x 768\n",
      "accommodating at index 40: [0.16071467101573944, 0.1875777542591095, -0.02297905646264553, 0.18755941092967987, 0.7809291481971741]\n",
      "Grand sum of 1 tensor sets is: [0.16071467101573944, 0.1875777542591095, -0.02297905646264553, 0.18755941092967987, 0.7809291481971741]\n",
      "Mean of tensors is: tensor([ 0.1607,  0.1876, -0.0230,  0.1876,  0.7809]) (768 features in tensor)\n",
      "Saved the embedding for accommodating.\n",
      "Saved the count of sentences used to create accommodating embedding\n",
      "Run time for accommodating was 0.15657807399975354 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "accomplished\n",
      "\n",
      "Instance 1 of accomplished.\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "accomplished at index 18: [-0.04389871656894684, -0.006582438945770264, 0.22913262248039246, -0.1600954830646515, 0.07970038801431656]\n",
      "Grand sum of 1 tensor sets is: [-0.04389871656894684, -0.006582438945770264, 0.22913262248039246, -0.1600954830646515, 0.07970038801431656]\n",
      "\n",
      "Instance 2 of accomplished.\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "accomplished at index 2: [0.13993659615516663, 0.07892926037311554, 0.40447723865509033, -0.07857934385538101, 0.22581759095191956]\n",
      "Grand sum of 2 tensor sets is: [0.09603787958621979, 0.07234682142734528, 0.6336098909378052, -0.2386748194694519, 0.3055179715156555]\n",
      "\n",
      "Instance 3 of accomplished.\n",
      "Indices are [11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "accomplished at index 11: [0.040347304195165634, 0.3676615357398987, 0.15343107283115387, -0.23540496826171875, -0.05082383751869202]\n",
      "Grand sum of 3 tensor sets is: [0.13638518750667572, 0.44000834226608276, 0.7870409488677979, -0.47407978773117065, 0.2546941339969635]\n",
      "Mean of tensors is: tensor([ 0.0455,  0.1467,  0.2623, -0.1580,  0.0849]) (768 features in tensor)\n",
      "Saved the embedding for accomplished.\n",
      "Saved the count of sentences used to create accomplished embedding\n",
      "Run time for accomplished was 0.20214683099993636 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "accord\n",
      "ant\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for accordant.\n",
      "Saved the count of sentences used to create accordant embedding\n",
      "Run time for accordant was 0.029641990999607515 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "acc\n",
      "ursed\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for accursed.\n",
      "Saved the count of sentences used to create accursed embedding\n",
      "Run time for accursed was 0.0260073550002744 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "accus\n",
      "atory\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for accusatory.\n",
      "Saved the count of sentences used to create accusatory embedding\n",
      "Run time for accusatory was 0.02654710700016949 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "accused\n",
      "\n",
      "Instance 1 of accused.\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "accused at index 15: [-0.08472411334514618, -0.28736555576324463, 0.20843791961669922, 0.0966869592666626, 0.04399723559617996]\n",
      "Grand sum of 1 tensor sets is: [-0.08472411334514618, -0.28736555576324463, 0.20843791961669922, 0.0966869592666626, 0.04399723559617996]\n",
      "\n",
      "Instance 2 of accused.\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "accused at index 4: [-0.0013168156147003174, -0.2663661241531372, 0.23553645610809326, 0.10628014802932739, 0.7536068558692932]\n",
      "Grand sum of 2 tensor sets is: [-0.0860409289598465, -0.5537316799163818, 0.4439743757247925, 0.20296710729599, 0.7976040840148926]\n",
      "\n",
      "Instance 3 of accused.\n",
      "Indices are [58]\n",
      "Size of token embeddings is torch.Size([72, 13, 768])\n",
      "Shape of summed layers is: 72 x 768\n",
      "accused at index 58: [-0.2051772177219391, -0.25139403343200684, 0.011400710791349411, -0.05303548276424408, 0.49842119216918945]\n",
      "Grand sum of 3 tensor sets is: [-0.2912181615829468, -0.8051257133483887, 0.455375075340271, 0.1499316245317459, 1.296025276184082]\n",
      "\n",
      "Instance 4 of accused.\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "accused at index 4: [0.07826172560453415, -0.13160057365894318, 0.04041782021522522, 0.20116397738456726, 0.3077247738838196]\n",
      "Grand sum of 4 tensor sets is: [-0.21295642852783203, -0.9367262721061707, 0.4957928955554962, 0.35109561681747437, 1.6037499904632568]\n",
      "\n",
      "Instance 5 of accused.\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "accused at index 20: [0.008823782205581665, -0.3748401999473572, 0.15085813403129578, 0.005297700874507427, -0.06421923637390137]\n",
      "Grand sum of 5 tensor sets is: [-0.20413264632225037, -1.3115664720535278, 0.646651029586792, 0.35639330744743347, 1.5395307540893555]\n",
      "\n",
      "Instance 6 of accused.\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "accused at index 12: [-0.028045237064361572, -0.3713756799697876, 0.20793962478637695, -0.041945356875658035, 0.41213861107826233]\n",
      "Grand sum of 6 tensor sets is: [-0.23217788338661194, -1.6829421520233154, 0.854590654373169, 0.31444793939590454, 1.9516693353652954]\n",
      "\n",
      "Instance 7 of accused.\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "accused at index 10: [-0.07170556485652924, -0.23620320856571198, 0.1663372665643692, 0.0634080246090889, 0.1652279794216156]\n",
      "Grand sum of 7 tensor sets is: [-0.30388343334198, -1.9191453456878662, 1.020927906036377, 0.37785595655441284, 2.1168973445892334]\n",
      "\n",
      "Instance 8 of accused.\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "accused at index 5: [0.02282571792602539, -0.2696911096572876, 0.11522143334150314, -0.139625683426857, 0.019563600420951843]\n",
      "Grand sum of 8 tensor sets is: [-0.2810577154159546, -2.1888365745544434, 1.136149287223816, 0.23823027312755585, 2.136461019515991]\n",
      "Mean of tensors is: tensor([-0.0351, -0.2736,  0.1420,  0.0298,  0.2671]) (768 features in tensor)\n",
      "Saved the embedding for accused.\n",
      "Saved the count of sentences used to create accused embedding\n",
      "Run time for accused was 0.6592034149998653 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "accusing\n",
      "\n",
      "Instance 1 of accusing.\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "accusing at index 17: [-0.06880398094654083, -0.06326080858707428, 0.033858709037303925, -0.06842761486768723, 0.5156391263008118]\n",
      "Grand sum of 1 tensor sets is: [-0.06880398094654083, -0.06326080858707428, 0.033858709037303925, -0.06842761486768723, 0.5156391263008118]\n",
      "Mean of tensors is: tensor([-0.0688, -0.0633,  0.0339, -0.0684,  0.5156]) (768 features in tensor)\n",
      "Saved the embedding for accusing.\n",
      "Saved the count of sentences used to create accusing embedding\n",
      "Run time for accusing was 0.09612278800022978 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized text:\n",
      "ac\n",
      "erb\n",
      "ic\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for acerbic.\n",
      "Saved the count of sentences used to create acerbic embedding\n",
      "Run time for acerbic was 0.03604516799987323 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "acidic\n",
      "\n",
      "Instance 1 of acidic.\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([69, 13, 768])\n",
      "Shape of summed layers is: 69 x 768\n",
      "acidic at index 23: [0.05240260809659958, -0.06946180760860443, -0.03852126747369766, -0.027015866711735725, 0.40639230608940125]\n",
      "Grand sum of 1 tensor sets is: [0.05240260809659958, -0.06946180760860443, -0.03852126747369766, -0.027015866711735725, 0.40639230608940125]\n",
      "Mean of tensors is: tensor([ 0.0524, -0.0695, -0.0385, -0.0270,  0.4064]) (768 features in tensor)\n",
      "Saved the embedding for acidic.\n",
      "Saved the count of sentences used to create acidic embedding\n",
      "Run time for acidic was 0.16998532499974317 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "active\n",
      "\n",
      "Instance 1 of active.\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "active at index 24: [-0.10148297250270844, 0.06392201781272888, 0.0965723916888237, 0.3678937256336212, -0.11603179574012756]\n",
      "Grand sum of 1 tensor sets is: [-0.10148297250270844, 0.06392201781272888, 0.0965723916888237, 0.3678937256336212, -0.11603179574012756]\n",
      "\n",
      "Instance 2 of active.\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "active at index 19: [0.08776158094406128, -0.0004821270704269409, 0.06824815273284912, 0.191207155585289, -0.31917595863342285]\n",
      "Grand sum of 2 tensor sets is: [-0.013721391558647156, 0.06343989074230194, 0.16482055187225342, 0.559100866317749, -0.4352077543735504]\n",
      "\n",
      "Instance 3 of active.\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "active at index 17: [0.017609374597668648, 0.2944364547729492, 0.361152708530426, 0.30739685893058777, -0.7852016091346741]\n",
      "Grand sum of 3 tensor sets is: [0.003887983039021492, 0.35787636041641235, 0.5259732604026794, 0.8664977550506592, -1.2204093933105469]\n",
      "\n",
      "Instance 4 of active.\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "active at index 26: [0.0973002165555954, 0.0032387971878051758, -0.00901215709745884, 0.3552281856536865, -0.35985302925109863]\n",
      "Grand sum of 4 tensor sets is: [0.10118819773197174, 0.36111515760421753, 0.5169610977172852, 1.2217259407043457, -1.5802624225616455]\n",
      "\n",
      "Instance 5 of active.\n",
      "Indices are [9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "active at index 9: [0.18956197798252106, 0.22880248725414276, 0.11391574889421463, 0.5409116744995117, -0.6028144359588623]\n",
      "Grand sum of 5 tensor sets is: [0.2907501757144928, 0.5899176597595215, 0.6308768391609192, 1.7626376152038574, -2.183076858520508]\n",
      "\n",
      "Instance 6 of active.\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "active at index 27: [-0.025129973888397217, -0.08917112648487091, -0.019709086045622826, 0.5450406074523926, -0.842296838760376]\n",
      "Grand sum of 6 tensor sets is: [0.2656202018260956, 0.5007465481758118, 0.6111677289009094, 2.30767822265625, -3.025373697280884]\n",
      "\n",
      "Instance 7 of active.\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "active at index 14: [0.2151871621608734, -0.03998718410730362, 0.057494789361953735, 0.10463743656873703, -0.5762385725975037]\n",
      "Grand sum of 7 tensor sets is: [0.480807363986969, 0.46075937151908875, 0.6686625480651855, 2.412315607070923, -3.6016123294830322]\n",
      "\n",
      "Instance 8 of active.\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "active at index 29: [0.04733189195394516, -0.12996122241020203, 0.03889086842536926, -0.04028075188398361, 0.663524866104126]\n",
      "Grand sum of 8 tensor sets is: [0.5281392335891724, 0.3307981491088867, 0.7075533866882324, 2.372034788131714, -2.9380874633789062]\n",
      "\n",
      "Instance 9 of active.\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "active at index 17: [-0.12019650638103485, -0.2308669537305832, 0.02476239576935768, 0.20746958255767822, -0.7091408371925354]\n",
      "Grand sum of 9 tensor sets is: [0.4079427123069763, 0.09993119537830353, 0.7323157787322998, 2.5795044898986816, -3.647228240966797]\n",
      "\n",
      "Instance 10 of active.\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "active at index 5: [0.005576286464929581, 0.0764036551117897, 0.0006837770342826843, 0.5913102626800537, -0.4659793972969055]\n",
      "Grand sum of 10 tensor sets is: [0.4135189950466156, 0.17633485794067383, 0.7329995632171631, 3.1708147525787354, -4.113207817077637]\n",
      "\n",
      "Instance 11 of active.\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "active at index 38: [0.08234972506761551, -0.03710886836051941, 0.15418215095996857, 0.08560000360012054, -0.6471866965293884]\n",
      "Grand sum of 11 tensor sets is: [0.4958687126636505, 0.13922598958015442, 0.8871816992759705, 3.2564146518707275, -4.76039457321167]\n",
      "\n",
      "Instance 12 of active.\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([60, 13, 768])\n",
      "Shape of summed layers is: 60 x 768\n",
      "active at index 14: [-0.013257201761007309, -0.15667670965194702, 0.19594529271125793, 0.258463054895401, -0.3136965036392212]\n",
      "Grand sum of 12 tensor sets is: [0.4826115071773529, -0.017450720071792603, 1.0831270217895508, 3.5148777961730957, -5.074090957641602]\n",
      "\n",
      "Instance 13 of active.\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "active at index 13: [0.21349862217903137, -0.015965953469276428, 0.18963642418384552, 0.15169523656368256, 0.08317317068576813]\n",
      "Grand sum of 13 tensor sets is: [0.6961101293563843, -0.03341667354106903, 1.2727634906768799, 3.6665730476379395, -4.990917682647705]\n",
      "\n",
      "Instance 14 of active.\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "active at index 19: [0.1213967427611351, -0.03324344754219055, 0.23199748992919922, 0.30310383439064026, -0.5003369450569153]\n",
      "Grand sum of 14 tensor sets is: [0.8175068497657776, -0.06666012108325958, 1.504760980606079, 3.969676971435547, -5.491254806518555]\n",
      "\n",
      "Instance 15 of active.\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "active at index 8: [0.17608051002025604, -0.06387120485305786, -0.023674344643950462, 0.32881563901901245, 0.13299313187599182]\n",
      "Grand sum of 15 tensor sets is: [0.9935873746871948, -0.13053132593631744, 1.4810866117477417, 4.298492431640625, -5.358261585235596]\n",
      "\n",
      "Instance 16 of active.\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "active at index 3: [0.053572703152894974, 0.010863777250051498, 0.10981204360723495, 0.5526849627494812, -0.5628464818000793]\n",
      "Grand sum of 16 tensor sets is: [1.047160029411316, -0.11966754496097565, 1.5908986330032349, 4.851177215576172, -5.921108245849609]\n",
      "\n",
      "Instance 17 of active.\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "active at index 6: [-0.035960614681243896, 0.07013612985610962, 0.017918026074767113, 0.27866247296333313, -0.22501930594444275]\n",
      "Grand sum of 17 tensor sets is: [1.0111994743347168, -0.04953141510486603, 1.6088166236877441, 5.129839897155762, -6.146127700805664]\n",
      "\n",
      "Instance 18 of active.\n",
      "Indices are [3, 12]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "active at index 3: [-0.016649000346660614, 0.04156996309757233, 0.07571525871753693, 0.6398730278015137, -0.4375948905944824]\n",
      "active at index 12: [-0.024205662310123444, -0.10534630715847015, 0.16392068564891815, 0.489614874124527, -0.648101270198822]\n",
      "Grand sum of 18 tensor sets is: [0.9907721281051636, -0.08141958713531494, 1.7286345958709717, 5.694583892822266, -6.688975811004639]\n",
      "\n",
      "Instance 19 of active.\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "active at index 10: [0.051385123282670975, 0.07780876755714417, 0.07646449655294418, 0.5947818756103516, -0.08255429565906525]\n",
      "Grand sum of 19 tensor sets is: [1.0421572923660278, -0.0036108195781707764, 1.8050991296768188, 6.289365768432617, -6.7715301513671875]\n",
      "\n",
      "Instance 20 of active.\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "active at index 4: [0.02248813770711422, -0.08689554035663605, -0.08119618147611618, 0.37943235039711, -0.3275769352912903]\n",
      "Grand sum of 20 tensor sets is: [1.0646454095840454, -0.09050635993480682, 1.723902940750122, 6.668797969818115, -7.099107265472412]\n",
      "\n",
      "Instance 21 of active.\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "active at index 3: [-0.03464864194393158, 0.002550557255744934, 0.026797380298376083, 0.5373083353042603, -0.25723904371261597]\n",
      "Grand sum of 21 tensor sets is: [1.0299967527389526, -0.08795580267906189, 1.7507003545761108, 7.206106185913086, -7.356346130371094]\n",
      "\n",
      "Instance 22 of active.\n",
      "Indices are [32]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "active at index 32: [-0.029190607368946075, 0.25667625665664673, 0.18565872311592102, 0.2579237222671509, -0.016910135746002197]\n",
      "Grand sum of 22 tensor sets is: [1.0008060932159424, 0.16872045397758484, 1.9363590478897095, 7.464029788970947, -7.373256206512451]\n",
      "\n",
      "Instance 23 of active.\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "active at index 22: [-0.04695665091276169, 0.01645827293395996, 0.19837507605552673, 0.4608666002750397, -0.49026280641555786]\n",
      "Grand sum of 23 tensor sets is: [0.9538494348526001, 0.1851787269115448, 2.1347341537475586, 7.924896240234375, -7.863519191741943]\n",
      "Mean of tensors is: tensor([ 0.0415,  0.0081,  0.0928,  0.3446, -0.3419]) (768 features in tensor)\n",
      "Saved the embedding for active.\n",
      "Saved the count of sentences used to create active embedding\n",
      "Run time for active was 1.8238456369999767 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "acute\n",
      "\n",
      "Instance 1 of acute.\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "acute at index 6: [0.39808544516563416, 0.07963380217552185, 0.21332883834838867, 0.7716720104217529, 0.8014234304428101]\n",
      "Grand sum of 1 tensor sets is: [0.39808544516563416, 0.07963380217552185, 0.21332883834838867, 0.7716720104217529, 0.8014234304428101]\n",
      "Mean of tensors is: tensor([0.3981, 0.0796, 0.2133, 0.7717, 0.8014]) (768 features in tensor)\n",
      "Saved the embedding for acute.\n",
      "Saved the count of sentences used to create acute embedding\n",
      "Run time for acute was 0.14413995199993224 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "adamant\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for adamant.\n",
      "Saved the count of sentences used to create adamant embedding\n",
      "Run time for adamant was 0.03210973900013414 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "add\n",
      "led\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for addled.\n",
      "Saved the count of sentences used to create addled embedding\n",
      "Run time for addled was 0.02997281900024973 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "admiration\n",
      "\n",
      "Instance 1 of admiration.\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "admiration at index 9: [0.047818660736083984, -0.046084366738796234, 0.15116649866104126, -0.4402097761631012, -0.13214081525802612]\n",
      "Grand sum of 1 tensor sets is: [0.047818660736083984, -0.046084366738796234, 0.15116649866104126, -0.4402097761631012, -0.13214081525802612]\n",
      "\n",
      "Instance 2 of admiration.\n",
      "Indices are [11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "admiration at index 11: [-0.016715526580810547, -0.2254863828420639, 0.2587202787399292, -0.5055378079414368, -0.2607336938381195]\n",
      "Grand sum of 2 tensor sets is: [0.031103134155273438, -0.27157074213027954, 0.40988677740097046, -0.9457476139068604, -0.39287450909614563]\n",
      "\n",
      "Instance 3 of admiration.\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "admiration at index 16: [-0.15213486552238464, -0.23397298157215118, 0.4559718072414398, -0.32083696126937866, -0.3369424641132355]\n",
      "Grand sum of 3 tensor sets is: [-0.1210317313671112, -0.5055437088012695, 0.8658585548400879, -1.2665846347808838, -0.7298169732093811]\n",
      "Mean of tensors is: tensor([-0.0403, -0.1685,  0.2886, -0.4222, -0.2433]) (768 features in tensor)\n",
      "Saved the embedding for admiration.\n",
      "Saved the count of sentences used to create admiration embedding\n",
      "Run time for admiration was 0.2421841990003486 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "admit\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for admit.\n",
      "Saved the count of sentences used to create admit embedding\n",
      "Run time for admit was 0.030635346000053687 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "ad\n",
      "oration\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for adoration.\n",
      "Saved the count of sentences used to create adoration embedding\n",
      "Run time for adoration was 0.027487318000112282 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "ad\n",
      "oring\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for adoring.\n",
      "Saved the count of sentences used to create adoring embedding\n",
      "Run time for adoring was 0.027720030999716982 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "ad\n",
      "rift\n",
      "\n",
      "Instance 1 of adrift.\n",
      "Indices are [22, 23]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "ad at index 22: [-0.2264108955860138, 0.28767263889312744, -0.021957309916615486, 0.10376035422086716, -0.10914026200771332]\n",
      "rift at index 23: [-0.04530048370361328, -0.031957708299160004, -0.09653664380311966, 0.4133065640926361, 1.3435378074645996]\n",
      "Grand sum of 1 tensor sets is: [-0.13585568964481354, 0.12785746157169342, -0.059246975928545, 0.25853344798088074, 0.6171987652778625]\n",
      "Mean of tensors is: tensor([-0.1359,  0.1279, -0.0592,  0.2585,  0.6172]) (768 features in tensor)\n",
      "Saved the embedding for adrift.\n",
      "Saved the count of sentences used to create adrift embedding\n",
      "Run time for adrift was 0.10484203500027434 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "advers\n",
      "arial\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for adversarial.\n",
      "Saved the count of sentences used to create adversarial embedding\n",
      "Run time for adversarial was 0.02963683999996647 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "aff\n",
      "ability\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for affability.\n",
      "Saved the count of sentences used to create affability embedding\n",
      "Run time for affability was 0.02827710700012176 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "affected\n",
      "\n",
      "Instance 1 of affected.\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "affected at index 19: [-0.004114847630262375, 0.6229269504547119, 0.024000372737646103, 0.5934088230133057, -0.4544891119003296]\n",
      "Grand sum of 1 tensor sets is: [-0.004114847630262375, 0.6229269504547119, 0.024000372737646103, 0.5934088230133057, -0.4544891119003296]\n",
      "\n",
      "Instance 2 of affected.\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "affected at index 11: [0.2560499310493469, 0.5016027688980103, -0.005015883594751358, 0.6992526054382324, -0.012045100331306458]\n",
      "Grand sum of 2 tensor sets is: [0.25193509459495544, 1.1245297193527222, 0.018984489142894745, 1.292661428451538, -0.46653419733047485]\n",
      "\n",
      "Instance 3 of affected.\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "affected at index 10: [-0.23789358139038086, 0.5481199622154236, 0.5174052715301514, 0.34906208515167236, -0.4996485114097595]\n",
      "Grand sum of 3 tensor sets is: [0.014041513204574585, 1.672649621963501, 0.5363897681236267, 1.6417235136032104, -0.9661827087402344]\n",
      "\n",
      "Instance 4 of affected.\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "affected at index 8: [0.21088820695877075, 0.593582272529602, 0.010191100649535656, 0.5257152318954468, -0.24742642045021057]\n",
      "Grand sum of 4 tensor sets is: [0.22492972016334534, 2.2662320137023926, 0.5465808510780334, 2.1674387454986572, -1.2136090993881226]\n",
      "\n",
      "Instance 5 of affected.\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "affected at index 3: [0.08629624545574188, 0.3434506058692932, 0.08586379140615463, 0.7004913091659546, -0.14348086714744568]\n",
      "Grand sum of 5 tensor sets is: [0.311225950717926, 2.609682559967041, 0.6324446201324463, 2.8679299354553223, -1.3570899963378906]\n",
      "\n",
      "Instance 6 of affected.\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "affected at index 5: [0.19279122352600098, 0.5335580706596375, 0.212822824716568, 0.3287631571292877, 0.3169293999671936]\n",
      "Grand sum of 6 tensor sets is: [0.504017174243927, 3.1432406902313232, 0.8452674150466919, 3.196693181991577, -1.0401606559753418]\n",
      "\n",
      "Instance 7 of affected.\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "affected at index 5: [0.2380753457546234, 0.4090193510055542, 0.17236706614494324, 0.48851925134658813, 0.1720593273639679]\n",
      "Grand sum of 7 tensor sets is: [0.742092490196228, 3.552259922027588, 1.0176345109939575, 3.6852123737335205, -0.8681013584136963]\n",
      "\n",
      "Instance 8 of affected.\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "affected at index 23: [0.12588655948638916, 0.47192567586898804, 0.07193931937217712, 0.6647350788116455, 0.21739661693572998]\n",
      "Grand sum of 8 tensor sets is: [0.8679790496826172, 4.024185657501221, 1.089573860168457, 4.349947452545166, -0.6507047414779663]\n",
      "\n",
      "Instance 9 of affected.\n",
      "Indices are [9]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-746268df15f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Indices are {indices}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0;31m# Get the feature vectors for all tokens in the line/sentence.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0mtoken_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_token_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                     \u001b[0;31m# Sum the last four layers to get embeddings for the line/sentence.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m#                         for t in v_tokens[1:-1]:ik\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-70eaa1f8a020>\u001b[0m in \u001b[0;36mcreate_token_embeddings\u001b[0;34m(tokenized_text)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Batch size 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_lm_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mencoded_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtoken_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/venv/lib/python3.7/site-packages/transformers/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, masked_lm_labels)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         )\n\u001b[1;32m    234\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/venv/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m         )\n\u001b[1;32m    738\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/venv/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             layer_outputs = layer_module(\n\u001b[0;32m--> 407\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             )\n\u001b[1;32m    409\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/venv/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add cross attentions if we output attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/venv/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/venv/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/venv/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1612\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1615\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1616\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Process vocabulary words in the outer loop.\n",
    "for v in vocab:\n",
    "    start = timer()\n",
    "    with open(context_file, 'r') as lines:\n",
    "        v_sum = torch.zeros([1, 768])\n",
    "        v_tokens = tokenize_text(v, tokenizer)\n",
    "        print_tokenized_text(v_tokens, tokenizer)\n",
    "        count_sentence = 0\n",
    "        count_tensor = 0\n",
    "        \n",
    "        # Process all lines in the context file in the inner loop.\n",
    "        for line in lines:\n",
    "            # Check for this vocab word in this line; if found, split the line into individual sentences.\n",
    "            if v in line.lower().split():\n",
    "                for sentence in line.split('.'):\n",
    "                    if v in sentence.lower():\n",
    "                        line = sentence\n",
    "                        count_sentence += 1\n",
    "                        print(f'\\nInstance {count_sentence} of {tokenizer.decode(v_tokens[1:-1]).strip()}.')\n",
    "                        break  # We'll take the first instance of the word and discard the rest of the line.\n",
    "                # Split the new sentence-based line into tokens.\n",
    "                line_tokens = tokenize_text(line, tokenizer)\n",
    "                \n",
    "                # Get the indices of the line at which our vocabulary word tokens are located.\n",
    "                indices = get_vocab_indices(v_tokens, line_tokens, tokenizer)                             \n",
    "\n",
    "                # If the vocabulary word was found, process the containing line.\n",
    "                if indices:\n",
    "                    # Get the feature vectors for all tokens in the line/sentence.\n",
    "                    token_embeddings = create_token_embeddings(line_tokens)\n",
    "                    # Sum the last four layers to get embeddings for the line/sentence.\n",
    "#                         for t in v_tokens[1:-1]:ik\n",
    "#                             for i, token_str in enumerate(tokenized_text):\n",
    "#                                 if (tokenizer.decode(token_str).strip() == tokenizer.decode(t).strip()):\n",
    "#                                     print(f'{tokenizer.decode(token_str).strip()} is index {i} in the sentence and {token_str} in the vocabulary.')\n",
    "                    token_vecs_layer = get_layer_token_vecs(token_embeddings, 12)\n",
    "\n",
    "                    # Get the vocab word's contextual embedding for this line.\n",
    "                    tensor_layer = torch.zeros([1, 768])\n",
    "                    for i in range(len(indices)):\n",
    "                        v_index = i % len(v_tokens[1:-1])\n",
    "                        print(f'{tokenizer.decode(v_tokens[v_index + 1]).strip()} at index {indices[i]}: {token_vecs_layer[indices[i]][:5].tolist()}')\n",
    "                        tensor_layer += token_vecs_layer[indices[i]]\n",
    "#                         print(f'Sum of tensors is: {tensor_layer[0][:5].tolist()} before taking the mean.')\n",
    "\n",
    "                    # If our vocab word is broken into more than one token, we need to get the mean of the token embeddings.\n",
    "                    tensor_layer /= len(indices)\n",
    "#                     print(f'Sum of tensors is: {tensor_layer[0][:5].tolist()} after taking the mean.')\n",
    "\n",
    "                    # Add the embedding distilled from this line to the sum of embeddings for all lines.\n",
    "                    v_sum += tensor_layer\n",
    "                    count_tensor += 1\n",
    "                    print(f'Grand sum of {count_tensor} tensor sets is: {v_sum[0][:5].tolist()}')\n",
    "                ###################################################################################\n",
    "            # Stop processing lines once we've found 2000 instances of our vocab word.\n",
    "            if count_tensor >= 2000:\n",
    "                break\n",
    "        \n",
    "        # We're done processing all lines of 512 tokens or less containing our vocab word.\n",
    "        # Get the mean embedding for the word.\n",
    "        v_mean = v_sum / count_tensor\n",
    "        print(f'Mean of tensors is: {v_mean[0][:5]} ({len(v_mean[0])} features in tensor)')\n",
    "        write_embedding(output_file, v, v_mean)\n",
    "        try:\n",
    "            with open(count_file, 'a') as counts:\n",
    "                counts.write(v + ', ' + str(count_tensor) + '\\n')\n",
    "            print(f'Saved the count of sentences used to create {v} embedding')\n",
    "        except:\n",
    "            print('Wha?! Could not write the sentence count.')\n",
    "    end = timer()\n",
    "    print(f'Run time for {v} was {end - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(vocab_file):\n",
    "    \"\"\"Convert a file of newline separated words into a Python list and return it.\"\"\"\n",
    "    vocab = []\n",
    "    with open(vocab_file, 'r') as v:\n",
    "        vocab = v.read().splitlines()\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, tokenizer):\n",
    "    \"\"\"Break the input text into tokens the model can use, and return them.\n",
    "    Use max_length to avoid overflowing the maximum sequence length for the model.\"\"\"\n",
    "    tokenized_text = tokenizer.encode(text, add_special_tokens=True, max_length=512)\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tokenized_text(tokens, tokenizer):\n",
    "    \"\"\"Print the number of tokens in some tokenized text, not counting the leading and trailing separators.\n",
    "    Print each token without any leading or trailing whitespace.\"\"\"\n",
    "    print(f'\\nThere are {len(tokens) - 2} tokens in tokenized text:')\n",
    "    for t in tokens[1:-1]:\n",
    "        print(tokenizer.decode(t).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_indices(v_tokens, line_tokens, tokenizer):\n",
    "    \"\"\"Search a line for all tokens of a vocabulary word, and return the indices of their locations.\"\"\"\n",
    "    indices = []              \n",
    "    for t in v_tokens[1:-1]:\n",
    "        for i, token_str in enumerate(line_tokens):\n",
    "            if tokenizer.decode(token_str).strip() == tokenizer.decode(t).strip():\n",
    "                indices.append(i)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_token_embeddings(tokenized_text):\n",
    "    \n",
    "    input_ids = torch.tensor(tokenized_text).unsqueeze(0)  # Batch size 1\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, masked_lm_labels=input_ids)\n",
    "        encoded_layers = outputs[2]\n",
    "        token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        token_embeddings = token_embeddings.permute(1,0,2)\n",
    "        print(f'Size of token embeddings is {token_embeddings.size()}')\n",
    "        return token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the last 4 layers' features\n",
    "def sum_last_four_token_vecs(token_embeddings):\n",
    "    token_vecs_sum_last_four = []\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "        # `token` is a [13 x 768] tensor\n",
    "        # Sum the vectors from the last 4 layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum_last_four.append(sum_vec)\n",
    "\n",
    "    print ('Shape of summed layers is: %d x %d' % (len(token_vecs_sum_last_four), len(token_vecs_sum_last_four[0])))\n",
    "    # Shape is: <token count> x 768\n",
    "    return token_vecs_sum_last_four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a single layer of the model.\n",
    "def get_layer_token_vecs(token_embeddings, layer_number):\n",
    "    token_vecs_layer = []\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "        # `token` is a [13 x 768] tensor\n",
    "        # Sum the vectors from the last 4 layers.\n",
    "        layer_vec = token[layer_number]\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_layer.append(layer_vec)\n",
    "\n",
    "    print ('Shape of summed layers is: %d x %d' % (len(token_vecs_layer), len(token_vecs_layer[0])))\n",
    "    # Shape is: <token count> x 768\n",
    "    return token_vecs_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_embedding(embeddings_file, vocab_word, contextual_embedding):\n",
    "    try:\n",
    "        with open(embeddings_file, 'a') as f:\n",
    "            f.write(vocab_word)\n",
    "            for value in contextual_embedding[0]:\n",
    "                f.write(' ' + str(value.item()))\n",
    "            f.write('\\n')\n",
    "        print(f'Saved the embedding for {vocab_word}.')\n",
    "    except:\n",
    "        print('Oh no! Unable to write to the embeddings file.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
