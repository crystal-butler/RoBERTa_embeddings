{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaForMaskedLM, RobertaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/crystal.butler/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure we're in the transformers directory with fine-tuned model output.\n",
    "os.chdir('/Users/crystal.butler/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from the tutorial at https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
    "# and Transformers documentation: https://huggingface.co/transformers/model_doc/roberta.html#robertaformaskedlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('/Users/crystal.butler/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/data/output_wiki-103_filtered')\n",
    "config = RobertaConfig.from_pretrained('/Users/crystal.butler/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/data/output_wiki-103_filtered')\n",
    "model = RobertaForMaskedLM.from_pretrained('/Users/crystal.butler/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/data/output_wiki-103_filtered', config=config)\n",
    "model.eval()\n",
    "\n",
    "context_file = \"/Users/crystal.butler/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/data/wiki.test.raw.out\"\n",
    "output_file = '/Users/crystal.butler/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/data/roberta_test.txt'\n",
    "count_file = '/Users/crystal.butler/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/data/roberta_test_counts.txt'\n",
    "vocab_file = '/Users/crystal.butler/Documents/Code_Projects/RoBERTa_Embeddings/RoBERTa_embeddings/data/vocab_checked.txt'\n",
    "vocab = make_vocab(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "aback\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for aback.\n",
      "Saved the count of sentences used to create aback embedding\n",
      "Run time for aback was 0.03236524099975213 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "ab\n",
      "ashed\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for abashed.\n",
      "Saved the count of sentences used to create abashed embedding\n",
      "Run time for abashed was 0.030247872000018106 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "abhor\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for abhor.\n",
      "Saved the count of sentences used to create abhor embedding\n",
      "Run time for abhor was 0.027797740000096383 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "abhor\n",
      "red\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for abhorred.\n",
      "Saved the count of sentences used to create abhorred embedding\n",
      "Run time for abhorred was 0.026862125000207016 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "abhor\n",
      "rence\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for abhorrence.\n",
      "Saved the count of sentences used to create abhorrence embedding\n",
      "Run time for abhorrence was 0.023808746000213432 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "abhor\n",
      "rent\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for abhorrent.\n",
      "Saved the count of sentences used to create abhorrent embedding\n",
      "Run time for abhorrent was 0.024043774000347184 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized text:\n",
      "ab\n",
      "omin\n",
      "able\n",
      "\n",
      "Instance 1 of abominable.\n",
      "Looking for vocab token: ab\n",
      "Looking for vocab token: omin\n",
      "Looking for vocab token: able\n",
      "Indices are [15, 16]\n",
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "ab at index 15: [0.06792555004358292, 0.28369536995887756, 0.05051126331090927, 0.46096566319465637, -0.4600364565849304]\n",
      "omin at index 16: [-0.016813814640045166, 0.19277450442314148, 0.06668046861886978, -0.04752948507666588, 0.9479538202285767]\n",
      "Grand sum of 1 tensor sets is: [0.025555867701768875, 0.23823493719100952, 0.058595865964889526, 0.2067180871963501, 0.24395868182182312]\n",
      "Mean of tensors is: tensor([0.0256, 0.2382, 0.0586, 0.2067, 0.2440]) (768 features in tensor)\n",
      "Saved the embedding for abominable.\n",
      "Saved the count of sentences used to create abominable embedding\n",
      "Run time for abominable was 0.10794239999995625 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "abound\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for abound.\n",
      "Saved the count of sentences used to create abound embedding\n",
      "Run time for abound was 0.04445080400000734 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "absent\n",
      "\n",
      "Instance 1 of absent.\n",
      "Looking for vocab token: absent\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "absent at index 31: [0.14843827486038208, 0.04582355171442032, 0.17491157352924347, -0.08133866637945175, 0.2885882556438446]\n",
      "Grand sum of 1 tensor sets is: [0.14843827486038208, 0.04582355171442032, 0.17491157352924347, -0.08133866637945175, 0.2885882556438446]\n",
      "\n",
      "Instance 2 of absent.\n",
      "Looking for vocab token: absent\n",
      "Indices are [16, 37]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "absent at index 16: [-0.02350948005914688, 0.16972917318344116, 0.1736392080783844, -0.15934348106384277, -0.31368789076805115]\n",
      "absent at index 37: [0.01831918954849243, 0.13984918594360352, 0.2514174282550812, -0.13553345203399658, -0.03203746676445007]\n",
      "Grand sum of 2 tensor sets is: [0.14584313333034515, 0.20061272382736206, 0.38743990659713745, -0.22877714037895203, 0.115725576877594]\n",
      "\n",
      "Instance 3 of absent.\n",
      "Looking for vocab token: absent\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "absent at index 23: [0.07817971706390381, 0.16546979546546936, 0.11813221126794815, -0.15412814915180206, 0.39116743206977844]\n",
      "Grand sum of 3 tensor sets is: [0.22402285039424896, 0.3660825192928314, 0.5055721402168274, -0.3829053044319153, 0.5068930387496948]\n",
      "\n",
      "Instance 4 of absent.\n",
      "Looking for vocab token: absent\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "absent at index 17: [0.0689462199807167, -0.024369552731513977, 0.09811405092477798, -0.12455013394355774, 0.019996173679828644]\n",
      "Grand sum of 4 tensor sets is: [0.29296907782554626, 0.34171295166015625, 0.6036862134933472, -0.5074554681777954, 0.5268892049789429]\n",
      "Mean of tensors is: tensor([ 0.0732,  0.0854,  0.1509, -0.1269,  0.1317]) (768 features in tensor)\n",
      "Saved the embedding for absent.\n",
      "Saved the count of sentences used to create absent embedding\n",
      "Run time for absent was 0.3515433689999554 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "absorbed\n",
      "\n",
      "Instance 1 of absorbed.\n",
      "Looking for vocab token: absorbed\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "absorbed at index 38: [0.16908174753189087, 0.10756009817123413, -0.008513839915394783, 0.27851057052612305, 0.3770231008529663]\n",
      "Grand sum of 1 tensor sets is: [0.16908174753189087, 0.10756009817123413, -0.008513839915394783, 0.27851057052612305, 0.3770231008529663]\n",
      "\n",
      "Instance 2 of absorbed.\n",
      "Looking for vocab token: absorbed\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "absorbed at index 30: [0.16352909803390503, 0.09212237596511841, -0.13305869698524475, -0.048667483031749725, 0.7765839695930481]\n",
      "Grand sum of 2 tensor sets is: [0.3326108455657959, 0.19968247413635254, -0.14157253503799438, 0.22984308004379272, 1.1536071300506592]\n",
      "\n",
      "Instance 3 of absorbed.\n",
      "Looking for vocab token: absorbed\n",
      "Indices are [45]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "absorbed at index 45: [-0.032996103167533875, 0.3596811294555664, -0.02506312169134617, -0.026045866310596466, -0.07001574337482452]\n",
      "Grand sum of 3 tensor sets is: [0.29961472749710083, 0.559363603591919, -0.166635662317276, 0.20379722118377686, 1.083591341972351]\n",
      "Mean of tensors is: tensor([ 0.0999,  0.1865, -0.0555,  0.0679,  0.3612]) (768 features in tensor)\n",
      "Saved the embedding for absorbed.\n",
      "Saved the count of sentences used to create absorbed embedding\n",
      "Run time for absorbed was 0.280776545000208 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "acceptance\n",
      "\n",
      "Instance 1 of acceptance.\n",
      "Looking for vocab token: acceptance\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "acceptance at index 3: [0.17614762485027313, -0.11515526473522186, 0.18490572273731232, -0.030470168218016624, 0.3363168239593506]\n",
      "Grand sum of 1 tensor sets is: [0.17614762485027313, -0.11515526473522186, 0.18490572273731232, -0.030470168218016624, 0.3363168239593506]\n",
      "\n",
      "Instance 2 of acceptance.\n",
      "Looking for vocab token: acceptance\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "acceptance at index 14: [-0.034419864416122437, 0.1038825511932373, 0.1136576384305954, 0.16635248064994812, -0.1867552101612091]\n",
      "Grand sum of 2 tensor sets is: [0.1417277604341507, -0.011272713541984558, 0.2985633611679077, 0.13588231801986694, 0.14956161379814148]\n",
      "Mean of tensors is: tensor([ 0.0709, -0.0056,  0.1493,  0.0679,  0.0748]) (768 features in tensor)\n",
      "Saved the embedding for acceptance.\n",
      "Saved the count of sentences used to create acceptance embedding\n",
      "Run time for acceptance was 0.2533496789997116 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "accepted\n",
      "\n",
      "Instance 1 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "accepted at index 3: [0.31527239084243774, 0.3824237585067749, 0.03503768518567085, 0.3566093444824219, 0.13334782421588898]\n",
      "Grand sum of 1 tensor sets is: [0.31527239084243774, 0.3824237585067749, 0.03503768518567085, 0.3566093444824219, 0.13334782421588898]\n",
      "\n",
      "Instance 2 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "accepted at index 3: [0.07364761084318161, 0.03981919214129448, 0.026067452505230904, 0.1746038794517517, 0.8003053069114685]\n",
      "Grand sum of 2 tensor sets is: [0.38892000913619995, 0.4222429394721985, 0.061105139553546906, 0.5312132239341736, 0.9336531162261963]\n",
      "\n",
      "Instance 3 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "accepted at index 27: [-0.11929482221603394, 0.02767271175980568, -0.13306935131549835, 0.4885920286178589, 0.16952836513519287]\n",
      "Grand sum of 3 tensor sets is: [0.269625186920166, 0.44991564750671387, -0.07196421176195145, 1.0198051929473877, 1.1031814813613892]\n",
      "\n",
      "Instance 4 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "accepted at index 26: [0.1258816421031952, -0.08449521660804749, -0.12492911517620087, 0.5103059411048889, 0.30571746826171875]\n",
      "Grand sum of 4 tensor sets is: [0.3955068290233612, 0.3654204308986664, -0.1968933343887329, 1.5301110744476318, 1.408898949623108]\n",
      "\n",
      "Instance 5 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "accepted at index 22: [-0.005839262157678604, 0.0065529122948646545, 0.007053704932332039, 0.2620042860507965, 0.30201423168182373]\n",
      "Grand sum of 5 tensor sets is: [0.3896675705909729, 0.37197333574295044, -0.18983963131904602, 1.792115330696106, 1.7109131813049316]\n",
      "\n",
      "Instance 6 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "accepted at index 19: [0.055956169962882996, 0.10654482245445251, -0.1882839798927307, 0.127366304397583, 0.40761083364486694]\n",
      "Grand sum of 6 tensor sets is: [0.4456237554550171, 0.47851815819740295, -0.37812361121177673, 1.919481635093689, 2.1185240745544434]\n",
      "\n",
      "Instance 7 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "accepted at index 3: [0.04771412909030914, -0.06445986032485962, -0.07482258975505829, 0.20206670463085175, 0.9173025488853455]\n",
      "Grand sum of 7 tensor sets is: [0.49333786964416504, 0.41405829787254333, -0.45294618606567383, 2.1215484142303467, 3.0358266830444336]\n",
      "\n",
      "Instance 8 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "accepted at index 24: [0.15371015667915344, 0.27520525455474854, -0.004592472687363625, -0.0031294722575694323, 0.3256704807281494]\n",
      "Grand sum of 8 tensor sets is: [0.6470479965209961, 0.6892635822296143, -0.4575386643409729, 2.1184189319610596, 3.361497163772583]\n",
      "\n",
      "Instance 9 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "accepted at index 14: [0.19017377495765686, 0.11428150534629822, 0.010414614342153072, 0.42573899030685425, 0.11101701855659485]\n",
      "Grand sum of 9 tensor sets is: [0.8372217416763306, 0.8035451173782349, -0.44712406396865845, 2.5441579818725586, 3.4725141525268555]\n",
      "\n",
      "Instance 10 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "accepted at index 11: [-0.07164224237203598, 0.10343874990940094, -0.07630510628223419, 0.40019911527633667, 0.31786173582077026]\n",
      "Grand sum of 10 tensor sets is: [0.7655795216560364, 0.9069838523864746, -0.5234291553497314, 2.94435715675354, 3.7903759479522705]\n",
      "\n",
      "Instance 11 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "accepted at index 36: [0.25432631373405457, 0.10714972019195557, 0.014403698034584522, 0.1570647805929184, -0.3822193741798401]\n",
      "Grand sum of 11 tensor sets is: [1.0199058055877686, 1.0141335725784302, -0.5090254545211792, 3.10142183303833, 3.408156633377075]\n",
      "\n",
      "Instance 12 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "accepted at index 9: [0.043595749884843826, -0.005929537117481232, -0.12919555604457855, 0.21360057592391968, 0.6784359812736511]\n",
      "Grand sum of 12 tensor sets is: [1.0635015964508057, 1.0082039833068848, -0.638221025466919, 3.3150224685668945, 4.086592674255371]\n",
      "\n",
      "Instance 13 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "accepted at index 23: [0.12998247146606445, -0.027080610394477844, 0.08765701204538345, 0.48577961325645447, -0.010006099939346313]\n",
      "Grand sum of 13 tensor sets is: [1.1934840679168701, 0.9811233878135681, -0.5505639910697937, 3.800801992416382, 4.076586723327637]\n",
      "\n",
      "Instance 14 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "accepted at index 5: [0.05722447484731674, 0.3825082778930664, -0.005771718919277191, 0.2277694195508957, 0.012131556868553162]\n",
      "Grand sum of 14 tensor sets is: [1.2507085800170898, 1.3636317253112793, -0.5563356876373291, 4.028571605682373, 4.088718414306641]\n",
      "\n",
      "Instance 15 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "accepted at index 25: [0.12250278145074844, 0.034740619361400604, 0.027965279296040535, 0.35419175028800964, -0.344702810049057]\n",
      "Grand sum of 15 tensor sets is: [1.37321138381958, 1.3983722925186157, -0.5283703804016113, 4.382763385772705, 3.744015693664551]\n",
      "\n",
      "Instance 16 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "accepted at index 4: [0.030197689309716225, -0.021172717213630676, 0.008647509850561619, 0.5794013142585754, 0.21002572774887085]\n",
      "Grand sum of 16 tensor sets is: [1.4034091234207153, 1.3771995306015015, -0.5197228789329529, 4.962164878845215, 3.9540414810180664]\n",
      "\n",
      "Instance 17 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "accepted at index 28: [-0.02572029083967209, -0.17438963055610657, -0.14242656528949738, 0.2915196418762207, -0.08960045874118805]\n",
      "Grand sum of 17 tensor sets is: [1.3776888847351074, 1.2028099298477173, -0.6621494293212891, 5.2536845207214355, 3.86444091796875]\n",
      "\n",
      "Instance 18 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "accepted at index 12: [0.08536431938409805, 0.05067424476146698, -0.15328656136989594, 0.25065332651138306, 0.39228034019470215]\n",
      "Grand sum of 18 tensor sets is: [1.4630532264709473, 1.2534841299057007, -0.8154360055923462, 5.504337787628174, 4.256721496582031]\n",
      "\n",
      "Instance 19 of accepted.\n",
      "Looking for vocab token: accepted\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "accepted at index 33: [-0.05929044634103775, 0.11495724320411682, -0.11384306848049164, 0.3998710811138153, 0.2771539092063904]\n",
      "Grand sum of 19 tensor sets is: [1.4037628173828125, 1.3684413433074951, -0.929279088973999, 5.904208660125732, 4.533875465393066]\n",
      "Mean of tensors is: tensor([ 0.0739,  0.0720, -0.0489,  0.3107,  0.2386]) (768 features in tensor)\n",
      "Saved the embedding for accepted.\n",
      "Saved the count of sentences used to create accepted embedding\n",
      "Run time for accepted was 1.343493714000033 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "accepting\n",
      "\n",
      "Instance 1 of accepting.\n",
      "Looking for vocab token: accepting\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "accepting at index 10: [0.24595578014850616, 0.07148412615060806, -0.04664992541074753, 0.30898892879486084, 0.4655201733112335]\n",
      "Grand sum of 1 tensor sets is: [0.24595578014850616, 0.07148412615060806, -0.04664992541074753, 0.30898892879486084, 0.4655201733112335]\n",
      "\n",
      "Instance 2 of accepting.\n",
      "Looking for vocab token: accepting\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "accepting at index 11: [0.02351229265332222, 0.19728612899780273, 0.026657521724700928, 0.20548617839813232, 0.7040669918060303]\n",
      "Grand sum of 2 tensor sets is: [0.2694680690765381, 0.2687702476978302, -0.0199924036860466, 0.5144751071929932, 1.1695871353149414]\n",
      "\n",
      "Instance 3 of accepting.\n",
      "Looking for vocab token: accepting\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "accepting at index 33: [-0.07259106636047363, 0.03504542261362076, -0.028884420171380043, -0.03840100020170212, 0.3502596914768219]\n",
      "Grand sum of 3 tensor sets is: [0.19687700271606445, 0.30381566286087036, -0.048876821994781494, 0.47607409954071045, 1.519846796989441]\n",
      "Mean of tensors is: tensor([ 0.0656,  0.1013, -0.0163,  0.1587,  0.5066]) (768 features in tensor)\n",
      "Saved the embedding for accepting.\n",
      "Saved the count of sentences used to create accepting embedding\n",
      "Run time for accepting was 0.24939480000011827 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "accommodating\n",
      "\n",
      "Instance 1 of accommodating.\n",
      "Looking for vocab token: accommodating\n",
      "Indices are [40]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([74, 13, 768])\n",
      "Shape of summed layers is: 74 x 768\n",
      "accommodating at index 40: [0.16071467101573944, 0.1875777542591095, -0.02297905646264553, 0.18755941092967987, 0.7809291481971741]\n",
      "Grand sum of 1 tensor sets is: [0.16071467101573944, 0.1875777542591095, -0.02297905646264553, 0.18755941092967987, 0.7809291481971741]\n",
      "Mean of tensors is: tensor([ 0.1607,  0.1876, -0.0230,  0.1876,  0.7809]) (768 features in tensor)\n",
      "Saved the embedding for accommodating.\n",
      "Saved the count of sentences used to create accommodating embedding\n",
      "Run time for accommodating was 0.1806203529999948 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "accomplished\n",
      "\n",
      "Instance 1 of accomplished.\n",
      "Looking for vocab token: accomplished\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "accomplished at index 18: [-0.04389871656894684, -0.006582438945770264, 0.22913262248039246, -0.1600954830646515, 0.07970038801431656]\n",
      "Grand sum of 1 tensor sets is: [-0.04389871656894684, -0.006582438945770264, 0.22913262248039246, -0.1600954830646515, 0.07970038801431656]\n",
      "\n",
      "Instance 2 of accomplished.\n",
      "Looking for vocab token: accomplished\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "accomplished at index 2: [0.13993659615516663, 0.07892926037311554, 0.40447723865509033, -0.07857934385538101, 0.22581759095191956]\n",
      "Grand sum of 2 tensor sets is: [0.09603787958621979, 0.07234682142734528, 0.6336098909378052, -0.2386748194694519, 0.3055179715156555]\n",
      "\n",
      "Instance 3 of accomplished.\n",
      "Looking for vocab token: accomplished\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "accomplished at index 11: [0.040347304195165634, 0.3676615357398987, 0.15343107283115387, -0.23540496826171875, -0.05082383751869202]\n",
      "Grand sum of 3 tensor sets is: [0.13638518750667572, 0.44000834226608276, 0.7870409488677979, -0.47407978773117065, 0.2546941339969635]\n",
      "Mean of tensors is: tensor([ 0.0455,  0.1467,  0.2623, -0.1580,  0.0849]) (768 features in tensor)\n",
      "Saved the embedding for accomplished.\n",
      "Saved the count of sentences used to create accomplished embedding\n",
      "Run time for accomplished was 0.2041215999997803 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "accord\n",
      "ant\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for accordant.\n",
      "Saved the count of sentences used to create accordant embedding\n",
      "Run time for accordant was 0.028316181000263896 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "acc\n",
      "ursed\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for accursed.\n",
      "Saved the count of sentences used to create accursed embedding\n",
      "Run time for accursed was 0.024659122000230127 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "accus\n",
      "atory\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for accusatory.\n",
      "Saved the count of sentences used to create accusatory embedding\n",
      "Run time for accusatory was 0.024190658000406984 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "accused\n",
      "\n",
      "Instance 1 of accused.\n",
      "Looking for vocab token: accused\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "accused at index 15: [-0.08472411334514618, -0.28736555576324463, 0.20843791961669922, 0.0966869592666626, 0.04399723559617996]\n",
      "Grand sum of 1 tensor sets is: [-0.08472411334514618, -0.28736555576324463, 0.20843791961669922, 0.0966869592666626, 0.04399723559617996]\n",
      "\n",
      "Instance 2 of accused.\n",
      "Looking for vocab token: accused\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "accused at index 4: [-0.0013168156147003174, -0.2663661241531372, 0.23553645610809326, 0.10628014802932739, 0.7536068558692932]\n",
      "Grand sum of 2 tensor sets is: [-0.0860409289598465, -0.5537316799163818, 0.4439743757247925, 0.20296710729599, 0.7976040840148926]\n",
      "\n",
      "Instance 3 of accused.\n",
      "Looking for vocab token: accused\n",
      "Indices are [58]\n",
      "Size of token embeddings is torch.Size([72, 13, 768])\n",
      "Shape of summed layers is: 72 x 768\n",
      "accused at index 58: [-0.2051772177219391, -0.25139403343200684, 0.011400710791349411, -0.05303548276424408, 0.49842119216918945]\n",
      "Grand sum of 3 tensor sets is: [-0.2912181615829468, -0.8051257133483887, 0.455375075340271, 0.1499316245317459, 1.296025276184082]\n",
      "\n",
      "Instance 4 of accused.\n",
      "Looking for vocab token: accused\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "accused at index 4: [0.07826172560453415, -0.13160057365894318, 0.04041782021522522, 0.20116397738456726, 0.3077247738838196]\n",
      "Grand sum of 4 tensor sets is: [-0.21295642852783203, -0.9367262721061707, 0.4957928955554962, 0.35109561681747437, 1.6037499904632568]\n",
      "\n",
      "Instance 5 of accused.\n",
      "Looking for vocab token: accused\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "accused at index 20: [0.008823782205581665, -0.3748401999473572, 0.15085813403129578, 0.005297700874507427, -0.06421923637390137]\n",
      "Grand sum of 5 tensor sets is: [-0.20413264632225037, -1.3115664720535278, 0.646651029586792, 0.35639330744743347, 1.5395307540893555]\n",
      "\n",
      "Instance 6 of accused.\n",
      "Looking for vocab token: accused\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "accused at index 12: [-0.028045237064361572, -0.3713756799697876, 0.20793962478637695, -0.041945356875658035, 0.41213861107826233]\n",
      "Grand sum of 6 tensor sets is: [-0.23217788338661194, -1.6829421520233154, 0.854590654373169, 0.31444793939590454, 1.9516693353652954]\n",
      "\n",
      "Instance 7 of accused.\n",
      "Looking for vocab token: accused\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "accused at index 10: [-0.07170556485652924, -0.23620320856571198, 0.1663372665643692, 0.0634080246090889, 0.1652279794216156]\n",
      "Grand sum of 7 tensor sets is: [-0.30388343334198, -1.9191453456878662, 1.020927906036377, 0.37785595655441284, 2.1168973445892334]\n",
      "\n",
      "Instance 8 of accused.\n",
      "Looking for vocab token: accused\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "accused at index 5: [0.02282571792602539, -0.2696911096572876, 0.11522143334150314, -0.139625683426857, 0.019563600420951843]\n",
      "Grand sum of 8 tensor sets is: [-0.2810577154159546, -2.1888365745544434, 1.136149287223816, 0.23823027312755585, 2.136461019515991]\n",
      "Mean of tensors is: tensor([-0.0351, -0.2736,  0.1420,  0.0298,  0.2671]) (768 features in tensor)\n",
      "Saved the embedding for accused.\n",
      "Saved the count of sentences used to create accused embedding\n",
      "Run time for accused was 0.5144897330001186 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "accusing\n",
      "\n",
      "Instance 1 of accusing.\n",
      "Looking for vocab token: accusing\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "accusing at index 17: [-0.06880398094654083, -0.06326080858707428, 0.033858709037303925, -0.06842761486768723, 0.5156391263008118]\n",
      "Grand sum of 1 tensor sets is: [-0.06880398094654083, -0.06326080858707428, 0.033858709037303925, -0.06842761486768723, 0.5156391263008118]\n",
      "Mean of tensors is: tensor([-0.0688, -0.0633,  0.0339, -0.0684,  0.5156]) (768 features in tensor)\n",
      "Saved the embedding for accusing.\n",
      "Saved the count of sentences used to create accusing embedding\n",
      "Run time for accusing was 0.08354636099966228 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized text:\n",
      "ac\n",
      "erb\n",
      "ic\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for acerbic.\n",
      "Saved the count of sentences used to create acerbic embedding\n",
      "Run time for acerbic was 0.030019889999948646 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "acidic\n",
      "\n",
      "Instance 1 of acidic.\n",
      "Looking for vocab token: acidic\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([69, 13, 768])\n",
      "Shape of summed layers is: 69 x 768\n",
      "acidic at index 23: [0.05240260809659958, -0.06946180760860443, -0.03852126747369766, -0.027015866711735725, 0.40639230608940125]\n",
      "Grand sum of 1 tensor sets is: [0.05240260809659958, -0.06946180760860443, -0.03852126747369766, -0.027015866711735725, 0.40639230608940125]\n",
      "Mean of tensors is: tensor([ 0.0524, -0.0695, -0.0385, -0.0270,  0.4064]) (768 features in tensor)\n",
      "Saved the embedding for acidic.\n",
      "Saved the count of sentences used to create acidic embedding\n",
      "Run time for acidic was 0.11584446300003037 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "active\n",
      "\n",
      "Instance 1 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "active at index 24: [-0.10148297250270844, 0.06392201781272888, 0.0965723916888237, 0.3678937256336212, -0.11603179574012756]\n",
      "Grand sum of 1 tensor sets is: [-0.10148297250270844, 0.06392201781272888, 0.0965723916888237, 0.3678937256336212, -0.11603179574012756]\n",
      "\n",
      "Instance 2 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "active at index 19: [0.08776158094406128, -0.0004821270704269409, 0.06824815273284912, 0.191207155585289, -0.31917595863342285]\n",
      "Grand sum of 2 tensor sets is: [-0.013721391558647156, 0.06343989074230194, 0.16482055187225342, 0.559100866317749, -0.4352077543735504]\n",
      "\n",
      "Instance 3 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "active at index 17: [0.017609374597668648, 0.2944364547729492, 0.361152708530426, 0.30739685893058777, -0.7852016091346741]\n",
      "Grand sum of 3 tensor sets is: [0.003887983039021492, 0.35787636041641235, 0.5259732604026794, 0.8664977550506592, -1.2204093933105469]\n",
      "\n",
      "Instance 4 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "active at index 26: [0.0973002165555954, 0.0032387971878051758, -0.00901215709745884, 0.3552281856536865, -0.35985302925109863]\n",
      "Grand sum of 4 tensor sets is: [0.10118819773197174, 0.36111515760421753, 0.5169610977172852, 1.2217259407043457, -1.5802624225616455]\n",
      "\n",
      "Instance 5 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "active at index 9: [0.18956197798252106, 0.22880248725414276, 0.11391574889421463, 0.5409116744995117, -0.6028144359588623]\n",
      "Grand sum of 5 tensor sets is: [0.2907501757144928, 0.5899176597595215, 0.6308768391609192, 1.7626376152038574, -2.183076858520508]\n",
      "\n",
      "Instance 6 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "active at index 27: [-0.025129973888397217, -0.08917112648487091, -0.019709086045622826, 0.5450406074523926, -0.842296838760376]\n",
      "Grand sum of 6 tensor sets is: [0.2656202018260956, 0.5007465481758118, 0.6111677289009094, 2.30767822265625, -3.025373697280884]\n",
      "\n",
      "Instance 7 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "active at index 14: [0.2151871621608734, -0.03998718410730362, 0.057494789361953735, 0.10463743656873703, -0.5762385725975037]\n",
      "Grand sum of 7 tensor sets is: [0.480807363986969, 0.46075937151908875, 0.6686625480651855, 2.412315607070923, -3.6016123294830322]\n",
      "\n",
      "Instance 8 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "active at index 29: [0.04733189195394516, -0.12996122241020203, 0.03889086842536926, -0.04028075188398361, 0.663524866104126]\n",
      "Grand sum of 8 tensor sets is: [0.5281392335891724, 0.3307981491088867, 0.7075533866882324, 2.372034788131714, -2.9380874633789062]\n",
      "\n",
      "Instance 9 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "active at index 17: [-0.12019650638103485, -0.2308669537305832, 0.02476239576935768, 0.20746958255767822, -0.7091408371925354]\n",
      "Grand sum of 9 tensor sets is: [0.4079427123069763, 0.09993119537830353, 0.7323157787322998, 2.5795044898986816, -3.647228240966797]\n",
      "\n",
      "Instance 10 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "active at index 5: [0.005576286464929581, 0.0764036551117897, 0.0006837770342826843, 0.5913102626800537, -0.4659793972969055]\n",
      "Grand sum of 10 tensor sets is: [0.4135189950466156, 0.17633485794067383, 0.7329995632171631, 3.1708147525787354, -4.113207817077637]\n",
      "\n",
      "Instance 11 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "active at index 38: [0.08234972506761551, -0.03710886836051941, 0.15418215095996857, 0.08560000360012054, -0.6471866965293884]\n",
      "Grand sum of 11 tensor sets is: [0.4958687126636505, 0.13922598958015442, 0.8871816992759705, 3.2564146518707275, -4.76039457321167]\n",
      "\n",
      "Instance 12 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([60, 13, 768])\n",
      "Shape of summed layers is: 60 x 768\n",
      "active at index 14: [-0.013257201761007309, -0.15667670965194702, 0.19594529271125793, 0.258463054895401, -0.3136965036392212]\n",
      "Grand sum of 12 tensor sets is: [0.4826115071773529, -0.017450720071792603, 1.0831270217895508, 3.5148777961730957, -5.074090957641602]\n",
      "\n",
      "Instance 13 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "active at index 13: [0.21349862217903137, -0.015965953469276428, 0.18963642418384552, 0.15169523656368256, 0.08317317068576813]\n",
      "Grand sum of 13 tensor sets is: [0.6961101293563843, -0.03341667354106903, 1.2727634906768799, 3.6665730476379395, -4.990917682647705]\n",
      "\n",
      "Instance 14 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "active at index 19: [0.1213967427611351, -0.03324344754219055, 0.23199748992919922, 0.30310383439064026, -0.5003369450569153]\n",
      "Grand sum of 14 tensor sets is: [0.8175068497657776, -0.06666012108325958, 1.504760980606079, 3.969676971435547, -5.491254806518555]\n",
      "\n",
      "Instance 15 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "active at index 8: [0.17608051002025604, -0.06387120485305786, -0.023674344643950462, 0.32881563901901245, 0.13299313187599182]\n",
      "Grand sum of 15 tensor sets is: [0.9935873746871948, -0.13053132593631744, 1.4810866117477417, 4.298492431640625, -5.358261585235596]\n",
      "\n",
      "Instance 16 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([19, 13, 768])\n",
      "Shape of summed layers is: 19 x 768\n",
      "active at index 3: [0.053572703152894974, 0.010863777250051498, 0.10981204360723495, 0.5526849627494812, -0.5628464818000793]\n",
      "Grand sum of 16 tensor sets is: [1.047160029411316, -0.11966754496097565, 1.5908986330032349, 4.851177215576172, -5.921108245849609]\n",
      "\n",
      "Instance 17 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([50, 13, 768])\n",
      "Shape of summed layers is: 50 x 768\n",
      "active at index 6: [-0.035960614681243896, 0.07013612985610962, 0.017918026074767113, 0.27866247296333313, -0.22501930594444275]\n",
      "Grand sum of 17 tensor sets is: [1.0111994743347168, -0.04953141510486603, 1.6088166236877441, 5.129839897155762, -6.146127700805664]\n",
      "\n",
      "Instance 18 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [3, 12]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "active at index 3: [-0.016649000346660614, 0.04156996309757233, 0.07571525871753693, 0.6398730278015137, -0.4375948905944824]\n",
      "active at index 12: [-0.024205662310123444, -0.10534630715847015, 0.16392068564891815, 0.489614874124527, -0.648101270198822]\n",
      "Grand sum of 18 tensor sets is: [0.9907721281051636, -0.08141958713531494, 1.7286345958709717, 5.694583892822266, -6.688975811004639]\n",
      "\n",
      "Instance 19 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "active at index 10: [0.051385123282670975, 0.07780876755714417, 0.07646449655294418, 0.5947818756103516, -0.08255429565906525]\n",
      "Grand sum of 19 tensor sets is: [1.0421572923660278, -0.0036108195781707764, 1.8050991296768188, 6.289365768432617, -6.7715301513671875]\n",
      "\n",
      "Instance 20 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "active at index 4: [0.02248813770711422, -0.08689554035663605, -0.08119618147611618, 0.37943235039711, -0.3275769352912903]\n",
      "Grand sum of 20 tensor sets is: [1.0646454095840454, -0.09050635993480682, 1.723902940750122, 6.668797969818115, -7.099107265472412]\n",
      "\n",
      "Instance 21 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "active at index 3: [-0.03464864194393158, 0.002550557255744934, 0.026797380298376083, 0.5373083353042603, -0.25723904371261597]\n",
      "Grand sum of 21 tensor sets is: [1.0299967527389526, -0.08795580267906189, 1.7507003545761108, 7.206106185913086, -7.356346130371094]\n",
      "\n",
      "Instance 22 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "active at index 32: [-0.029190607368946075, 0.25667625665664673, 0.18565872311592102, 0.2579237222671509, -0.016910135746002197]\n",
      "Grand sum of 22 tensor sets is: [1.0008060932159424, 0.16872045397758484, 1.9363590478897095, 7.464029788970947, -7.373256206512451]\n",
      "\n",
      "Instance 23 of active.\n",
      "Looking for vocab token: active\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "active at index 22: [-0.04695665091276169, 0.01645827293395996, 0.19837507605552673, 0.4608666002750397, -0.49026280641555786]\n",
      "Grand sum of 23 tensor sets is: [0.9538494348526001, 0.1851787269115448, 2.1347341537475586, 7.924896240234375, -7.863519191741943]\n",
      "Mean of tensors is: tensor([ 0.0415,  0.0081,  0.0928,  0.3446, -0.3419]) (768 features in tensor)\n",
      "Saved the embedding for active.\n",
      "Saved the count of sentences used to create active embedding\n",
      "Run time for active was 1.4489221190001444 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "acute\n",
      "\n",
      "Instance 1 of acute.\n",
      "Looking for vocab token: acute\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([59, 13, 768])\n",
      "Shape of summed layers is: 59 x 768\n",
      "acute at index 6: [0.39808544516563416, 0.07963380217552185, 0.21332883834838867, 0.7716720104217529, 0.8014234304428101]\n",
      "Grand sum of 1 tensor sets is: [0.39808544516563416, 0.07963380217552185, 0.21332883834838867, 0.7716720104217529, 0.8014234304428101]\n",
      "Mean of tensors is: tensor([0.3981, 0.0796, 0.2133, 0.7717, 0.8014]) (768 features in tensor)\n",
      "Saved the embedding for acute.\n",
      "Saved the count of sentences used to create acute embedding\n",
      "Run time for acute was 0.17259519200024442 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "adamant\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for adamant.\n",
      "Saved the count of sentences used to create adamant embedding\n",
      "Run time for adamant was 0.026731051000297157 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "add\n",
      "led\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for addled.\n",
      "Saved the count of sentences used to create addled embedding\n",
      "Run time for addled was 0.025466219000009005 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "admiration\n",
      "\n",
      "Instance 1 of admiration.\n",
      "Looking for vocab token: admiration\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "admiration at index 9: [0.047818660736083984, -0.046084366738796234, 0.15116649866104126, -0.4402097761631012, -0.13214081525802612]\n",
      "Grand sum of 1 tensor sets is: [0.047818660736083984, -0.046084366738796234, 0.15116649866104126, -0.4402097761631012, -0.13214081525802612]\n",
      "\n",
      "Instance 2 of admiration.\n",
      "Looking for vocab token: admiration\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "admiration at index 11: [-0.016715526580810547, -0.2254863828420639, 0.2587202787399292, -0.5055378079414368, -0.2607336938381195]\n",
      "Grand sum of 2 tensor sets is: [0.031103134155273438, -0.27157074213027954, 0.40988677740097046, -0.9457476139068604, -0.39287450909614563]\n",
      "\n",
      "Instance 3 of admiration.\n",
      "Looking for vocab token: admiration\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "admiration at index 16: [-0.15213486552238464, -0.23397298157215118, 0.4559718072414398, -0.32083696126937866, -0.3369424641132355]\n",
      "Grand sum of 3 tensor sets is: [-0.1210317313671112, -0.5055437088012695, 0.8658585548400879, -1.2665846347808838, -0.7298169732093811]\n",
      "Mean of tensors is: tensor([-0.0403, -0.1685,  0.2886, -0.4222, -0.2433]) (768 features in tensor)\n",
      "Oh no! Unable to write to the embeddings file.\n",
      "Saved the count of sentences used to create admiration embedding\n",
      "Run time for admiration was 0.2169907319998856 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "admit\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for admit.\n",
      "Saved the count of sentences used to create admit embedding\n",
      "Run time for admit was 0.03547595700001693 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "ad\n",
      "oration\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for adoration.\n",
      "Saved the count of sentences used to create adoration embedding\n",
      "Run time for adoration was 0.02722943899971142 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "ad\n",
      "oring\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for adoring.\n",
      "Saved the count of sentences used to create adoring embedding\n",
      "Run time for adoring was 0.02822460500010493 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "ad\n",
      "rift\n",
      "\n",
      "Instance 1 of adrift.\n",
      "Looking for vocab token: ad\n",
      "Looking for vocab token: rift\n",
      "Indices are [22, 23]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "ad at index 22: [-0.2264108955860138, 0.28767263889312744, -0.021957309916615486, 0.10376035422086716, -0.10914026200771332]\n",
      "rift at index 23: [-0.04530048370361328, -0.031957708299160004, -0.09653664380311966, 0.4133065640926361, 1.3435378074645996]\n",
      "Grand sum of 1 tensor sets is: [-0.13585568964481354, 0.12785746157169342, -0.059246975928545, 0.25853344798088074, 0.6171987652778625]\n",
      "Mean of tensors is: tensor([-0.1359,  0.1279, -0.0592,  0.2585,  0.6172]) (768 features in tensor)\n",
      "Saved the embedding for adrift.\n",
      "Saved the count of sentences used to create adrift embedding\n",
      "Run time for adrift was 0.08844169299982241 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "advers\n",
      "arial\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for adversarial.\n",
      "Saved the count of sentences used to create adversarial embedding\n",
      "Run time for adversarial was 0.026034753000203636 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "aff\n",
      "ability\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for affability.\n",
      "Saved the count of sentences used to create affability embedding\n",
      "Run time for affability was 0.024314890999903582 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "affected\n",
      "\n",
      "Instance 1 of affected.\n",
      "Looking for vocab token: affected\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "affected at index 19: [-0.004114847630262375, 0.6229269504547119, 0.024000372737646103, 0.5934088230133057, -0.4544891119003296]\n",
      "Grand sum of 1 tensor sets is: [-0.004114847630262375, 0.6229269504547119, 0.024000372737646103, 0.5934088230133057, -0.4544891119003296]\n",
      "\n",
      "Instance 2 of affected.\n",
      "Looking for vocab token: affected\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "affected at index 11: [0.2560499310493469, 0.5016027688980103, -0.005015883594751358, 0.6992526054382324, -0.012045100331306458]\n",
      "Grand sum of 2 tensor sets is: [0.25193509459495544, 1.1245297193527222, 0.018984489142894745, 1.292661428451538, -0.46653419733047485]\n",
      "\n",
      "Instance 3 of affected.\n",
      "Looking for vocab token: affected\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "affected at index 10: [-0.23789358139038086, 0.5481199622154236, 0.5174052715301514, 0.34906208515167236, -0.4996485114097595]\n",
      "Grand sum of 3 tensor sets is: [0.014041513204574585, 1.672649621963501, 0.5363897681236267, 1.6417235136032104, -0.9661827087402344]\n",
      "\n",
      "Instance 4 of affected.\n",
      "Looking for vocab token: affected\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "affected at index 8: [0.21088820695877075, 0.593582272529602, 0.010191100649535656, 0.5257152318954468, -0.24742642045021057]\n",
      "Grand sum of 4 tensor sets is: [0.22492972016334534, 2.2662320137023926, 0.5465808510780334, 2.1674387454986572, -1.2136090993881226]\n",
      "\n",
      "Instance 5 of affected.\n",
      "Looking for vocab token: affected\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "affected at index 3: [0.08629624545574188, 0.3434506058692932, 0.08586379140615463, 0.7004913091659546, -0.14348086714744568]\n",
      "Grand sum of 5 tensor sets is: [0.311225950717926, 2.609682559967041, 0.6324446201324463, 2.8679299354553223, -1.3570899963378906]\n",
      "\n",
      "Instance 6 of affected.\n",
      "Looking for vocab token: affected\n",
      "Indices are [5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "affected at index 5: [0.19279122352600098, 0.5335580706596375, 0.212822824716568, 0.3287631571292877, 0.3169293999671936]\n",
      "Grand sum of 6 tensor sets is: [0.504017174243927, 3.1432406902313232, 0.8452674150466919, 3.196693181991577, -1.0401606559753418]\n",
      "\n",
      "Instance 7 of affected.\n",
      "Looking for vocab token: affected\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "affected at index 5: [0.2380753457546234, 0.4090193510055542, 0.17236706614494324, 0.48851925134658813, 0.1720593273639679]\n",
      "Grand sum of 7 tensor sets is: [0.742092490196228, 3.552259922027588, 1.0176345109939575, 3.6852123737335205, -0.8681013584136963]\n",
      "\n",
      "Instance 8 of affected.\n",
      "Looking for vocab token: affected\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "affected at index 23: [0.12588655948638916, 0.47192567586898804, 0.07193931937217712, 0.6647350788116455, 0.21739661693572998]\n",
      "Grand sum of 8 tensor sets is: [0.8679790496826172, 4.024185657501221, 1.089573860168457, 4.349947452545166, -0.6507047414779663]\n",
      "\n",
      "Instance 9 of affected.\n",
      "Looking for vocab token: affected\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([54, 13, 768])\n",
      "Shape of summed layers is: 54 x 768\n",
      "affected at index 9: [-0.05033008009195328, 0.21489432454109192, 0.08987255394458771, 0.5326311588287354, 0.17906028032302856]\n",
      "Grand sum of 9 tensor sets is: [0.8176489472389221, 4.23907995223999, 1.1794464588165283, 4.8825788497924805, -0.47164446115493774]\n",
      "\n",
      "Instance 10 of affected.\n",
      "Looking for vocab token: affected\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "affected at index 7: [0.057363126426935196, 0.5202317833900452, 0.11155638098716736, 0.5733896493911743, 0.09103856980800629]\n",
      "Grand sum of 10 tensor sets is: [0.8750120997428894, 4.759311676025391, 1.291002869606018, 5.455968379974365, -0.38060587644577026]\n",
      "\n",
      "Instance 11 of affected.\n",
      "Looking for vocab token: affected\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "affected at index 25: [0.03514000028371811, 0.14788757264614105, 0.07729656994342804, 0.39655354619026184, -0.3930742144584656]\n",
      "Grand sum of 11 tensor sets is: [0.9101520776748657, 4.907199382781982, 1.3682994842529297, 5.852521896362305, -0.7736800909042358]\n",
      "\n",
      "Instance 12 of affected.\n",
      "Looking for vocab token: affected\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "affected at index 16: [0.14838993549346924, 0.5929445028305054, 0.050800152122974396, 0.5316956043243408, -0.378156840801239]\n",
      "Grand sum of 12 tensor sets is: [1.058542013168335, 5.500144004821777, 1.4190996885299683, 6.384217262268066, -1.15183687210083]\n",
      "Mean of tensors is: tensor([ 0.0882,  0.4583,  0.1183,  0.5320, -0.0960]) (768 features in tensor)\n",
      "Saved the embedding for affected.\n",
      "Saved the count of sentences used to create affected embedding\n",
      "Run time for affected was 0.7974734379999973 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "affection\n",
      "ate\n",
      "\n",
      "Instance 1 of affectionate.\n",
      "Looking for vocab token: affection\n",
      "Looking for vocab token: ate\n",
      "Indices are [21, 22]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "affection at index 21: [0.08059604465961456, 0.31559157371520996, 0.2713240385055542, -0.24563291668891907, -1.9019033908843994]\n",
      "ate at index 22: [-0.07819665968418121, 0.16949772834777832, 0.1435145139694214, 0.7502479553222656, 1.0565195083618164]\n",
      "Grand sum of 1 tensor sets is: [0.0011996924877166748, 0.24254465103149414, 0.2074192762374878, 0.2523075342178345, -0.4226919412612915]\n",
      "Mean of tensors is: tensor([ 0.0012,  0.2425,  0.2074,  0.2523, -0.4227]) (768 features in tensor)\n",
      "Saved the embedding for affectionate.\n",
      "Saved the count of sentences used to create affectionate embedding\n",
      "Run time for affectionate was 0.12501106999980038 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "afflicted\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for afflicted.\n",
      "Saved the count of sentences used to create afflicted embedding\n",
      "Run time for afflicted was 0.026561211999705847 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized text:\n",
      "aff\n",
      "ront\n",
      "ed\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for affronted.\n",
      "Saved the count of sentences used to create affronted embedding\n",
      "Run time for affronted was 0.024288343000080204 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized text:\n",
      "a\n",
      "fl\n",
      "utter\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for aflutter.\n",
      "Saved the count of sentences used to create aflutter embedding\n",
      "Run time for aflutter was 0.025924908999968466 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "afraid\n",
      "\n",
      "Instance 1 of afraid.\n",
      "Looking for vocab token: afraid\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "afraid at index 5: [-0.021128512918949127, -0.14597797393798828, 0.4306657314300537, 0.4270578622817993, 1.0973641872406006]\n",
      "Grand sum of 1 tensor sets is: [-0.021128512918949127, -0.14597797393798828, 0.4306657314300537, 0.4270578622817993, 1.0973641872406006]\n",
      "\n",
      "Instance 2 of afraid.\n",
      "Looking for vocab token: afraid\n",
      "Indices are [5, 13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "afraid at index 5: [-0.08513058722019196, -0.29803192615509033, 0.13244567811489105, 0.2129269391298294, 0.9306025505065918]\n",
      "afraid at index 13: [-0.11178436875343323, -0.33336079120635986, 0.18283161520957947, 0.18674224615097046, 1.1694145202636719]\n",
      "Grand sum of 2 tensor sets is: [-0.11958599090576172, -0.4616743326187134, 0.5883044004440308, 0.6268924474716187, 2.1473727226257324]\n",
      "Mean of tensors is: tensor([-0.0598, -0.2308,  0.2942,  0.3134,  1.0737]) (768 features in tensor)\n",
      "Saved the embedding for afraid.\n",
      "Saved the count of sentences used to create afraid embedding\n",
      "Run time for afraid was 0.12271107400010806 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "ag\n",
      "ape\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for agape.\n",
      "Saved the count of sentences used to create agape embedding\n",
      "Run time for agape was 0.028211470000314876 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "aggravated\n",
      "\n",
      "Instance 1 of aggravated.\n",
      "Looking for vocab token: aggravated\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "aggravated at index 5: [-0.04036819189786911, 0.2066463679075241, 0.13810168206691742, 0.02779260277748108, 0.06575316190719604]\n",
      "Grand sum of 1 tensor sets is: [-0.04036819189786911, 0.2066463679075241, 0.13810168206691742, 0.02779260277748108, 0.06575316190719604]\n",
      "Mean of tensors is: tensor([-0.0404,  0.2066,  0.1381,  0.0278,  0.0658]) (768 features in tensor)\n",
      "Saved the embedding for aggravated.\n",
      "Saved the count of sentences used to create aggravated embedding\n",
      "Run time for aggravated was 0.08286535500019454 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "aggrav\n",
      "ation\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for aggravation.\n",
      "Saved the count of sentences used to create aggravation embedding\n",
      "Run time for aggravation was 0.028813135999826045 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "aggression\n",
      "\n",
      "Instance 1 of aggression.\n",
      "Looking for vocab token: aggression\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "aggression at index 6: [-0.002479780465364456, 0.1832132190465927, 0.02853044681251049, 0.0570811852812767, -0.36287781596183777]\n",
      "Grand sum of 1 tensor sets is: [-0.002479780465364456, 0.1832132190465927, 0.02853044681251049, 0.0570811852812767, -0.36287781596183777]\n",
      "Mean of tensors is: tensor([-0.0025,  0.1832,  0.0285,  0.0571, -0.3629]) (768 features in tensor)\n",
      "Saved the embedding for aggression.\n",
      "Saved the count of sentences used to create aggression embedding\n",
      "Run time for aggression was 0.0752520729997741 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "aggressive\n",
      "\n",
      "Instance 1 of aggressive.\n",
      "Looking for vocab token: aggressive\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "aggressive at index 21: [0.09531955420970917, -0.1685708612203598, -0.14524495601654053, 0.7110124230384827, -0.012212276458740234]\n",
      "Grand sum of 1 tensor sets is: [0.09531955420970917, -0.1685708612203598, -0.14524495601654053, 0.7110124230384827, -0.012212276458740234]\n",
      "\n",
      "Instance 2 of aggressive.\n",
      "Looking for vocab token: aggressive\n",
      "Indices are [31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([47, 13, 768])\n",
      "Shape of summed layers is: 47 x 768\n",
      "aggressive at index 31: [0.1439601480960846, 0.08833093196153641, 0.06896568089723587, 0.20601718127727509, 0.21589598059654236]\n",
      "Grand sum of 2 tensor sets is: [0.23927970230579376, -0.0802399292588234, -0.07627927511930466, 0.917029619216919, 0.20368370413780212]\n",
      "\n",
      "Instance 3 of aggressive.\n",
      "Looking for vocab token: aggressive\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "aggressive at index 4: [0.07398677617311478, 0.07459574192762375, 0.0619354322552681, 0.48943108320236206, 0.09881015866994858]\n",
      "Grand sum of 3 tensor sets is: [0.31326648592948914, -0.005644187331199646, -0.01434384286403656, 1.4064607620239258, 0.3024938702583313]\n",
      "\n",
      "Instance 4 of aggressive.\n",
      "Looking for vocab token: aggressive\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "aggressive at index 4: [0.05862955003976822, 0.013850729912519455, -0.031211281195282936, 0.47626039385795593, -0.42292577028274536]\n",
      "Grand sum of 4 tensor sets is: [0.37189602851867676, 0.008206542581319809, -0.04555512219667435, 1.882721185684204, -0.12043190002441406]\n",
      "Mean of tensors is: tensor([ 0.0930,  0.0021, -0.0114,  0.4707, -0.0301]) (768 features in tensor)\n",
      "Saved the embedding for aggressive.\n",
      "Saved the count of sentences used to create aggressive embedding\n",
      "Run time for aggressive was 0.2563975329999266 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "agg\n",
      "rieve\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for aggrieve.\n",
      "Saved the count of sentences used to create aggrieve embedding\n",
      "Run time for aggrieve was 0.029019317999882333 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "agg\n",
      "rieved\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for aggrieved.\n",
      "Saved the count of sentences used to create aggrieved embedding\n",
      "Run time for aggrieved was 0.025641370999892388 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized text:\n",
      "a\n",
      "gh\n",
      "ast\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for aghast.\n",
      "Saved the count of sentences used to create aghast embedding\n",
      "Run time for aghast was 0.02648300900000322 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "agitated\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for agitated.\n",
      "Saved the count of sentences used to create agitated embedding\n",
      "Run time for agitated was 0.02512797800000044 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "ag\n",
      "og\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for agog.\n",
      "Saved the count of sentences used to create agog embedding\n",
      "Run time for agog was 0.024324886000158585 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "agon\n",
      "ized\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for agonized.\n",
      "Saved the count of sentences used to create agonized embedding\n",
      "Run time for agonized was 0.0239882050000233 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "agreeable\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for agreeable.\n",
      "Saved the count of sentences used to create agreeable embedding\n",
      "Run time for agreeable was 0.024035702999753994 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "ag\n",
      "ressive\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for agressive.\n",
      "Saved the count of sentences used to create agressive embedding\n",
      "Run time for agressive was 0.024159587000212923 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "air\n",
      "head\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for airhead.\n",
      "Saved the count of sentences used to create airhead embedding\n",
      "Run time for airhead was 0.028819069999826752 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "alarm\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for alarm.\n",
      "Saved the count of sentences used to create alarm embedding\n",
      "Run time for alarm was 0.028250622000086878 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "alarmed\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for alarmed.\n",
      "Saved the count of sentences used to create alarmed embedding\n",
      "Run time for alarmed was 0.02877853399968444 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "alarming\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for alarming.\n",
      "Saved the count of sentences used to create alarming embedding\n",
      "Run time for alarming was 0.027964651999809575 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "alert\n",
      "\n",
      "Instance 1 of alert.\n",
      "Looking for vocab token: alert\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "alert at index 6: [-0.02452503889799118, -0.30278444290161133, -0.05021539330482483, 0.30885303020477295, 0.12821315228939056]\n",
      "Grand sum of 1 tensor sets is: [-0.02452503889799118, -0.30278444290161133, -0.05021539330482483, 0.30885303020477295, 0.12821315228939056]\n",
      "\n",
      "Instance 2 of alert.\n",
      "Looking for vocab token: alert\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "alert at index 11: [-0.12474393844604492, 0.04805102199316025, -0.15804138779640198, -0.2873021066188812, 0.05259528011083603]\n",
      "Grand sum of 2 tensor sets is: [-0.1492689847946167, -0.2547334134578705, -0.2082567811012268, 0.021550923585891724, 0.180808424949646]\n",
      "Mean of tensors is: tensor([-0.0746, -0.1274, -0.1041,  0.0108,  0.0904]) (768 features in tensor)\n",
      "Saved the embedding for alert.\n",
      "Saved the count of sentences used to create alert embedding\n",
      "Run time for alert was 0.13448608099997728 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "alerted\n",
      "\n",
      "Instance 1 of alerted.\n",
      "Looking for vocab token: alerted\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "alerted at index 9: [0.06697985529899597, -0.03200601786375046, 0.046300001442432404, -0.027864914387464523, 0.2281385362148285]\n",
      "Grand sum of 1 tensor sets is: [0.06697985529899597, -0.03200601786375046, 0.046300001442432404, -0.027864914387464523, 0.2281385362148285]\n",
      "\n",
      "Instance 2 of alerted.\n",
      "Looking for vocab token: alerted\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "alerted at index 2: [0.2878829836845398, -0.1626962423324585, 0.041646890342235565, -0.04731352999806404, 0.6773348450660706]\n",
      "Grand sum of 2 tensor sets is: [0.35486283898353577, -0.19470226764678955, 0.08794689178466797, -0.07517844438552856, 0.9054733514785767]\n",
      "Mean of tensors is: tensor([ 0.1774, -0.0974,  0.0440, -0.0376,  0.4527]) (768 features in tensor)\n",
      "Saved the embedding for alerted.\n",
      "Saved the count of sentences used to create alerted embedding\n",
      "Run time for alerted was 0.1964996029996655 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "alienated\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for alienated.\n",
      "Saved the count of sentences used to create alienated embedding\n",
      "Run time for alienated was 0.0255550349997975 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "allergic\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for allergic.\n",
      "Saved the count of sentences used to create allergic embedding\n",
      "Run time for allergic was 0.024165463999906933 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "allev\n",
      "iated\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for alleviated.\n",
      "Saved the count of sentences used to create alleviated embedding\n",
      "Run time for alleviated was 0.025934514999789826 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "all\n",
      "uring\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for alluring.\n",
      "Saved the count of sentences used to create alluring embedding\n",
      "Run time for alluring was 0.02558855699999185 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "al\n",
      "oof\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for aloof.\n",
      "Saved the count of sentences used to create aloof embedding\n",
      "Run time for aloof was 0.026894547000210878 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "am\n",
      "atory\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for amatory.\n",
      "Saved the count of sentences used to create amatory embedding\n",
      "Run time for amatory was 0.024337275000107184 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "amazed\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for amazed.\n",
      "Saved the count of sentences used to create amazed embedding\n",
      "Run time for amazed was 0.02423787199995786 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "amaz\n",
      "ement\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for amazement.\n",
      "Saved the count of sentences used to create amazement embedding\n",
      "Run time for amazement was 0.02420173500013334 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "amazing\n",
      "\n",
      "Instance 1 of amazing.\n",
      "Looking for vocab token: amazing\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "amazing at index 9: [0.11996796727180481, 0.1440725028514862, 0.02760280855000019, -0.37193062901496887, 0.3257947564125061]\n",
      "Grand sum of 1 tensor sets is: [0.11996796727180481, 0.1440725028514862, 0.02760280855000019, -0.37193062901496887, 0.3257947564125061]\n",
      "\n",
      "Instance 2 of amazing.\n",
      "Looking for vocab token: amazing\n",
      "Indices are [40]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([68, 13, 768])\n",
      "Shape of summed layers is: 68 x 768\n",
      "amazing at index 40: [-0.008208934217691422, 0.3720661401748657, -0.14578741788864136, -0.4101196825504303, 0.34287261962890625]\n",
      "Grand sum of 2 tensor sets is: [0.11175903677940369, 0.5161386728286743, -0.11818461120128632, -0.7820503115653992, 0.6686673760414124]\n",
      "\n",
      "Instance 3 of amazing.\n",
      "Looking for vocab token: amazing\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "amazing at index 13: [-0.07336670905351639, 0.16041766107082367, -0.08369830250740051, -0.24965515732765198, -0.48866844177246094]\n",
      "Grand sum of 3 tensor sets is: [0.0383923277258873, 0.6765563488006592, -0.20188291370868683, -1.0317054986953735, 0.17999893426895142]\n",
      "Mean of tensors is: tensor([ 0.0128,  0.2255, -0.0673, -0.3439,  0.0600]) (768 features in tensor)\n",
      "Saved the embedding for amazing.\n",
      "Saved the count of sentences used to create amazing embedding\n",
      "Run time for amazing was 0.22412933100031296 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "ambition\n",
      "\n",
      "Instance 1 of ambition.\n",
      "Looking for vocab token: ambition\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "ambition at index 3: [-0.06092153489589691, -0.29118484258651733, 0.1267220824956894, 0.21915477514266968, -0.40109068155288696]\n",
      "Grand sum of 1 tensor sets is: [-0.06092153489589691, -0.29118484258651733, 0.1267220824956894, 0.21915477514266968, -0.40109068155288696]\n",
      "\n",
      "Instance 2 of ambition.\n",
      "Looking for vocab token: ambition\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "ambition at index 8: [0.05712300166487694, -0.4759107828140259, 0.3213956952095032, 0.015220381319522858, -0.09168340265750885]\n",
      "Grand sum of 2 tensor sets is: [-0.0037985332310199738, -0.7670956254005432, 0.44811779260635376, 0.23437514901161194, -0.4927740693092346]\n",
      "\n",
      "Instance 3 of ambition.\n",
      "Looking for vocab token: ambition\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "ambition at index 7: [-0.003787245601415634, -0.1687094122171402, 0.19934116303920746, 0.19455231726169586, -0.7462382316589355]\n",
      "Grand sum of 3 tensor sets is: [-0.007585778832435608, -0.9358050227165222, 0.6474589705467224, 0.428927481174469, -1.2390122413635254]\n",
      "Mean of tensors is: tensor([-0.0025, -0.3119,  0.2158,  0.1430, -0.4130]) (768 features in tensor)\n",
      "Saved the embedding for ambition.\n",
      "Saved the count of sentences used to create ambition embedding\n",
      "Run time for ambition was 0.23817432499981805 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "ambitious\n",
      "\n",
      "Instance 1 of ambitious.\n",
      "Looking for vocab token: ambitious\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "ambitious at index 15: [0.17459988594055176, 0.08305463194847107, 0.2250860035419464, 0.15465351939201355, 0.2233530879020691]\n",
      "Grand sum of 1 tensor sets is: [0.17459988594055176, 0.08305463194847107, 0.2250860035419464, 0.15465351939201355, 0.2233530879020691]\n",
      "Mean of tensors is: tensor([0.1746, 0.0831, 0.2251, 0.1547, 0.2234]) (768 features in tensor)\n",
      "Saved the embedding for ambitious.\n",
      "Saved the count of sentences used to create ambitious embedding\n",
      "Run time for ambitious was 0.08963536600003863 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized text:\n",
      "amb\n",
      "ival\n",
      "ence\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for ambivalence.\n",
      "Saved the count of sentences used to create ambivalence embedding\n",
      "Run time for ambivalence was 0.02778498399993623 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "amb\n",
      "ivalent\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for ambivalent.\n",
      "Saved the count of sentences used to create ambivalent embedding\n",
      "Run time for ambivalent was 0.024869585000033112 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "am\n",
      "enable\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for amenable.\n",
      "Saved the count of sentences used to create amenable embedding\n",
      "Run time for amenable was 0.02398159999984273 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "am\n",
      "iable\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for amiable.\n",
      "Saved the count of sentences used to create amiable embedding\n",
      "Run time for amiable was 0.023819562999960908 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "am\n",
      "icable\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for amicable.\n",
      "Saved the count of sentences used to create amicable embedding\n",
      "Run time for amicable was 0.02474415599999702 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "amused\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for amused.\n",
      "Saved the count of sentences used to create amused embedding\n",
      "Run time for amused was 0.023221484999794484 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "amusement\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for amusement.\n",
      "Saved the count of sentences used to create amusement embedding\n",
      "Run time for amusement was 0.02316098100027375 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "analytical\n",
      "\n",
      "Instance 1 of analytical.\n",
      "Looking for vocab token: analytical\n",
      "Indices are [49]\n",
      "Size of token embeddings is torch.Size([55, 13, 768])\n",
      "Shape of summed layers is: 55 x 768\n",
      "analytical at index 49: [0.15147754549980164, 0.125524640083313, 0.064389668405056, 0.21269172430038452, 0.22992467880249023]\n",
      "Grand sum of 1 tensor sets is: [0.15147754549980164, 0.125524640083313, 0.064389668405056, 0.21269172430038452, 0.22992467880249023]\n",
      "Mean of tensors is: tensor([0.1515, 0.1255, 0.0644, 0.2127, 0.2299]) (768 features in tensor)\n",
      "Saved the embedding for analytical.\n",
      "Saved the count of sentences used to create analytical embedding\n",
      "Run time for analytical was 0.10767745900011505 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "analyzing\n",
      "\n",
      "Instance 1 of analyzing.\n",
      "Looking for vocab token: analyzing\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "analyzing at index 7: [0.27910715341567993, 0.2255050092935562, 0.10954922437667847, 0.12491613626480103, 0.6000325679779053]\n",
      "Grand sum of 1 tensor sets is: [0.27910715341567993, 0.2255050092935562, 0.10954922437667847, 0.12491613626480103, 0.6000325679779053]\n",
      "\n",
      "Instance 2 of analyzing.\n",
      "Looking for vocab token: analyzing\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "analyzing at index 19: [0.17776751518249512, 0.25048187375068665, 0.07400267571210861, 0.2684782147407532, 0.7525385618209839]\n",
      "Grand sum of 2 tensor sets is: [0.45687466859817505, 0.47598689794540405, 0.18355190753936768, 0.3933943510055542, 1.3525711297988892]\n",
      "Mean of tensors is: tensor([0.2284, 0.2380, 0.0918, 0.1967, 0.6763]) (768 features in tensor)\n",
      "Saved the embedding for analyzing.\n",
      "Saved the count of sentences used to create analyzing embedding\n",
      "Run time for analyzing was 0.14252179699997214 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "anger\n",
      "\n",
      "Instance 1 of anger.\n",
      "Looking for vocab token: anger\n",
      "Indices are [48]\n",
      "Size of token embeddings is torch.Size([67, 13, 768])\n",
      "Shape of summed layers is: 67 x 768\n",
      "anger at index 48: [-0.025699160993099213, -0.08612427115440369, -0.04524426907300949, 0.3557019829750061, 0.3233538866043091]\n",
      "Grand sum of 1 tensor sets is: [-0.025699160993099213, -0.08612427115440369, -0.04524426907300949, 0.3557019829750061, 0.3233538866043091]\n",
      "\n",
      "Instance 2 of anger.\n",
      "Looking for vocab token: anger\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "anger at index 33: [-0.009449727833271027, -0.2567516565322876, 0.030517050996422768, 0.40600883960723877, -0.04193046689033508]\n",
      "Grand sum of 2 tensor sets is: [-0.03514888882637024, -0.3428759276866913, -0.014727218076586723, 0.7617108225822449, 0.281423419713974]\n",
      "\n",
      "Instance 3 of anger.\n",
      "Looking for vocab token: anger\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "anger at index 5: [0.0523526668548584, -0.056002259254455566, 0.045360639691352844, 0.47759631276130676, -0.4250553846359253]\n",
      "Grand sum of 3 tensor sets is: [0.01720377802848816, -0.39887818694114685, 0.03063342161476612, 1.239307165145874, -0.1436319649219513]\n",
      "\n",
      "Instance 4 of anger.\n",
      "Looking for vocab token: anger\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "anger at index 5: [0.055991336703300476, -0.23053960502147675, 0.006804395467042923, 0.22060292959213257, 0.44504138827323914]\n",
      "Grand sum of 4 tensor sets is: [0.07319511473178864, -0.6294177770614624, 0.03743781894445419, 1.4599101543426514, 0.30140942335128784]\n",
      "\n",
      "Instance 5 of anger.\n",
      "Looking for vocab token: anger\n",
      "Indices are [32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "anger at index 32: [0.11743023246526718, -0.08298443257808685, 0.04215235635638237, 0.1773572415113449, 0.10809770226478577]\n",
      "Grand sum of 5 tensor sets is: [0.19062533974647522, -0.7124022245407104, 0.07959017157554626, 1.6372673511505127, 0.4095071256160736]\n",
      "\n",
      "Instance 6 of anger.\n",
      "Looking for vocab token: anger\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "anger at index 12: [0.1809227019548416, -0.1769171953201294, 0.10280376672744751, 0.189121812582016, -0.22888308763504028]\n",
      "Grand sum of 6 tensor sets is: [0.371548056602478, -0.8893194198608398, 0.18239393830299377, 1.826389193534851, 0.18062403798103333]\n",
      "\n",
      "Instance 7 of anger.\n",
      "Looking for vocab token: anger\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([24, 13, 768])\n",
      "Shape of summed layers is: 24 x 768\n",
      "anger at index 5: [0.003400493413209915, -0.24470971524715424, -0.0028918329626321793, 0.16916929185390472, 0.05786709487438202]\n",
      "Grand sum of 7 tensor sets is: [0.37494856119155884, -1.1340291500091553, 0.17950209975242615, 1.995558500289917, 0.23849113285541534]\n",
      "Mean of tensors is: tensor([ 0.0536, -0.1620,  0.0256,  0.2851,  0.0341]) (768 features in tensor)\n",
      "Saved the embedding for anger.\n",
      "Saved the count of sentences used to create anger embedding\n",
      "Run time for anger was 0.4929710289998184 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "angered\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for angered.\n",
      "Saved the count of sentences used to create angered embedding\n",
      "Run time for angered was 0.028575233000083244 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "angrily\n",
      "\n",
      "Instance 1 of angrily.\n",
      "Looking for vocab token: angrily\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "angrily at index 4: [-0.032908640801906586, 0.06928518414497375, -0.04905584454536438, 0.032097309827804565, 0.5097794532775879]\n",
      "Grand sum of 1 tensor sets is: [-0.032908640801906586, 0.06928518414497375, -0.04905584454536438, 0.032097309827804565, 0.5097794532775879]\n",
      "\n",
      "Instance 2 of angrily.\n",
      "Looking for vocab token: angrily\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "angrily at index 20: [0.03822272643446922, -0.06449541449546814, -0.0017080157995224, -0.18574970960617065, 0.696103572845459]\n",
      "Grand sum of 2 tensor sets is: [0.005314085632562637, 0.004789769649505615, -0.05076386034488678, -0.1536523997783661, 1.2058830261230469]\n",
      "Mean of tensors is: tensor([ 0.0027,  0.0024, -0.0254, -0.0768,  0.6029]) (768 features in tensor)\n",
      "Saved the embedding for angrily.\n",
      "Saved the count of sentences used to create angrily embedding\n",
      "Run time for angrily was 0.14105552999990323 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "angry\n",
      "\n",
      "Instance 1 of angry.\n",
      "Looking for vocab token: angry\n",
      "Indices are [59]\n",
      "Size of token embeddings is torch.Size([67, 13, 768])\n",
      "Shape of summed layers is: 67 x 768\n",
      "angry at index 59: [0.025556271895766258, 0.15035340189933777, 0.1268588900566101, 0.29824262857437134, 0.4213542342185974]\n",
      "Grand sum of 1 tensor sets is: [0.025556271895766258, 0.15035340189933777, 0.1268588900566101, 0.29824262857437134, 0.4213542342185974]\n",
      "\n",
      "Instance 2 of angry.\n",
      "Looking for vocab token: angry\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "angry at index 2: [0.10778298228979111, -0.0431809201836586, 0.14919762313365936, 0.0317004919052124, 0.7220667004585266]\n",
      "Grand sum of 2 tensor sets is: [0.13333925604820251, 0.10717248171567917, 0.27605652809143066, 0.32994312047958374, 1.143420934677124]\n",
      "\n",
      "Instance 3 of angry.\n",
      "Looking for vocab token: angry\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([11, 13, 768])\n",
      "Shape of summed layers is: 11 x 768\n",
      "angry at index 7: [0.11554709821939468, 0.027163542807102203, 0.17652654647827148, 0.26398488879203796, 0.30333977937698364]\n",
      "Grand sum of 3 tensor sets is: [0.2488863468170166, 0.13433602452278137, 0.45258307456970215, 0.5939279794692993, 1.446760654449463]\n",
      "\n",
      "Instance 4 of angry.\n",
      "Looking for vocab token: angry\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "angry at index 11: [0.07962578535079956, 0.06242655590176582, -0.013377612456679344, 0.05090974643826485, 0.5527651309967041]\n",
      "Grand sum of 4 tensor sets is: [0.32851213216781616, 0.1967625766992569, 0.43920546770095825, 0.6448377370834351, 1.999525785446167]\n",
      "\n",
      "Instance 5 of angry.\n",
      "Looking for vocab token: angry\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "angry at index 8: [0.17089807987213135, 0.10320709645748138, 0.3400115668773651, 0.21593666076660156, 0.08231763541698456]\n",
      "Grand sum of 5 tensor sets is: [0.4994102120399475, 0.2999696731567383, 0.779217004776001, 0.8607743978500366, 2.081843376159668]\n",
      "\n",
      "Instance 6 of angry.\n",
      "Looking for vocab token: angry\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "angry at index 11: [0.17164038121700287, -0.04835706204175949, 0.11896181106567383, 0.3060322403907776, 0.685426652431488]\n",
      "Grand sum of 6 tensor sets is: [0.6710506081581116, 0.2516126036643982, 0.8981788158416748, 1.166806697845459, 2.767270088195801]\n",
      "Mean of tensors is: tensor([0.1118, 0.0419, 0.1497, 0.1945, 0.4612]) (768 features in tensor)\n",
      "Saved the embedding for angry.\n",
      "Saved the count of sentences used to create angry embedding\n",
      "Run time for angry was 0.43964089300015985 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "angst\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for angst.\n",
      "Saved the count of sentences used to create angst embedding\n",
      "Run time for angst was 0.027067280000210303 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "anguish\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for anguish.\n",
      "Saved the count of sentences used to create anguish embedding\n",
      "Run time for anguish was 0.024234991999946942 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized text:\n",
      "ang\n",
      "u\n",
      "ished\n",
      "\n",
      "Instance 1 of anguished.\n",
      "Looking for vocab token: ang\n",
      "Looking for vocab token: u\n",
      "Looking for vocab token: ished\n",
      "Indices are [13, 14, 15]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "ang at index 13: [0.16204509139060974, -0.15306690335273743, 0.11153832823038101, 0.4389304220676422, -0.7705284357070923]\n",
      "u at index 14: [0.02222392149269581, 0.004106208682060242, 0.019230298697948456, 0.4784104526042938, -0.5122988820075989]\n",
      "ished at index 15: [0.10499301552772522, 0.1439545899629593, 0.1374567300081253, 0.4508945643901825, 1.1283552646636963]\n",
      "Grand sum of 1 tensor sets is: [0.09642067551612854, -0.0016687015304341912, 0.08940845727920532, 0.45607849955558777, -0.05149070546030998]\n",
      "\n",
      "Instance 2 of anguished.\n",
      "Looking for vocab token: ang\n",
      "Looking for vocab token: u\n",
      "Looking for vocab token: ished\n",
      "Indices are [13, 14, 15]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "ang at index 13: [0.058709532022476196, -0.11416856944561005, 0.1861008107662201, 0.2434139847755432, 0.43944957852363586]\n",
      "u at index 14: [0.0019599944353103638, -0.06468236446380615, 0.013993922621011734, 0.2814396321773529, 0.16379980742931366]\n",
      "ished at index 15: [0.09497621655464172, 0.24031168222427368, 0.05350429192185402, 0.5086886286735535, 0.9371973872184753]\n",
      "Grand sum of 2 tensor sets is: [0.1483025848865509, 0.018818214535713196, 0.1739414632320404, 0.8005925416946411, 0.4619915783405304]\n",
      "Mean of tensors is: tensor([0.0742, 0.0094, 0.0870, 0.4003, 0.2310]) (768 features in tensor)\n",
      "Saved the embedding for anguished.\n",
      "Saved the count of sentences used to create anguished embedding\n",
      "Run time for anguished was 0.13094208899974547 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "animated\n",
      "\n",
      "Instance 1 of animated.\n",
      "Looking for vocab token: animated\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "animated at index 9: [-0.13959121704101562, 0.1737341582775116, 0.024441640824079514, -0.11185142397880554, -0.11011719703674316]\n",
      "Grand sum of 1 tensor sets is: [-0.13959121704101562, 0.1737341582775116, 0.024441640824079514, -0.11185142397880554, -0.11011719703674316]\n",
      "\n",
      "Instance 2 of animated.\n",
      "Looking for vocab token: animated\n",
      "Indices are [33]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "animated at index 33: [-0.1319926530122757, 0.2044759839773178, 0.10342278331518173, -0.150216743350029, 0.12877792119979858]\n",
      "Grand sum of 2 tensor sets is: [-0.2715838551521301, 0.3782101273536682, 0.12786442041397095, -0.26206815242767334, 0.01866072416305542]\n",
      "\n",
      "Instance 3 of animated.\n",
      "Looking for vocab token: animated\n",
      "Indices are [44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([70, 13, 768])\n",
      "Shape of summed layers is: 70 x 768\n",
      "animated at index 44: [0.037575654685497284, 0.059396836906671524, 0.2357286512851715, 0.03179680183529854, -0.04932625591754913]\n",
      "Grand sum of 3 tensor sets is: [-0.23400819301605225, 0.43760696053504944, 0.36359307169914246, -0.2302713543176651, -0.030665531754493713]\n",
      "Mean of tensors is: tensor([-0.0780,  0.1459,  0.1212, -0.0768, -0.0102]) (768 features in tensor)\n",
      "Saved the embedding for animated.\n",
      "Saved the count of sentences used to create animated embedding\n",
      "Run time for animated was 0.30842976500025543 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "animosity\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for animosity.\n",
      "Saved the count of sentences used to create animosity embedding\n",
      "Run time for animosity was 0.031915009999920585 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "annoyance\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for annoyance.\n",
      "Saved the count of sentences used to create annoyance embedding\n",
      "Run time for annoyance was 0.028686132000075304 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "annoyed\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for annoyed.\n",
      "Saved the count of sentences used to create annoyed embedding\n",
      "Run time for annoyed was 0.027757371999996394 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "annoying\n",
      "\n",
      "Instance 1 of annoying.\n",
      "Looking for vocab token: annoying\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "annoying at index 11: [0.10794611275196075, 0.3627992272377014, 0.12529073655605316, 0.10958881676197052, 0.2889607846736908]\n",
      "Grand sum of 1 tensor sets is: [0.10794611275196075, 0.3627992272377014, 0.12529073655605316, 0.10958881676197052, 0.2889607846736908]\n",
      "Mean of tensors is: tensor([0.1079, 0.3628, 0.1253, 0.1096, 0.2890]) (768 features in tensor)\n",
      "Saved the embedding for annoying.\n",
      "Saved the count of sentences used to create annoying embedding\n",
      "Run time for annoying was 0.10770184300008623 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "antagon\n",
      "istic\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for antagonistic.\n",
      "Saved the count of sentences used to create antagonistic embedding\n",
      "Run time for antagonistic was 0.036262209999677 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "antagon\n",
      "ized\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for antagonized.\n",
      "Saved the count of sentences used to create antagonized embedding\n",
      "Run time for antagonized was 0.030298803999812662 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "anticipated\n",
      "\n",
      "Instance 1 of anticipated.\n",
      "Looking for vocab token: anticipated\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "anticipated at index 21: [-0.0014907903969287872, -0.12229287624359131, 0.07041046768426895, 0.026940083131194115, 0.6992916464805603]\n",
      "Grand sum of 1 tensor sets is: [-0.0014907903969287872, -0.12229287624359131, 0.07041046768426895, 0.026940083131194115, 0.6992916464805603]\n",
      "\n",
      "Instance 2 of anticipated.\n",
      "Looking for vocab token: anticipated\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "anticipated at index 21: [0.2141578495502472, 0.24588459730148315, 0.09696508944034576, 0.3095882833003998, -0.140160471200943]\n",
      "Grand sum of 2 tensor sets is: [0.2126670628786087, 0.12359172105789185, 0.1673755645751953, 0.33652836084365845, 0.5591311454772949]\n",
      "\n",
      "Instance 3 of anticipated.\n",
      "Looking for vocab token: anticipated\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "anticipated at index 14: [0.0822591632604599, 0.12259364873170853, -0.007730439305305481, -0.18174125254154205, -0.10468657314777374]\n",
      "Grand sum of 3 tensor sets is: [0.2949262261390686, 0.24618536233901978, 0.15964512526988983, 0.1547871083021164, 0.4544445872306824]\n",
      "\n",
      "Instance 4 of anticipated.\n",
      "Looking for vocab token: anticipated\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "anticipated at index 13: [0.12619692087173462, 0.03008950501680374, -0.01456332765519619, -0.2056160867214203, 0.40256017446517944]\n",
      "Grand sum of 4 tensor sets is: [0.4211231470108032, 0.2762748599052429, 0.1450818032026291, -0.050828978419303894, 0.8570047616958618]\n",
      "\n",
      "Instance 5 of anticipated.\n",
      "Looking for vocab token: anticipated\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "anticipated at index 3: [0.1059303879737854, -0.13581211864948273, 0.30717161297798157, 0.16465668380260468, 0.13923588395118713]\n",
      "Grand sum of 5 tensor sets is: [0.5270535349845886, 0.1404627412557602, 0.45225340127944946, 0.11382770538330078, 0.9962406158447266]\n",
      "\n",
      "Instance 6 of anticipated.\n",
      "Looking for vocab token: anticipated\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "anticipated at index 8: [-0.06317673623561859, 0.08525488525629044, -0.017199682071805, -0.006035104393959045, -0.4241142272949219]\n",
      "Grand sum of 6 tensor sets is: [0.46387678384780884, 0.22571763396263123, 0.4350537061691284, 0.10779260098934174, 0.5721263885498047]\n",
      "\n",
      "Instance 7 of anticipated.\n",
      "Looking for vocab token: anticipated\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "anticipated at index 10: [-0.04275298863649368, -0.03486596792936325, 0.26590293645858765, -0.16161994636058807, -0.0871654748916626]\n",
      "Grand sum of 7 tensor sets is: [0.42112380266189575, 0.19085165858268738, 0.7009566426277161, -0.05382734537124634, 0.4849609136581421]\n",
      "Mean of tensors is: tensor([ 0.0602,  0.0273,  0.1001, -0.0077,  0.0693]) (768 features in tensor)\n",
      "Saved the embedding for anticipated.\n",
      "Saved the count of sentences used to create anticipated embedding\n",
      "Run time for anticipated was 0.5560142199997244 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "anticipating\n",
      "\n",
      "Instance 1 of anticipating.\n",
      "Looking for vocab token: anticipating\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "anticipating at index 24: [0.10652883350849152, 0.1959572583436966, 0.07325226068496704, 0.15284724533557892, 0.7273950576782227]\n",
      "Grand sum of 1 tensor sets is: [0.10652883350849152, 0.1959572583436966, 0.07325226068496704, 0.15284724533557892, 0.7273950576782227]\n",
      "\n",
      "Instance 2 of anticipating.\n",
      "Looking for vocab token: anticipating\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "anticipating at index 29: [0.1753004789352417, 0.026505619287490845, -0.04865042865276337, 0.3018667995929718, 0.23644757270812988]\n",
      "Grand sum of 2 tensor sets is: [0.281829297542572, 0.22246287763118744, 0.024601832032203674, 0.4547140598297119, 0.9638426303863525]\n",
      "Mean of tensors is: tensor([0.1409, 0.1112, 0.0123, 0.2274, 0.4819]) (768 features in tensor)\n",
      "Saved the embedding for anticipating.\n",
      "Saved the count of sentences used to create anticipating embedding\n",
      "Run time for anticipating was 0.22868503399968176 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "anticipation\n",
      "\n",
      "Instance 1 of anticipation.\n",
      "Looking for vocab token: anticipation\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "anticipation at index 12: [0.1374605894088745, -0.046881601214408875, -0.11937476694583893, 0.06743603199720383, -0.5062006711959839]\n",
      "Grand sum of 1 tensor sets is: [0.1374605894088745, -0.046881601214408875, -0.11937476694583893, 0.06743603199720383, -0.5062006711959839]\n",
      "\n",
      "Instance 2 of anticipation.\n",
      "Looking for vocab token: anticipation\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "anticipation at index 14: [-0.03849043697118759, 0.01344374567270279, 0.2764108180999756, 0.0591999813914299, -0.20163637399673462]\n",
      "Grand sum of 2 tensor sets is: [0.09897015243768692, -0.033437855541706085, 0.15703605115413666, 0.12663601338863373, -0.7078370451927185]\n",
      "Mean of tensors is: tensor([ 0.0495, -0.0167,  0.0785,  0.0633, -0.3539]) (768 features in tensor)\n",
      "Saved the embedding for anticipation.\n",
      "Saved the count of sentences used to create anticipation embedding\n",
      "Run time for anticipation was 0.18521189299963225 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "anticip\n",
      "ative\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for anticipative.\n",
      "Saved the count of sentences used to create anticipative embedding\n",
      "Run time for anticipative was 0.02925774499999534 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "anticip\n",
      "atory\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for anticipatory.\n",
      "Saved the count of sentences used to create anticipatory embedding\n",
      "Run time for anticipatory was 0.025433883000005153 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "antip\n",
      "athy\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for antipathy.\n",
      "Saved the count of sentences used to create antipathy embedding\n",
      "Run time for antipathy was 0.02430273199979638 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "ants\n",
      "y\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for antsy.\n",
      "Saved the count of sentences used to create antsy embedding\n",
      "Run time for antsy was 0.0241050150002593 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "anxiety\n",
      "\n",
      "Instance 1 of anxiety.\n",
      "Looking for vocab token: anxiety\n",
      "Indices are [3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "anxiety at index 3: [0.16283810138702393, -0.16517367959022522, 0.15017762780189514, 0.06005309894680977, -0.16442930698394775]\n",
      "Grand sum of 1 tensor sets is: [0.16283810138702393, -0.16517367959022522, 0.15017762780189514, 0.06005309894680977, -0.16442930698394775]\n",
      "\n",
      "Instance 2 of anxiety.\n",
      "Looking for vocab token: anxiety\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "anxiety at index 36: [0.035467375069856644, 0.31055915355682373, 0.4142957627773285, -0.11021118611097336, 0.41283679008483887]\n",
      "Grand sum of 2 tensor sets is: [0.19830547273159027, 0.1453854739665985, 0.5644733905792236, -0.05015808716416359, 0.2484074831008911]\n",
      "Mean of tensors is: tensor([ 0.0992,  0.0727,  0.2822, -0.0251,  0.1242]) (768 features in tensor)\n",
      "Saved the embedding for anxiety.\n",
      "Saved the count of sentences used to create anxiety embedding\n",
      "Run time for anxiety was 0.14421112399986669 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "anxious\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for anxious.\n",
      "Saved the count of sentences used to create anxious embedding\n",
      "Run time for anxious was 0.027206472999750986 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "anx\n",
      "iously\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for anxiously.\n",
      "Saved the count of sentences used to create anxiously embedding\n",
      "Run time for anxiously was 0.02439312600017729 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "ap\n",
      "athetic\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for apathetic.\n",
      "Saved the count of sentences used to create apathetic embedding\n",
      "Run time for apathetic was 0.02438134700014416 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "ap\n",
      "athy\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for apathy.\n",
      "Saved the count of sentences used to create apathy embedding\n",
      "Run time for apathy was 0.024328848000095604 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "apolog\n",
      "etic\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for apologetic.\n",
      "Saved the count of sentences used to create apologetic embedding\n",
      "Run time for apologetic was 0.026163779999933467 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "appalled\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for appalled.\n",
      "Saved the count of sentences used to create appalled embedding\n",
      "Run time for appalled was 0.027184827999917616 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized text:\n",
      "app\n",
      "all\n",
      "ingly\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for appallingly.\n",
      "Saved the count of sentences used to create appallingly embedding\n",
      "Run time for appallingly was 0.025359719999869412 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "appe\n",
      "ased\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for appeased.\n",
      "Saved the count of sentences used to create appeased embedding\n",
      "Run time for appeased was 0.02556962200014823 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "appe\n",
      "asing\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for appeasing.\n",
      "Saved the count of sentences used to create appeasing embedding\n",
      "Run time for appeasing was 0.0275599130000046 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "appreci\n",
      "ative\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for appreciative.\n",
      "Saved the count of sentences used to create appreciative embedding\n",
      "Run time for appreciative was 0.02712804100019639 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "apprehension\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for apprehension.\n",
      "Saved the count of sentences used to create apprehension embedding\n",
      "Run time for apprehension was 0.02723940699979721 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "apprehens\n",
      "ive\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for apprehensive.\n",
      "Saved the count of sentences used to create apprehensive embedding\n",
      "Run time for apprehensive was 0.027618724999683764 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "approve\n",
      "\n",
      "Instance 1 of approve.\n",
      "Looking for vocab token: approve\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([38, 13, 768])\n",
      "Shape of summed layers is: 38 x 768\n",
      "approve at index 28: [0.01732192561030388, 0.4021766781806946, -0.062369994819164276, -0.055501632392406464, -0.5963248014450073]\n",
      "Grand sum of 1 tensor sets is: [0.01732192561030388, 0.4021766781806946, -0.062369994819164276, -0.055501632392406464, -0.5963248014450073]\n",
      "Mean of tensors is: tensor([ 0.0173,  0.4022, -0.0624, -0.0555, -0.5963]) (768 features in tensor)\n",
      "Saved the embedding for approve.\n",
      "Saved the count of sentences used to create approve embedding\n",
      "Run time for approve was 0.10723723799992513 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "approved\n",
      "\n",
      "Instance 1 of approved.\n",
      "Looking for vocab token: approved\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "approved at index 24: [0.14637750387191772, 0.007387086749076843, 0.10743226855993271, 0.3400673270225525, -0.5126274228096008]\n",
      "Grand sum of 1 tensor sets is: [0.14637750387191772, 0.007387086749076843, 0.10743226855993271, 0.3400673270225525, -0.5126274228096008]\n",
      "\n",
      "Instance 2 of approved.\n",
      "Looking for vocab token: approved\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "approved at index 4: [-0.0877799391746521, 0.030479885637760162, -0.04717652499675751, 0.5669403076171875, -0.0033235102891921997]\n",
      "Grand sum of 2 tensor sets is: [0.058597564697265625, 0.037866972386837006, 0.0602557435631752, 0.90700763463974, -0.5159509181976318]\n",
      "\n",
      "Instance 3 of approved.\n",
      "Looking for vocab token: approved\n",
      "Indices are [38]\n",
      "Size of token embeddings is torch.Size([60, 13, 768])\n",
      "Shape of summed layers is: 60 x 768\n",
      "approved at index 38: [-0.09312461316585541, 0.3032228350639343, -0.07409601658582687, 0.5928658246994019, 0.05345563590526581]\n",
      "Grand sum of 3 tensor sets is: [-0.03452704846858978, 0.34108981490135193, -0.013840273022651672, 1.499873399734497, -0.46249526739120483]\n",
      "\n",
      "Instance 4 of approved.\n",
      "Looking for vocab token: approved\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "approved at index 31: [0.08798334002494812, 0.18263523280620575, 0.06846115738153458, 0.32792046666145325, -0.187473326921463]\n",
      "Grand sum of 4 tensor sets is: [0.05345629155635834, 0.5237250328063965, 0.054620884358882904, 1.827793836593628, -0.6499686241149902]\n",
      "\n",
      "Instance 5 of approved.\n",
      "Looking for vocab token: approved\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([15, 13, 768])\n",
      "Shape of summed layers is: 15 x 768\n",
      "approved at index 5: [0.053949564695358276, 0.3826545476913452, -0.10893817245960236, 0.27703729271888733, -0.42938655614852905]\n",
      "Grand sum of 5 tensor sets is: [0.10740585625171661, 0.9063795804977417, -0.05431728810071945, 2.1048312187194824, -1.079355239868164]\n",
      "\n",
      "Instance 6 of approved.\n",
      "Looking for vocab token: approved\n",
      "Indices are [5]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "approved at index 5: [-0.08021722733974457, 0.059619173407554626, 0.04709672927856445, 0.4496241509914398, -0.25031498074531555]\n",
      "Grand sum of 6 tensor sets is: [0.027188628911972046, 0.9659987688064575, -0.007220558822154999, 2.554455280303955, -1.3296701908111572]\n",
      "Mean of tensors is: tensor([ 0.0045,  0.1610, -0.0012,  0.4257, -0.2216]) (768 features in tensor)\n",
      "Saved the embedding for approved.\n",
      "Saved the count of sentences used to create approved embedding\n",
      "Run time for approved was 0.3965455839997958 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "approving\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for approving.\n",
      "Saved the count of sentences used to create approving embedding\n",
      "Run time for approving was 0.030984567999894352 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "argue\n",
      "\n",
      "Instance 1 of argue.\n",
      "Looking for vocab token: argue\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "argue at index 7: [0.06884217262268066, 0.20509298145771027, 0.1924692839384079, 0.09119677543640137, -1.6444611549377441]\n",
      "Grand sum of 1 tensor sets is: [0.06884217262268066, 0.20509298145771027, 0.1924692839384079, 0.09119677543640137, -1.6444611549377441]\n",
      "\n",
      "Instance 2 of argue.\n",
      "Looking for vocab token: argue\n",
      "Indices are [21]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "argue at index 21: [-0.02402881532907486, 0.11744076013565063, 0.17665696144104004, 0.09680195152759552, -1.0717964172363281]\n",
      "Grand sum of 2 tensor sets is: [0.044813357293605804, 0.3225337266921997, 0.36912626028060913, 0.1879987269639969, -2.7162575721740723]\n",
      "\n",
      "Instance 3 of argue.\n",
      "Looking for vocab token: argue\n",
      "Indices are [6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([32, 13, 768])\n",
      "Shape of summed layers is: 32 x 768\n",
      "argue at index 6: [0.1798577755689621, 0.13027141988277435, 0.09999781847000122, 0.09113790094852448, -1.0571374893188477]\n",
      "Grand sum of 3 tensor sets is: [0.2246711254119873, 0.45280516147613525, 0.46912407875061035, 0.27913662791252136, -3.77339506149292]\n",
      "Mean of tensors is: tensor([ 0.0749,  0.1509,  0.1564,  0.0930, -1.2578]) (768 features in tensor)\n",
      "Saved the embedding for argue.\n",
      "Saved the count of sentences used to create argue embedding\n",
      "Run time for argue was 0.22842997099996865 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "argument\n",
      "ative\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for argumentative.\n",
      "Saved the count of sentences used to create argumentative embedding\n",
      "Run time for argumentative was 0.03341492700019444 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "aroused\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for aroused.\n",
      "Saved the count of sentences used to create aroused embedding\n",
      "Run time for aroused was 0.02758604300015577 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "arrogance\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for arrogance.\n",
      "Saved the count of sentences used to create arrogance embedding\n",
      "Run time for arrogance was 0.024406478000400966 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "arrogant\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for arrogant.\n",
      "Saved the count of sentences used to create arrogant embedding\n",
      "Run time for arrogant was 0.024347951999970974 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "arrog\n",
      "antly\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for arrogantly.\n",
      "Saved the count of sentences used to create arrogantly embedding\n",
      "Run time for arrogantly was 0.02456348799978514 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "artificial\n",
      "\n",
      "Instance 1 of artificial.\n",
      "Looking for vocab token: artificial\n",
      "Indices are [35]\n",
      "Size of token embeddings is torch.Size([39, 13, 768])\n",
      "Shape of summed layers is: 39 x 768\n",
      "artificial at index 35: [0.07960290461778641, 0.034764569252729416, 0.10795908421278, -0.22299298644065857, 0.5744372606277466]\n",
      "Grand sum of 1 tensor sets is: [0.07960290461778641, 0.034764569252729416, 0.10795908421278, -0.22299298644065857, 0.5744372606277466]\n",
      "\n",
      "Instance 2 of artificial.\n",
      "Looking for vocab token: artificial\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "artificial at index 4: [0.08108662068843842, 0.3269450068473816, 0.11364691704511642, -0.15487828850746155, 0.7841124534606934]\n",
      "Grand sum of 2 tensor sets is: [0.16068953275680542, 0.3617095649242401, 0.22160600125789642, -0.3778712749481201, 1.35854971408844]\n",
      "Mean of tensors is: tensor([ 0.0803,  0.1809,  0.1108, -0.1889,  0.6793]) (768 features in tensor)\n",
      "Saved the embedding for artificial.\n",
      "Saved the count of sentences used to create artificial embedding\n",
      "Run time for artificial was 0.1411373660002937 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "ashamed\n",
      "\n",
      "Instance 1 of ashamed.\n",
      "Looking for vocab token: ashamed\n",
      "Indices are [66]\n",
      "Size of token embeddings is torch.Size([69, 13, 768])\n",
      "Shape of summed layers is: 69 x 768\n",
      "ashamed at index 66: [-0.36356493830680847, -0.25257885456085205, -0.1825815737247467, 0.00846082903444767, -0.2561294436454773]\n",
      "Grand sum of 1 tensor sets is: [-0.36356493830680847, -0.25257885456085205, -0.1825815737247467, 0.00846082903444767, -0.2561294436454773]\n",
      "\n",
      "Instance 2 of ashamed.\n",
      "Looking for vocab token: ashamed\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "ashamed at index 6: [0.013952415436506271, -0.13214603066444397, -2.1062791347503662e-05, -0.2353881299495697, 0.5075865983963013]\n",
      "Grand sum of 2 tensor sets is: [-0.3496125340461731, -0.384724885225296, -0.1826026439666748, -0.22692729532718658, 0.251457154750824]\n",
      "Mean of tensors is: tensor([-0.1748, -0.1924, -0.0913, -0.1135,  0.1257]) (768 features in tensor)\n",
      "Saved the embedding for ashamed.\n",
      "Saved the count of sentences used to create ashamed embedding\n",
      "Run time for ashamed was 0.18934171199998673 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "aspiring\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for aspiring.\n",
      "Saved the count of sentences used to create aspiring embedding\n",
      "Run time for aspiring was 0.029682759000024816 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "assert\n",
      "ive\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for assertive.\n",
      "Saved the count of sentences used to create assertive embedding\n",
      "Run time for assertive was 0.0278562140001668 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "assert\n",
      "ively\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for assertively.\n",
      "Saved the count of sentences used to create assertively embedding\n",
      "Run time for assertively was 0.029390866000085225 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "assessing\n",
      "\n",
      "Instance 1 of assessing.\n",
      "Looking for vocab token: assessing\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([72, 13, 768])\n",
      "Shape of summed layers is: 72 x 768\n",
      "assessing at index 2: [0.12681639194488525, 0.19607220590114594, -0.2301521897315979, 0.08991909772157669, 1.0132317543029785]\n",
      "Grand sum of 1 tensor sets is: [0.12681639194488525, 0.19607220590114594, -0.2301521897315979, 0.08991909772157669, 1.0132317543029785]\n",
      "Mean of tensors is: tensor([ 0.1268,  0.1961, -0.2302,  0.0899,  1.0132]) (768 features in tensor)\n",
      "Saved the embedding for assessing.\n",
      "Saved the count of sentences used to create assessing embedding\n",
      "Run time for assessing was 0.15389010699982464 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "assured\n",
      "\n",
      "Instance 1 of assured.\n",
      "Looking for vocab token: assured\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "assured at index 15: [0.06265281140804291, -0.2211502343416214, 0.024464672431349754, -0.18672098219394684, 0.8194430470466614]\n",
      "Grand sum of 1 tensor sets is: [0.06265281140804291, -0.2211502343416214, 0.024464672431349754, -0.18672098219394684, 0.8194430470466614]\n",
      "Mean of tensors is: tensor([ 0.0627, -0.2212,  0.0245, -0.1867,  0.8194]) (768 features in tensor)\n",
      "Saved the embedding for assured.\n",
      "Saved the count of sentences used to create assured embedding\n",
      "Run time for assured was 0.086088866999944 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "astonished\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for astonished.\n",
      "Saved the count of sentences used to create astonished embedding\n",
      "Run time for astonished was 0.033366600000135804 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "aston\n",
      "ishment\n",
      "\n",
      "Instance 1 of astonishment.\n",
      "Looking for vocab token: aston\n",
      "Looking for vocab token: ishment\n",
      "Indices are [14, 15]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "aston at index 14: [0.17947961390018463, -0.04122838377952576, 0.2270057201385498, 0.2485000342130661, -0.8990238308906555]\n",
      "ishment at index 15: [-0.006226882338523865, -0.0732608288526535, -0.1315445899963379, 0.3091508448123932, 0.02471931278705597]\n",
      "Grand sum of 1 tensor sets is: [0.08662636578083038, -0.05724460631608963, 0.04773056507110596, 0.27882543206214905, -0.43715226650238037]\n",
      "\n",
      "Instance 2 of astonishment.\n",
      "Looking for vocab token: aston\n",
      "Looking for vocab token: ishment\n",
      "Indices are [23, 24]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "aston at index 23: [0.026200132444500923, 0.2283799648284912, 0.17866410315036774, 0.4976412057876587, -1.1627042293548584]\n",
      "ishment at index 24: [-0.013910967856645584, 0.15450137853622437, -0.13057099282741547, 0.5317883491516113, 0.20326219499111176]\n",
      "Grand sum of 2 tensor sets is: [0.09277094900608063, 0.13419607281684875, 0.07177712023258209, 0.7935402393341064, -0.9168732762336731]\n",
      "Mean of tensors is: tensor([ 0.0464,  0.0671,  0.0359,  0.3968, -0.4584]) (768 features in tensor)\n",
      "Saved the embedding for astonishment.\n",
      "Saved the count of sentences used to create astonishment embedding\n",
      "Run time for astonishment was 0.15087052600028983 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "ast\n",
      "ounded\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for astounded.\n",
      "Saved the count of sentences used to create astounded embedding\n",
      "Run time for astounded was 0.03136285199980193 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "attempting\n",
      "\n",
      "Instance 1 of attempting.\n",
      "Looking for vocab token: attempting\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "attempting at index 2: [0.268297404050827, 0.11452382802963257, 0.2384369969367981, 0.08031939715147018, 0.20495353639125824]\n",
      "Grand sum of 1 tensor sets is: [0.268297404050827, 0.11452382802963257, 0.2384369969367981, 0.08031939715147018, 0.20495353639125824]\n",
      "\n",
      "Instance 2 of attempting.\n",
      "Looking for vocab token: attempting\n",
      "Indices are [13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of token embeddings is torch.Size([44, 13, 768])\n",
      "Shape of summed layers is: 44 x 768\n",
      "attempting at index 13: [0.2750488817691803, 0.06962272524833679, 0.11182277649641037, 0.23127178847789764, 0.03039490431547165]\n",
      "Grand sum of 2 tensor sets is: [0.5433462858200073, 0.18414655327796936, 0.35025978088378906, 0.31159117817878723, 0.2353484332561493]\n",
      "\n",
      "Instance 3 of attempting.\n",
      "Looking for vocab token: attempting\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "attempting at index 11: [-0.004661872982978821, -0.012229569256305695, 0.20318102836608887, 0.1792888194322586, 0.581027626991272]\n",
      "Grand sum of 3 tensor sets is: [0.5386844277381897, 0.17191699147224426, 0.5534408092498779, 0.49088001251220703, 0.8163760900497437]\n",
      "\n",
      "Instance 4 of attempting.\n",
      "Looking for vocab token: attempting\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "attempting at index 27: [0.11186040937900543, 0.13188433647155762, 0.07992716133594513, 0.07227587699890137, 0.47849926352500916]\n",
      "Grand sum of 4 tensor sets is: [0.6505448222160339, 0.3038013279438019, 0.6333679556846619, 0.5631558895111084, 1.2948753833770752]\n",
      "\n",
      "Instance 5 of attempting.\n",
      "Looking for vocab token: attempting\n",
      "Indices are [15]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "attempting at index 15: [0.2207575887441635, 0.07597984373569489, 0.10291691869497299, 0.04342744126915932, 0.2601925730705261]\n",
      "Grand sum of 5 tensor sets is: [0.8713024258613586, 0.37978118658065796, 0.7362848520278931, 0.6065833568572998, 1.555068016052246]\n",
      "\n",
      "Instance 6 of attempting.\n",
      "Looking for vocab token: attempting\n",
      "Indices are [68]\n",
      "Size of token embeddings is torch.Size([76, 13, 768])\n",
      "Shape of summed layers is: 76 x 768\n",
      "attempting at index 68: [0.15483221411705017, 0.19438421726226807, 0.07633611559867859, -0.1123928651213646, 0.7156228423118591]\n",
      "Grand sum of 6 tensor sets is: [1.0261346101760864, 0.574165403842926, 0.812620997428894, 0.4941904842853546, 2.27069091796875]\n",
      "\n",
      "Instance 7 of attempting.\n",
      "Looking for vocab token: attempting\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "attempting at index 17: [0.000399664044380188, 0.04867810383439064, 0.10045544058084488, 0.12199667096138, 0.29201218485832214]\n",
      "Grand sum of 7 tensor sets is: [1.0265343189239502, 0.6228435039520264, 0.9130764603614807, 0.6161871552467346, 2.5627031326293945]\n",
      "\n",
      "Instance 8 of attempting.\n",
      "Looking for vocab token: attempting\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "attempting at index 2: [0.13421282172203064, 0.13369691371917725, 0.13294987380504608, -0.07670453935861588, 0.3058012127876282]\n",
      "Grand sum of 8 tensor sets is: [1.1607471704483032, 0.7565404176712036, 1.046026349067688, 0.539482593536377, 2.868504285812378]\n",
      "\n",
      "Instance 9 of attempting.\n",
      "Looking for vocab token: attempting\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([18, 13, 768])\n",
      "Shape of summed layers is: 18 x 768\n",
      "attempting at index 10: [0.12288235872983932, 0.3090379238128662, 0.1295420229434967, 0.1721075475215912, 0.24271804094314575]\n",
      "Grand sum of 9 tensor sets is: [1.2836295366287231, 1.0655783414840698, 1.1755683422088623, 0.7115901708602905, 3.111222267150879]\n",
      "\n",
      "Instance 10 of attempting.\n",
      "Looking for vocab token: attempting\n",
      "Indices are [37]\n",
      "Size of token embeddings is torch.Size([46, 13, 768])\n",
      "Shape of summed layers is: 46 x 768\n",
      "attempting at index 37: [-0.002199701964855194, 0.16664117574691772, 0.11146022379398346, 0.02229103073477745, 0.06106967478990555]\n",
      "Grand sum of 10 tensor sets is: [1.2814298868179321, 1.2322194576263428, 1.2870285511016846, 0.7338811755180359, 3.1722919940948486]\n",
      "Mean of tensors is: tensor([0.1281, 0.1232, 0.1287, 0.0734, 0.3172]) (768 features in tensor)\n",
      "Saved the embedding for attempting.\n",
      "Saved the count of sentences used to create attempting embedding\n",
      "Run time for attempting was 0.655596212999626 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "attentive\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for attentive.\n",
      "Saved the count of sentences used to create attentive embedding\n",
      "Run time for attentive was 0.028545842000312405 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "attent\n",
      "iveness\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for attentiveness.\n",
      "Saved the count of sentences used to create attentiveness embedding\n",
      "Run time for attentiveness was 0.0243159389997345 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "attracted\n",
      "\n",
      "Instance 1 of attracted.\n",
      "Looking for vocab token: attracted\n",
      "Indices are [42]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "attracted at index 42: [0.054477714002132416, -0.0397087037563324, -0.116180419921875, 0.16626279056072235, 0.4719522297382355]\n",
      "Grand sum of 1 tensor sets is: [0.054477714002132416, -0.0397087037563324, -0.116180419921875, 0.16626279056072235, 0.4719522297382355]\n",
      "\n",
      "Instance 2 of attracted.\n",
      "Looking for vocab token: attracted\n",
      "Indices are [17]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "attracted at index 17: [0.045469559729099274, 0.09940913319587708, -0.018614551052451134, 0.2462480068206787, -0.012619927525520325]\n",
      "Grand sum of 2 tensor sets is: [0.09994727373123169, 0.05970042943954468, -0.13479496538639069, 0.41251081228256226, 0.45933228731155396]\n",
      "\n",
      "Instance 3 of attracted.\n",
      "Looking for vocab token: attracted\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([30, 13, 768])\n",
      "Shape of summed layers is: 30 x 768\n",
      "attracted at index 3: [0.2868107855319977, 0.043074529618024826, 0.037776365876197815, 0.07543650269508362, -0.056826233863830566]\n",
      "Grand sum of 3 tensor sets is: [0.38675805926322937, 0.1027749627828598, -0.09701859951019287, 0.4879473149776459, 0.4025060534477234]\n",
      "\n",
      "Instance 4 of attracted.\n",
      "Looking for vocab token: attracted\n",
      "Indices are [44]\n",
      "Size of token embeddings is torch.Size([51, 13, 768])\n",
      "Shape of summed layers is: 51 x 768\n",
      "attracted at index 44: [0.335921972990036, 0.11685992777347565, 0.09310296922922134, -0.04570561647415161, 0.06674838811159134]\n",
      "Grand sum of 4 tensor sets is: [0.7226800322532654, 0.21963489055633545, -0.003915630280971527, 0.44224169850349426, 0.46925443410873413]\n",
      "\n",
      "Instance 5 of attracted.\n",
      "Looking for vocab token: attracted\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([10, 13, 768])\n",
      "Shape of summed layers is: 10 x 768\n",
      "attracted at index 3: [0.21531406044960022, 0.11360040307044983, -0.05642981827259064, -0.06978600472211838, 0.17136552929878235]\n",
      "Grand sum of 5 tensor sets is: [0.937994122505188, 0.3332352936267853, -0.060345448553562164, 0.3724556863307953, 0.6406199932098389]\n",
      "\n",
      "Instance 6 of attracted.\n",
      "Looking for vocab token: attracted\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "attracted at index 7: [0.12156382203102112, -0.3263832926750183, 0.053324032574892044, -0.032811351120471954, -0.8589804172515869]\n",
      "Grand sum of 6 tensor sets is: [1.0595579147338867, 0.006852000951766968, -0.00702141597867012, 0.33964434266090393, -0.21836042404174805]\n",
      "\n",
      "Instance 7 of attracted.\n",
      "Looking for vocab token: attracted\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "attracted at index 6: [0.3230738937854767, 0.021551162004470825, -0.16743266582489014, 0.23112425208091736, -0.023702040314674377]\n",
      "Grand sum of 7 tensor sets is: [1.382631778717041, 0.028403162956237793, -0.17445407807826996, 0.5707685947418213, -0.24206246435642242]\n",
      "\n",
      "Instance 8 of attracted.\n",
      "Looking for vocab token: attracted\n",
      "Indices are [6]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "attracted at index 6: [0.10536332428455353, -0.06269694864749908, -0.06131836771965027, 0.3149847984313965, 0.4225046634674072]\n",
      "Grand sum of 8 tensor sets is: [1.4879951477050781, -0.03429378569126129, -0.23577244579792023, 0.8857533931732178, 0.1804421991109848]\n",
      "Mean of tensors is: tensor([ 0.1860, -0.0043, -0.0295,  0.1107,  0.0226]) (768 features in tensor)\n",
      "Saved the embedding for attracted.\n",
      "Saved the count of sentences used to create attracted embedding\n",
      "Run time for attracted was 0.5368661359998441 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "aven\n",
      "ging\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for avenging.\n",
      "Saved the count of sentences used to create avenging embedding\n",
      "Run time for avenging was 0.029156884000258287 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "a\n",
      "verse\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for averse.\n",
      "Saved the count of sentences used to create averse embedding\n",
      "Run time for averse was 0.026822068999990734 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "aversion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance 1 of aversion.\n",
      "Looking for vocab token: aversion\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([27, 13, 768])\n",
      "Shape of summed layers is: 27 x 768\n",
      "aversion at index 23: [0.08816449344158173, 0.0816800445318222, 0.1390906423330307, 0.12265820056200027, -0.12753167748451233]\n",
      "Grand sum of 1 tensor sets is: [0.08816449344158173, 0.0816800445318222, 0.1390906423330307, 0.12265820056200027, -0.12753167748451233]\n",
      "Mean of tensors is: tensor([ 0.0882,  0.0817,  0.1391,  0.1227, -0.1275]) (768 features in tensor)\n",
      "Saved the embedding for aversion.\n",
      "Saved the count of sentences used to create aversion embedding\n",
      "Run time for aversion was 0.0866591049998533 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "a\n",
      "versive\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for aversive.\n",
      "Saved the count of sentences used to create aversive embedding\n",
      "Run time for aversive was 0.028115456999785238 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "avid\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for avid.\n",
      "Saved the count of sentences used to create avid embedding\n",
      "Run time for avid was 0.02394957800015618 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "avoiding\n",
      "\n",
      "Instance 1 of avoiding.\n",
      "Looking for vocab token: avoiding\n",
      "Indices are [28]\n",
      "Size of token embeddings is torch.Size([58, 13, 768])\n",
      "Shape of summed layers is: 58 x 768\n",
      "avoiding at index 28: [0.2064252644777298, -0.0027953237295150757, -0.09289681166410446, -0.17962346971035004, 0.4430348873138428]\n",
      "Grand sum of 1 tensor sets is: [0.2064252644777298, -0.0027953237295150757, -0.09289681166410446, -0.17962346971035004, 0.4430348873138428]\n",
      "\n",
      "Instance 2 of avoiding.\n",
      "Looking for vocab token: avoiding\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "avoiding at index 16: [0.35135895013809204, 0.3332180976867676, 0.05663122236728668, -0.2134239226579666, 0.5697475671768188]\n",
      "Grand sum of 2 tensor sets is: [0.5577841997146606, 0.3304227590560913, -0.03626558929681778, -0.39304739236831665, 1.0127824544906616]\n",
      "Mean of tensors is: tensor([ 0.2789,  0.1652, -0.0181, -0.1965,  0.5064]) (768 features in tensor)\n",
      "Saved the embedding for avoiding.\n",
      "Saved the count of sentences used to create avoiding embedding\n",
      "Run time for avoiding was 0.16503928599968276 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "awaiting\n",
      "\n",
      "Instance 1 of awaiting.\n",
      "Looking for vocab token: awaiting\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([21, 13, 768])\n",
      "Shape of summed layers is: 21 x 768\n",
      "awaiting at index 2: [0.09370365738868713, 0.315437912940979, 0.05047857016324997, 0.2967710494995117, 0.6203904747962952]\n",
      "Grand sum of 1 tensor sets is: [0.09370365738868713, 0.315437912940979, 0.05047857016324997, 0.2967710494995117, 0.6203904747962952]\n",
      "\n",
      "Instance 2 of awaiting.\n",
      "Looking for vocab token: awaiting\n",
      "Indices are [39]\n",
      "Size of token embeddings is torch.Size([61, 13, 768])\n",
      "Shape of summed layers is: 61 x 768\n",
      "awaiting at index 39: [0.13929256796836853, 0.1864623725414276, 0.048998650163412094, 0.3123489022254944, 0.2744264006614685]\n",
      "Grand sum of 2 tensor sets is: [0.23299622535705566, 0.501900315284729, 0.09947721660137177, 0.6091199517250061, 0.8948168754577637]\n",
      "\n",
      "Instance 3 of awaiting.\n",
      "Looking for vocab token: awaiting\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "awaiting at index 18: [-0.09810610115528107, 0.28537309169769287, -0.0058890450745821, 0.46089544892311096, 0.2718583047389984]\n",
      "Grand sum of 3 tensor sets is: [0.1348901242017746, 0.7872734069824219, 0.09358817338943481, 1.0700154304504395, 1.1666752099990845]\n",
      "Mean of tensors is: tensor([0.0450, 0.2624, 0.0312, 0.3567, 0.3889]) (768 features in tensor)\n",
      "Saved the embedding for awaiting.\n",
      "Saved the count of sentences used to create awaiting embedding\n",
      "Run time for awaiting was 0.23871276999989277 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "awakened\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for awakened.\n",
      "Saved the count of sentences used to create awakened embedding\n",
      "Run time for awakened was 0.02671390000023166 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "aware\n",
      "\n",
      "Instance 1 of aware.\n",
      "Looking for vocab token: aware\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([42, 13, 768])\n",
      "Shape of summed layers is: 42 x 768\n",
      "aware at index 12: [0.06320196390151978, 0.01043689250946045, -0.12627558410167694, 0.1915077269077301, 0.5086623430252075]\n",
      "Grand sum of 1 tensor sets is: [0.06320196390151978, 0.01043689250946045, -0.12627558410167694, 0.1915077269077301, 0.5086623430252075]\n",
      "\n",
      "Instance 2 of aware.\n",
      "Looking for vocab token: aware\n",
      "Indices are [9]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "aware at index 9: [0.09955587983131409, -0.28351521492004395, -0.09196937829256058, -0.057791613042354584, -0.2638980746269226]\n",
      "Grand sum of 2 tensor sets is: [0.16275784373283386, -0.2730783224105835, -0.21824496984481812, 0.13371610641479492, 0.2447642683982849]\n",
      "\n",
      "Instance 3 of aware.\n",
      "Looking for vocab token: aware\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "aware at index 7: [0.04753277078270912, -0.1793387532234192, -0.07096859812736511, 0.000461921445094049, -0.19554373621940613]\n",
      "Grand sum of 3 tensor sets is: [0.21029061079025269, -0.4524170756340027, -0.2892135679721832, 0.134178027510643, 0.049220532178878784]\n",
      "\n",
      "Instance 4 of aware.\n",
      "Looking for vocab token: aware\n",
      "Indices are [29]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "aware at index 29: [0.2820882201194763, 0.1717272698879242, 0.015065348707139492, 0.1655404269695282, 0.20110273361206055]\n",
      "Grand sum of 4 tensor sets is: [0.492378830909729, -0.2806898057460785, -0.27414822578430176, 0.29971843957901, 0.25032326579093933]\n",
      "\n",
      "Instance 5 of aware.\n",
      "Looking for vocab token: aware\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([33, 13, 768])\n",
      "Shape of summed layers is: 33 x 768\n",
      "aware at index 4: [0.30891552567481995, -0.13771463930606842, -0.15519794821739197, 0.09748797863721848, 0.39901819825172424]\n",
      "Grand sum of 5 tensor sets is: [0.8012943267822266, -0.4184044599533081, -0.4293461740016937, 0.3972064256668091, 0.6493414640426636]\n",
      "\n",
      "Instance 6 of aware.\n",
      "Looking for vocab token: aware\n",
      "Indices are [31]\n",
      "Size of token embeddings is torch.Size([72, 13, 768])\n",
      "Shape of summed layers is: 72 x 768\n",
      "aware at index 31: [0.2632296085357666, -0.287253737449646, -0.31184181571006775, -0.1163700744509697, 0.5469584465026855]\n",
      "Grand sum of 6 tensor sets is: [1.0645239353179932, -0.7056581974029541, -0.7411879897117615, 0.2808363437652588, 1.1962999105453491]\n",
      "\n",
      "Instance 7 of aware.\n",
      "Looking for vocab token: aware\n",
      "Indices are [44]\n",
      "Size of token embeddings is torch.Size([52, 13, 768])\n",
      "Shape of summed layers is: 52 x 768\n",
      "aware at index 44: [0.3832920491695404, 0.030457448214292526, -0.029434071853756905, 0.4989261031150818, -0.5551372766494751]\n",
      "Grand sum of 7 tensor sets is: [1.447816014289856, -0.6752007603645325, -0.7706220746040344, 0.7797624468803406, 0.641162633895874]\n",
      "\n",
      "Instance 8 of aware.\n",
      "Looking for vocab token: aware\n",
      "\n",
      "Instance 9 of aware.\n",
      "Looking for vocab token: aware\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "aware at index 30: [0.25534212589263916, 0.2867928743362427, -0.1632080078125, 0.2671853005886078, 0.2932778596878052]\n",
      "Grand sum of 8 tensor sets is: [1.7031581401824951, -0.3884078860282898, -0.9338300824165344, 1.046947717666626, 0.9344404935836792]\n",
      "Mean of tensors is: tensor([ 0.2129, -0.0486, -0.1167,  0.1309,  0.1168]) (768 features in tensor)\n",
      "Saved the embedding for aware.\n",
      "Saved the count of sentences used to create aware embedding\n",
      "Run time for aware was 0.5594062270001814 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "awareness\n",
      "\n",
      "Instance 1 of awareness.\n",
      "Looking for vocab token: awareness\n",
      "\n",
      "Instance 2 of awareness.\n",
      "Looking for vocab token: awareness\n",
      "Indices are [8]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "awareness at index 8: [-0.2500919997692108, -0.0290285125374794, 0.11069278419017792, 0.09179925918579102, -0.02760794758796692]\n",
      "Grand sum of 1 tensor sets is: [-0.2500919997692108, -0.0290285125374794, 0.11069278419017792, 0.09179925918579102, -0.02760794758796692]\n",
      "Mean of tensors is: tensor([-0.2501, -0.0290,  0.1107,  0.0918, -0.0276]) (768 features in tensor)\n",
      "Saved the embedding for awareness.\n",
      "Saved the count of sentences used to create awareness embedding\n",
      "Run time for awareness was 0.08647794299986344 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "awe\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for awe.\n",
      "Saved the count of sentences used to create awe embedding\n",
      "Run time for awe was 0.02920987800007424 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "aw\n",
      "ed\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for awed.\n",
      "Saved the count of sentences used to create awed embedding\n",
      "Run time for awed was 0.024812586999814812 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized text:\n",
      "aw\n",
      "est\n",
      "ruck\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for awestruck.\n",
      "Saved the count of sentences used to create awestruck embedding\n",
      "Run time for awestruck was 0.024019015999783733 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "awful\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for awful.\n",
      "Saved the count of sentences used to create awful embedding\n",
      "Run time for awful was 0.02389653599993835 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "awkward\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for awkward.\n",
      "Saved the count of sentences used to create awkward embedding\n",
      "Run time for awkward was 0.02393403699988994 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "awkward\n",
      "ness\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for awkwardness.\n",
      "Saved the count of sentences used to create awkwardness embedding\n",
      "Run time for awkwardness was 0.023863528000219958 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "ax\n",
      "ed\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for axed.\n",
      "Saved the count of sentences used to create axed embedding\n",
      "Run time for axed was 0.023627934000160167 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "back\n",
      "handed\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved the embedding for backhanded.\n",
      "Saved the count of sentences used to create backhanded embedding\n",
      "Run time for backhanded was 0.02496350799992797 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "badly\n",
      "\n",
      "Instance 1 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "badly at index 3: [-0.04932598024606705, 0.20432643592357635, 0.14524024724960327, 0.2621002197265625, 1.0760900974273682]\n",
      "Grand sum of 1 tensor sets is: [-0.04932598024606705, 0.20432643592357635, 0.14524024724960327, 0.2621002197265625, 1.0760900974273682]\n",
      "\n",
      "Instance 2 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [7]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "badly at index 7: [-0.14191602170467377, 0.14972223341464996, 0.05356631055474281, 0.3544180989265442, 0.6540783643722534]\n",
      "Grand sum of 2 tensor sets is: [-0.1912420094013214, 0.3540486693382263, 0.1988065540790558, 0.6165183186531067, 1.7301684617996216]\n",
      "\n",
      "Instance 3 of badly.\n",
      "Looking for vocab token: badly\n",
      "\n",
      "Instance 4 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [36]\n",
      "Size of token embeddings is torch.Size([61, 13, 768])\n",
      "Shape of summed layers is: 61 x 768\n",
      "badly at index 36: [0.17451748251914978, 0.006813496351242065, 0.08840584754943848, -0.4612594544887543, 1.023608684539795]\n",
      "Grand sum of 3 tensor sets is: [-0.01672452688217163, 0.3608621656894684, 0.28721240162849426, 0.15525886416435242, 2.753777027130127]\n",
      "\n",
      "Instance 5 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [27]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "badly at index 27: [-0.02822815626859665, 0.13040566444396973, 0.0401155911386013, -0.31503725051879883, 0.9988101124763489]\n",
      "Grand sum of 4 tensor sets is: [-0.04495268315076828, 0.4912678301334381, 0.32732799649238586, -0.1597783863544464, 3.752587080001831]\n",
      "\n",
      "Instance 6 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [23]\n",
      "Size of token embeddings is torch.Size([48, 13, 768])\n",
      "Shape of summed layers is: 48 x 768\n",
      "badly at index 23: [0.026280522346496582, 0.2800995111465454, 0.04932811111211777, -0.2359740436077118, 1.8147668838500977]\n",
      "Grand sum of 5 tensor sets is: [-0.018672160804271698, 0.7713673114776611, 0.37665611505508423, -0.3957524299621582, 5.567354202270508]\n",
      "\n",
      "Instance 7 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "badly at index 10: [0.09102112054824829, 0.26622334122657776, 0.04872165992856026, -0.06009083613753319, 1.7007012367248535]\n",
      "Grand sum of 6 tensor sets is: [0.07234895974397659, 1.0375906229019165, 0.4253777861595154, -0.4558432698249817, 7.268055438995361]\n",
      "\n",
      "Instance 8 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [26]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "badly at index 26: [0.08634793758392334, 0.21357418596744537, 0.07354362308979034, -0.09781914204359055, 1.094259262084961]\n",
      "Grand sum of 7 tensor sets is: [0.15869688987731934, 1.2511647939682007, 0.49892139434814453, -0.5536624193191528, 8.362314224243164]\n",
      "\n",
      "Instance 9 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "badly at index 2: [0.004996135830879211, 0.17452892661094666, 0.19005540013313293, 0.08308257162570953, 1.730299949645996]\n",
      "Grand sum of 8 tensor sets is: [0.16369302570819855, 1.4256937503814697, 0.6889767646789551, -0.4705798625946045, 10.09261417388916]\n",
      "\n",
      "Instance 10 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "badly at index 4: [0.06123572587966919, 0.16714262962341309, 0.08532989770174026, 0.26651373505592346, 1.234102725982666]\n",
      "Grand sum of 9 tensor sets is: [0.22492875158786774, 1.5928363800048828, 0.7743066549301147, -0.20406612753868103, 11.326717376708984]\n",
      "\n",
      "Instance 11 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([37, 13, 768])\n",
      "Shape of summed layers is: 37 x 768\n",
      "badly at index 25: [0.005772687494754791, 0.2867071330547333, 0.0031701549887657166, 0.2649258077144623, 1.2053329944610596]\n",
      "Grand sum of 10 tensor sets is: [0.23070144653320312, 1.8795435428619385, 0.7774767875671387, 0.06085968017578125, 12.532050132751465]\n",
      "\n",
      "Instance 12 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [10]\n",
      "Size of token embeddings is torch.Size([13, 13, 768])\n",
      "Shape of summed layers is: 13 x 768\n",
      "badly at index 10: [0.04940548911690712, 0.2936899662017822, 0.12275620549917221, 0.4555037319660187, 1.0187597274780273]\n",
      "Grand sum of 11 tensor sets is: [0.28010693192481995, 2.1732335090637207, 0.9002329707145691, 0.5163633823394775, 13.550809860229492]\n",
      "\n",
      "Instance 13 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([12, 13, 768])\n",
      "Shape of summed layers is: 12 x 768\n",
      "badly at index 4: [0.0024076253175735474, 0.16427108645439148, 0.05563846975564957, 0.1699882596731186, 1.3828911781311035]\n",
      "Grand sum of 12 tensor sets is: [0.2825145721435547, 2.3375046253204346, 0.9558714628219604, 0.6863516569137573, 14.933700561523438]\n",
      "\n",
      "Instance 14 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [4, 10]\n",
      "Size of token embeddings is torch.Size([23, 13, 768])\n",
      "Shape of summed layers is: 23 x 768\n",
      "badly at index 4: [-0.03750115633010864, -0.02017318457365036, 0.14083419740200043, -0.2616604268550873, 1.1273969411849976]\n",
      "badly at index 10: [0.01732657477259636, 0.06904539465904236, 0.16496728360652924, -0.18441979587078094, 1.0890846252441406]\n",
      "Grand sum of 13 tensor sets is: [0.2724272906780243, 2.361940622329712, 1.1087721586227417, 0.4633115530014038, 16.041940689086914]\n",
      "\n",
      "Instance 15 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "badly at index 4: [0.025189783424139023, 0.3244208097457886, -0.005362933501601219, -0.07355137169361115, 0.8606026768684387]\n",
      "Grand sum of 14 tensor sets is: [0.2976170778274536, 2.686361312866211, 1.1034091711044312, 0.38976019620895386, 16.902544021606445]\n",
      "\n",
      "Instance 16 of badly.\n",
      "Looking for vocab token: badly\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "badly at index 11: [-0.083777517080307, 0.31061992049217224, 0.12527114152908325, 0.0837322548031807, 1.392991542816162]\n",
      "Grand sum of 15 tensor sets is: [0.2138395607471466, 2.996981143951416, 1.2286803722381592, 0.47349244356155396, 18.295536041259766]\n",
      "Mean of tensors is: tensor([0.0143, 0.1998, 0.0819, 0.0316, 1.2197]) (768 features in tensor)\n",
      "Saved the embedding for badly.\n",
      "Saved the count of sentences used to create badly embedding\n",
      "Run time for badly was 0.8326089370002592 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "baff\n",
      "le\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for baffle.\n",
      "Saved the count of sentences used to create baffle embedding\n",
      "Run time for baffle was 0.030576286000268738 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "baffled\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for baffled.\n",
      "Saved the count of sentences used to create baffled embedding\n",
      "Run time for baffled was 0.025908252999670367 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "baff\n",
      "ling\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for baffling.\n",
      "Saved the count of sentences used to create baffling embedding\n",
      "Run time for baffling was 0.023962588000358664 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "baked\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for baked.\n",
      "Saved the count of sentences used to create baked embedding\n",
      "Run time for baked was 0.025543277999986458 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "ban\n",
      "al\n",
      "\n",
      "Instance 1 of banal.\n",
      "Looking for vocab token: ban\n",
      "Looking for vocab token: al\n",
      "Indices are [29, 30]\n",
      "Size of token embeddings is torch.Size([74, 13, 768])\n",
      "Shape of summed layers is: 74 x 768\n",
      "ban at index 29: [0.37564027309417725, 0.8007642030715942, 0.19039347767829895, 0.27616459131240845, -0.15786463022232056]\n",
      "al at index 30: [-0.1630508452653885, 0.42835676670074463, 0.037747547030448914, 0.03732722997665405, 1.1688928604125977]\n",
      "Grand sum of 1 tensor sets is: [0.10629471391439438, 0.6145604848861694, 0.11407051235437393, 0.15674591064453125, 0.5055141448974609]\n",
      "Mean of tensors is: tensor([0.1063, 0.6146, 0.1141, 0.1567, 0.5055]) (768 features in tensor)\n",
      "Saved the embedding for banal.\n",
      "Saved the count of sentences used to create banal embedding\n",
      "Run time for banal was 0.11698645000024044 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "barking\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for barking.\n",
      "Saved the count of sentences used to create barking embedding\n",
      "Run time for barking was 0.024808699999994133 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "bash\n",
      "ful\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bashful.\n",
      "Saved the count of sentences used to create bashful embedding\n",
      "Run time for bashful was 0.024266112000077555 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "be\n",
      "aming\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for beaming.\n",
      "Saved the count of sentences used to create beaming embedding\n",
      "Run time for beaming was 0.023779596999702335 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "bear\n",
      "ish\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bearish.\n",
      "Saved the count of sentences used to create bearish embedding\n",
      "Run time for bearish was 0.025203564000094048 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "beat\n",
      "\n",
      "Instance 1 of beat.\n",
      "Looking for vocab token: beat\n",
      "\n",
      "Instance 2 of beat.\n",
      "Looking for vocab token: beat\n",
      "\n",
      "Instance 3 of beat.\n",
      "Looking for vocab token: beat\n",
      "\n",
      "Instance 4 of beat.\n",
      "Looking for vocab token: beat\n",
      "\n",
      "Instance 5 of beat.\n",
      "Looking for vocab token: beat\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "beat at index 4: [0.18232806026935577, -0.28301751613616943, -0.23293912410736084, 0.048649244010448456, -0.488630473613739]\n",
      "Grand sum of 1 tensor sets is: [0.18232806026935577, -0.28301751613616943, -0.23293912410736084, 0.048649244010448456, -0.488630473613739]\n",
      "\n",
      "Instance 6 of beat.\n",
      "Looking for vocab token: beat\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "beat at index 12: [0.18745851516723633, -0.06715984642505646, 0.08181329071521759, -0.5155776739120483, -0.7835223078727722]\n",
      "Grand sum of 2 tensor sets is: [0.3697865605354309, -0.3501773476600647, -0.15112583339214325, -0.4669284224510193, -1.2721527814865112]\n",
      "\n",
      "Instance 7 of beat.\n",
      "Looking for vocab token: beat\n",
      "Indices are [18]\n",
      "Size of token embeddings is torch.Size([41, 13, 768])\n",
      "Shape of summed layers is: 41 x 768\n",
      "beat at index 18: [0.18999524414539337, -0.04766558110713959, -0.07772503793239594, -0.35929590463638306, -0.7232933044433594]\n",
      "Grand sum of 3 tensor sets is: [0.5597817897796631, -0.3978429436683655, -0.22885087132453918, -0.8262243270874023, -1.9954460859298706]\n",
      "\n",
      "Instance 8 of beat.\n",
      "Looking for vocab token: beat\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "beat at index 4: [0.22316795587539673, -0.15126532316207886, -0.03902377933263779, -0.278055340051651, -1.4533255100250244]\n",
      "Grand sum of 4 tensor sets is: [0.7829497456550598, -0.5491082668304443, -0.26787465810775757, -1.104279637336731, -3.4487714767456055]\n",
      "\n",
      "Instance 9 of beat.\n",
      "Looking for vocab token: beat\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([36, 13, 768])\n",
      "Shape of summed layers is: 36 x 768\n",
      "beat at index 3: [0.1869942992925644, 0.09900295734405518, -0.1742640882730484, -0.4189644753932953, -0.06966196000576019]\n",
      "Grand sum of 5 tensor sets is: [0.9699440598487854, -0.45010530948638916, -0.4421387314796448, -1.5232441425323486, -3.5184333324432373]\n",
      "\n",
      "Instance 10 of beat.\n",
      "Looking for vocab token: beat\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([20, 13, 768])\n",
      "Shape of summed layers is: 20 x 768\n",
      "beat at index 14: [0.1090816855430603, 0.06753363460302353, -0.49266180396080017, -0.3019265830516815, -0.13274100422859192]\n",
      "Grand sum of 6 tensor sets is: [1.0790257453918457, -0.38257166743278503, -0.9348005056381226, -1.8251707553863525, -3.651174306869507]\n",
      "\n",
      "Instance 11 of beat.\n",
      "Looking for vocab token: beat\n",
      "Indices are [2]\n",
      "Size of token embeddings is torch.Size([49, 13, 768])\n",
      "Shape of summed layers is: 49 x 768\n",
      "beat at index 2: [0.018019884824752808, -0.022769883275032043, -0.3281351625919342, -0.2826218605041504, 0.15953904390335083]\n",
      "Grand sum of 7 tensor sets is: [1.097045660018921, -0.40534156560897827, -1.2629356384277344, -2.107792615890503, -3.491635322570801]\n",
      "\n",
      "Instance 12 of beat.\n",
      "Looking for vocab token: beat\n",
      "Indices are [19, 33]\n",
      "Size of token embeddings is torch.Size([43, 13, 768])\n",
      "Shape of summed layers is: 43 x 768\n",
      "beat at index 19: [0.24958230555057526, 0.16294661164283752, -0.22214184701442719, -0.2210734784603119, -1.1659855842590332]\n",
      "beat at index 33: [0.24658863246440887, 0.15531721711158752, -0.25946545600891113, -0.4825374186038971, -1.278578758239746]\n",
      "Grand sum of 8 tensor sets is: [1.3451311588287354, -0.24620965123176575, -1.503739356994629, -2.4595980644226074, -4.7139177322387695]\n",
      "\n",
      "Instance 13 of beat.\n",
      "Looking for vocab token: beat\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "beat at index 22: [0.24204668402671814, -0.010289318859577179, -0.17888998985290527, -0.3407951295375824, -0.6598373651504517]\n",
      "Grand sum of 9 tensor sets is: [1.5871778726577759, -0.25649896264076233, -1.6826293468475342, -2.8003931045532227, -5.373754978179932]\n",
      "\n",
      "Instance 14 of beat.\n",
      "Looking for vocab token: beat\n",
      "Indices are [11]\n",
      "Size of token embeddings is torch.Size([14, 13, 768])\n",
      "Shape of summed layers is: 14 x 768\n",
      "beat at index 11: [0.010382764041423798, 0.03301608934998512, -0.02156142331659794, -0.2697214186191559, -0.8073258399963379]\n",
      "Grand sum of 10 tensor sets is: [1.5975606441497803, -0.2234828770160675, -1.704190731048584, -3.0701146125793457, -6.1810808181762695]\n",
      "\n",
      "Instance 15 of beat.\n",
      "Looking for vocab token: beat\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "beat at index 14: [0.1288866400718689, -0.10030436515808105, 0.05099240690469742, -0.17634311318397522, -0.7278710603713989]\n",
      "Grand sum of 11 tensor sets is: [1.726447343826294, -0.32378724217414856, -1.6531983613967896, -3.246457815170288, -6.908951759338379]\n",
      "\n",
      "Instance 16 of beat.\n",
      "Looking for vocab token: beat\n",
      "Indices are [14]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "beat at index 14: [0.15994703769683838, -0.1538230925798416, -0.1925855129957199, 0.05143578723073006, -0.3928861618041992]\n",
      "Grand sum of 12 tensor sets is: [1.8863943815231323, -0.47761034965515137, -1.8457838296890259, -3.1950221061706543, -7.301837921142578]\n",
      "\n",
      "Instance 17 of beat.\n",
      "Looking for vocab token: beat\n",
      "Indices are [24]\n",
      "Size of token embeddings is torch.Size([28, 13, 768])\n",
      "Shape of summed layers is: 28 x 768\n",
      "beat at index 24: [0.06507625430822372, -0.10517197847366333, 0.033558398485183716, -0.29775190353393555, -0.6926261782646179]\n",
      "Grand sum of 13 tensor sets is: [1.9514706134796143, -0.5827823281288147, -1.8122254610061646, -3.49277400970459, -7.994463920593262]\n",
      "\n",
      "Instance 18 of beat.\n",
      "Looking for vocab token: beat\n",
      "Indices are [13]\n",
      "Size of token embeddings is torch.Size([17, 13, 768])\n",
      "Shape of summed layers is: 17 x 768\n",
      "beat at index 13: [0.03614579886198044, -0.13984794914722443, 0.06919234246015549, -0.2266642451286316, -0.6214209198951721]\n",
      "Grand sum of 14 tensor sets is: [1.9876164197921753, -0.7226302623748779, -1.7430331707000732, -3.719438314437866, -8.615884780883789]\n",
      "Mean of tensors is: tensor([ 0.1420, -0.0516, -0.1245, -0.2657, -0.6154]) (768 features in tensor)\n",
      "Saved the embedding for beat.\n",
      "Saved the count of sentences used to create beat embedding\n",
      "Run time for beat was 0.8218678900002487 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "beaten\n",
      "\n",
      "Instance 1 of beaten.\n",
      "Looking for vocab token: beaten\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([29, 13, 768])\n",
      "Shape of summed layers is: 29 x 768\n",
      "beaten at index 4: [-0.13692939281463623, -0.1129075288772583, -0.14185190200805664, -0.39873701333999634, -0.4848898649215698]\n",
      "Grand sum of 1 tensor sets is: [-0.13692939281463623, -0.1129075288772583, -0.14185190200805664, -0.39873701333999634, -0.4848898649215698]\n",
      "Mean of tensors is: tensor([-0.1369, -0.1129, -0.1419, -0.3987, -0.4849]) (768 features in tensor)\n",
      "Saved the embedding for beaten.\n",
      "Saved the count of sentences used to create beaten embedding\n",
      "Run time for beaten was 0.08391781299997092 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized text:\n",
      "bed\n",
      "ev\n",
      "iled\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bedeviled.\n",
      "Saved the count of sentences used to create bedeviled embedding\n",
      "Run time for bedeviled was 0.028520204999949783 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized text:\n",
      "be\n",
      "f\n",
      "uddled\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for befuddled.\n",
      "Saved the count of sentences used to create befuddled embedding\n",
      "Run time for befuddled was 0.024525868999717204 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "begging\n",
      "\n",
      "Instance 1 of begging.\n",
      "Looking for vocab token: begging\n",
      "Indices are [16]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "begging at index 16: [-0.084084153175354, 0.19248074293136597, 0.0530625618994236, 0.0920068770647049, 1.6260147094726562]\n",
      "Grand sum of 1 tensor sets is: [-0.084084153175354, 0.19248074293136597, 0.0530625618994236, 0.0920068770647049, 1.6260147094726562]\n",
      "\n",
      "Instance 2 of begging.\n",
      "Looking for vocab token: begging\n",
      "Indices are [4]\n",
      "Size of token embeddings is torch.Size([31, 13, 768])\n",
      "Shape of summed layers is: 31 x 768\n",
      "begging at index 4: [0.22923703491687775, -0.2411428540945053, -0.01716305874288082, 0.3000563979148865, 1.1454803943634033]\n",
      "Grand sum of 2 tensor sets is: [0.14515288174152374, -0.04866211116313934, 0.03589950501918793, 0.3920632600784302, 2.7714951038360596]\n",
      "Mean of tensors is: tensor([ 0.0726, -0.0243,  0.0179,  0.1960,  1.3857]) (768 features in tensor)\n",
      "Saved the embedding for begging.\n",
      "Saved the count of sentences used to create begging embedding\n",
      "Run time for begging was 0.1295522489999712 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized text:\n",
      "be\n",
      "gr\n",
      "udge\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for begrudge.\n",
      "Saved the count of sentences used to create begrudge embedding\n",
      "Run time for begrudge was 0.02601353599993672 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized text:\n",
      "be\n",
      "gr\n",
      "udging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for begrudging.\n",
      "Saved the count of sentences used to create begrudging embedding\n",
      "Run time for begrudging was 0.024960101999567996 seconds.\n",
      "\n",
      "There are 4 tokens in tokenized text:\n",
      "be\n",
      "gr\n",
      "udging\n",
      "ly\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for begrudgingly.\n",
      "Saved the count of sentences used to create begrudgingly embedding\n",
      "Run time for begrudgingly was 0.024953889999778767 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized text:\n",
      "beg\n",
      "u\n",
      "iled\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for beguiled.\n",
      "Saved the count of sentences used to create beguiled embedding\n",
      "Run time for beguiled was 0.024300262999986444 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "bel\n",
      "ated\n",
      "\n",
      "Instance 1 of belated.\n",
      "Looking for vocab token: bel\n",
      "Looking for vocab token: ated\n",
      "Indices are [14, 15]\n",
      "Size of token embeddings is torch.Size([25, 13, 768])\n",
      "Shape of summed layers is: 25 x 768\n",
      "bel at index 14: [0.021198946982622147, 0.04176769778132439, 0.026334745809435844, -0.025293787941336632, -0.19791942834854126]\n",
      "ated at index 15: [-0.07539894431829453, 0.08971923589706421, 0.05585380643606186, 0.25272104144096375, 0.45447394251823425]\n",
      "Grand sum of 1 tensor sets is: [-0.02709999866783619, 0.06574346870183945, 0.041094277054071426, 0.11371362954378128, 0.1282772570848465]\n",
      "Mean of tensors is: tensor([-0.0271,  0.0657,  0.0411,  0.1137,  0.1283]) (768 features in tensor)\n",
      "Saved the embedding for belated.\n",
      "Saved the count of sentences used to create belated embedding\n",
      "Run time for belated was 0.07166422299997066 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized text:\n",
      "bel\n",
      "itt\n",
      "ling\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for belittling.\n",
      "Saved the count of sentences used to create belittling embedding\n",
      "Run time for belittling was 0.029367357999944943 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "bellig\n",
      "erence\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for belligerence.\n",
      "Saved the count of sentences used to create belligerence embedding\n",
      "Run time for belligerence was 0.02497743399999308 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "bellig\n",
      "erent\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for belligerent.\n",
      "Saved the count of sentences used to create belligerent embedding\n",
      "Run time for belligerent was 0.02559704600025725 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "belonging\n",
      "\n",
      "Instance 1 of belonging.\n",
      "Looking for vocab token: belonging\n",
      "Indices are [20]\n",
      "Size of token embeddings is torch.Size([40, 13, 768])\n",
      "Shape of summed layers is: 40 x 768\n",
      "belonging at index 20: [0.24389567971229553, 0.03324664384126663, -0.3973850607872009, 0.2695547938346863, 0.9879419803619385]\n",
      "Grand sum of 1 tensor sets is: [0.24389567971229553, 0.03324664384126663, -0.3973850607872009, 0.2695547938346863, 0.9879419803619385]\n",
      "\n",
      "Instance 2 of belonging.\n",
      "Looking for vocab token: belonging\n",
      "Indices are [3]\n",
      "Size of token embeddings is torch.Size([22, 13, 768])\n",
      "Shape of summed layers is: 22 x 768\n",
      "belonging at index 3: [0.2925761938095093, 0.08313213288784027, -0.14643928408622742, 0.09320096671581268, 0.427541583776474]\n",
      "Grand sum of 2 tensor sets is: [0.5364718437194824, 0.1163787767291069, -0.543824315071106, 0.36275577545166016, 1.4154835939407349]\n",
      "Mean of tensors is: tensor([ 0.2682,  0.0582, -0.2719,  0.1814,  0.7077]) (768 features in tensor)\n",
      "Saved the embedding for belonging.\n",
      "Saved the count of sentences used to create belonging embedding\n",
      "Run time for belonging was 0.1377768140000626 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized text:\n",
      "be\n",
      "m\n",
      "used\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bemused.\n",
      "Saved the count of sentences used to create bemused embedding\n",
      "Run time for bemused was 0.026671044000067923 seconds.\n",
      "\n",
      "There are 4 tokens in tokenized text:\n",
      "be\n",
      "m\n",
      "use\n",
      "ment\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bemusement.\n",
      "Saved the count of sentences used to create bemusement embedding\n",
      "Run time for bemusement was 0.023779233999903227 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized text:\n",
      "bene\n",
      "vol\n",
      "ence\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for benevolence.\n",
      "Saved the count of sentences used to create benevolence embedding\n",
      "Run time for benevolence was 0.024454314000195154 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "benevolent\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for benevolent.\n",
      "Saved the count of sentences used to create benevolent embedding\n",
      "Run time for benevolent was 0.026290366000012 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized text:\n",
      "ben\n",
      "umb\n",
      "ed\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for benumbed.\n",
      "Saved the count of sentences used to create benumbed embedding\n",
      "Run time for benumbed was 0.024408523000147397 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "ber\n",
      "ate\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for berate.\n",
      "Saved the count of sentences used to create berate embedding\n",
      "Run time for berate was 0.025499029000002338 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "ber\n",
      "ating\n",
      "\n",
      "Instance 1 of berating.\n",
      "Looking for vocab token: ber\n",
      "Looking for vocab token: ating\n",
      "Indices are [11, 12]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "ber at index 11: [-0.05139554291963577, 0.11074674129486084, 0.012256844900548458, 0.41428881883621216, -0.5741423964500427]\n",
      "ating at index 12: [-0.008366528898477554, 0.17898835241794586, 0.08633004873991013, 0.8704319000244141, 0.8281299471855164]\n",
      "Grand sum of 1 tensor sets is: [-0.029881035909056664, 0.14486753940582275, 0.04929344728589058, 0.6423603296279907, 0.12699377536773682]\n",
      "Mean of tensors is: tensor([-0.0299,  0.1449,  0.0493,  0.6424,  0.1270]) (768 features in tensor)\n",
      "Saved the embedding for berating.\n",
      "Saved the count of sentences used to create berating embedding\n",
      "Run time for berating was 0.08700275999990481 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "bere\n",
      "aved\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bereaved.\n",
      "Saved the count of sentences used to create bereaved embedding\n",
      "Run time for bereaved was 0.027680533999955514 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "bere\n",
      "ft\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bereft.\n",
      "Saved the count of sentences used to create bereft embedding\n",
      "Run time for bereft was 0.027052465999986453 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized text:\n",
      "bes\n",
      "ee\n",
      "ching\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for beseeching.\n",
      "Saved the count of sentences used to create beseeching embedding\n",
      "Run time for beseeching was 0.0255892679997487 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "best\n",
      "ed\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bested.\n",
      "Saved the count of sentences used to create bested embedding\n",
      "Run time for bested was 0.024444011000014143 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "betrayal\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for betrayal.\n",
      "Saved the count of sentences used to create betrayal embedding\n",
      "Run time for betrayal was 0.025018638000346982 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "betrayed\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for betrayed.\n",
      "Saved the count of sentences used to create betrayed embedding\n",
      "Run time for betrayed was 0.024538199999824428 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "bewild\n",
      "ered\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bewildered.\n",
      "Saved the count of sentences used to create bewildered embedding\n",
      "Run time for bewildered was 0.024533651000183454 seconds.\n",
      "\n",
      "There are 3 tokens in tokenized text:\n",
      "bewild\n",
      "er\n",
      "ment\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bewilderment.\n",
      "Saved the count of sentences used to create bewilderment embedding\n",
      "Run time for bewilderment was 0.028449888000068313 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "bi\n",
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bi.\n",
      "Saved the count of sentences used to create bi embedding\n",
      "Run time for bi was 0.029597384999760834 seconds.\n",
      "\n",
      "There are 2 tokens in tokenized text:\n",
      "bil\n",
      "ious\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of tensors is: tensor([nan, nan, nan, nan, nan]) (768 features in tensor)\n",
      "Saved the embedding for bilious.\n",
      "Saved the count of sentences used to create bilious embedding\n",
      "Run time for bilious was 0.031006902000171976 seconds.\n",
      "\n",
      "There are 1 tokens in tokenized text:\n",
      "bit\n",
      "\n",
      "Instance 1 of bit.\n",
      "Looking for vocab token: bit\n",
      "Indices are [22]\n",
      "Size of token embeddings is torch.Size([26, 13, 768])\n",
      "Shape of summed layers is: 26 x 768\n",
      "bit at index 22: [0.21086853742599487, 0.5196075439453125, 0.3275662958621979, 0.3908795118331909, 1.1037216186523438]\n",
      "Grand sum of 1 tensor sets is: [0.21086853742599487, 0.5196075439453125, 0.3275662958621979, 0.3908795118331909, 1.1037216186523438]\n",
      "\n",
      "Instance 2 of bit.\n",
      "Looking for vocab token: bit\n",
      "Indices are [12]\n",
      "Size of token embeddings is torch.Size([16, 13, 768])\n",
      "Shape of summed layers is: 16 x 768\n",
      "bit at index 12: [0.24724599719047546, 0.42722558975219727, 0.332744836807251, 0.42216357588768005, 0.9602119326591492]\n",
      "Grand sum of 2 tensor sets is: [0.45811453461647034, 0.9468331336975098, 0.6603111028671265, 0.8130431175231934, 2.0639336109161377]\n",
      "\n",
      "Instance 3 of bit.\n",
      "Looking for vocab token: bit\n",
      "Indices are [30]\n",
      "Size of token embeddings is torch.Size([45, 13, 768])\n",
      "Shape of summed layers is: 45 x 768\n",
      "bit at index 30: [0.017276005819439888, 0.2702726125717163, 0.29617807269096375, 0.4095306396484375, 0.7606801390647888]\n",
      "Grand sum of 3 tensor sets is: [0.47539055347442627, 1.217105746269226, 0.9564892053604126, 1.2225737571716309, 2.8246138095855713]\n",
      "\n",
      "Instance 4 of bit.\n",
      "Looking for vocab token: bit\n",
      "Indices are [25]\n",
      "Size of token embeddings is torch.Size([35, 13, 768])\n",
      "Shape of summed layers is: 35 x 768\n",
      "bit at index 25: [0.024170154705643654, 0.33883804082870483, 0.44729408621788025, 0.1355164796113968, 0.9544157385826111]\n",
      "Grand sum of 4 tensor sets is: [0.49956071376800537, 1.5559437274932861, 1.4037833213806152, 1.3580902814865112, 3.779029607772827]\n",
      "\n",
      "Instance 5 of bit.\n",
      "Looking for vocab token: bit\n",
      "Indices are [19]\n",
      "Size of token embeddings is torch.Size([34, 13, 768])\n",
      "Shape of summed layers is: 34 x 768\n",
      "bit at index 19: [0.22912615537643433, 0.4870338439941406, 0.26886531710624695, 0.6389355659484863, 1.3444633483886719]\n",
      "Grand sum of 5 tensor sets is: [0.7286868691444397, 2.0429775714874268, 1.6726486682891846, 1.9970258474349976, 5.123493194580078]\n",
      "\n",
      "Instance 6 of bit.\n",
      "Looking for vocab token: bit\n",
      "Indices are [16]\n"
     ]
    }
   ],
   "source": [
    "# Process vocabulary words in the outer loop.\n",
    "for v in vocab:\n",
    "    start = timer()\n",
    "    with open(context_file, 'r') as lines:\n",
    "        v_sum = torch.zeros([1, 768])\n",
    "        v_tokens = tokenize_text(v, tokenizer)\n",
    "        print_tokenized_text(v_tokens, tokenizer)\n",
    "        count_sentence = 0\n",
    "        count_tensor = 0\n",
    "        \n",
    "        # Process all lines in the context file in the inner loop.\n",
    "        for line in lines:\n",
    "            # Check for this vocab word in this line; if found, split the line into individual sentences.\n",
    "            if v in line.lower().split():\n",
    "                for sentence in line.split('.'):\n",
    "                    if v in sentence.lower():\n",
    "                        line = sentence\n",
    "                        count_sentence += 1\n",
    "                        print(f'\\nInstance {count_sentence} of {tokenizer.decode(v_tokens[1:-1]).strip()}.')\n",
    "                        break\n",
    "                # Split the new sentence-based line into tokens.\n",
    "                # Use max_length to avoid overflowing the maximum sequence length for the model.\n",
    "                tokenized_text = tokenize_text(line, tokenizer)\n",
    "#                 print(f'The decoded sentence has {len(tokenized_text)} tokens and is: {tokenizer.decode(tokenized_text)}')\n",
    "                indices = []              \n",
    "\n",
    "                # Check to see whether the vocab word is found in this particular line.\n",
    "                # Initially, some lines may have comprised multiple sentences, which were\n",
    "                # broken out individually above.\n",
    "                for t in v_tokens[1:-1]:\n",
    "                    print(f'Looking for vocab token: {tokenizer.decode(t).strip()}')\n",
    "                    for i, token_str in enumerate(tokenized_text):\n",
    "#                         print(f'Next sentence token: {tokenizer.decode(token_str).strip()}')\n",
    "#                         print(tokenizer.decode(token_str).strip() == tokenizer.decode(t).strip())\n",
    "                        if tokenizer.decode(token_str).strip() == tokenizer.decode(t).strip():\n",
    "                            indices.append(i)               \n",
    "\n",
    "                ###################################################################################\n",
    "                # If the vocabulary word was found, process the containing line.\n",
    "                if indices:\n",
    "\n",
    "                    # The vocab word was found in this line/sentence, at the locations in indices.\n",
    "                    print(f'Indices are {indices}')\n",
    "                    # Get the feature vectors for all tokens in the line/sentence.\n",
    "                    token_embeddings = create_token_embeddings(tokenized_text)\n",
    "                    # Sum the last four layers to get embeddings for the line/sentence.\n",
    "#                         for t in v_tokens[1:-1]:ik\n",
    "#                             for i, token_str in enumerate(tokenized_text):\n",
    "#                                 if (tokenizer.decode(token_str).strip() == tokenizer.decode(t).strip()):\n",
    "#                                     print(f'{tokenizer.decode(token_str).strip()} is index {i} in the sentence and {token_str} in the vocabulary.')\n",
    "                    token_vecs_layer = get_layer_token_vecs(token_embeddings, 12)\n",
    "\n",
    "                    # Get the vocab word's contextual embedding for this line.\n",
    "                    tensor_layer = torch.zeros([1, 768])\n",
    "                    for i in range(len(indices)):\n",
    "                        v_index = i % len(v_tokens[1:-1])\n",
    "                        print(f'{tokenizer.decode(v_tokens[v_index + 1]).strip()} at index {indices[i]}: {token_vecs_layer[indices[i]][:5].tolist()}')\n",
    "                        tensor_layer += token_vecs_layer[indices[i]]\n",
    "#                         print(f'Sum of tensors is: {tensor_layer[0][:5].tolist()} before taking the mean.')\n",
    "\n",
    "                    # If our vocab word is broken into more than one token, we need to get the mean of the token embeddings.\n",
    "                    tensor_layer /= len(indices)\n",
    "#                     print(f'Sum of tensors is: {tensor_layer[0][:5].tolist()} after taking the mean.')\n",
    "\n",
    "                    # Add the embedding distilled from this line to the sum of embeddings for all lines.\n",
    "                    v_sum += tensor_layer\n",
    "                    count_tensor += 1\n",
    "                    print(f'Grand sum of {count_tensor} tensor sets is: {v_sum[0][:5].tolist()}')\n",
    "                ###################################################################################\n",
    "            # Stop processing lines once we've found 2000 instances of our vocab word.\n",
    "            if count_tensor >= 2000:\n",
    "                break\n",
    "        \n",
    "        # We're done processing all lines of 512 tokens or less containing our vocab word.\n",
    "        # Get the mean embedding for the word.\n",
    "        v_mean = v_sum / count_tensor\n",
    "        print(f'Mean of tensors is: {v_mean[0][:5]} ({len(v_mean[0])} features in tensor)')\n",
    "        write_embedding(output_file, v, v_mean)\n",
    "        try:\n",
    "            with open(count_file, 'a') as counts:\n",
    "                counts.write(v + ', ' + str(count_tensor) + '\\n')\n",
    "            print(f'Saved the count of sentences used to create {v} embedding')\n",
    "        except:\n",
    "            print('Wha?! Could not write the sentence count.')\n",
    "    end = timer()\n",
    "    print(f'Run time for {v} was {end - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(vocab_file):\n",
    "    \"\"\"Convert a file of newline separated words into a Python list and return it.\"\"\"\n",
    "    vocab = []\n",
    "    with open(vocab_file, 'r') as v:\n",
    "        vocab = v.read().splitlines()\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, tokenizer):\n",
    "    \"\"\"Break the input text into tokens the model can use, and return them.\"\"\"\n",
    "    tokenized_text = tokenizer.encode(text, add_special_tokens=True, max_length=512)\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tokenized_text(tokens, tokenizer):\n",
    "    \"\"\"Print the number of tokens in some tokenized text, not counting the leading and trailing separators.\n",
    "    Print each token without any leading or trailing whitespace.\"\"\"\n",
    "    print(f'\\nThere are {len(tokens) - 2} tokens in tokenized text:')\n",
    "    for t in tokens[1:-1]:\n",
    "        print(tokenizer.decode(t).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_token_embeddings(tokenized_text):\n",
    "    \n",
    "    input_ids = torch.tensor(tokenized_text).unsqueeze(0)  # Batch size 1\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, masked_lm_labels=input_ids)\n",
    "        encoded_layers = outputs[2]\n",
    "        token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        token_embeddings = token_embeddings.permute(1,0,2)\n",
    "        print(f'Size of token embeddings is {token_embeddings.size()}')\n",
    "        return token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the last 4 layers' features\n",
    "def sum_last_four_token_vecs(token_embeddings):\n",
    "    token_vecs_sum_last_four = []\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "        # `token` is a [13 x 768] tensor\n",
    "        # Sum the vectors from the last 4 layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum_last_four.append(sum_vec)\n",
    "\n",
    "    print ('Shape of summed layers is: %d x %d' % (len(token_vecs_sum_last_four), len(token_vecs_sum_last_four[0])))\n",
    "    # Shape is: <token count> x 768\n",
    "    return token_vecs_sum_last_four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a single layer of the model.\n",
    "def get_layer_token_vecs(token_embeddings, layer_number):\n",
    "    token_vecs_layer = []\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "        # `token` is a [13 x 768] tensor\n",
    "        # Sum the vectors from the last 4 layers.\n",
    "        layer_vec = token[layer_number]\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_layer.append(layer_vec)\n",
    "\n",
    "    print ('Shape of summed layers is: %d x %d' % (len(token_vecs_layer), len(token_vecs_layer[0])))\n",
    "    # Shape is: <token count> x 768\n",
    "    return token_vecs_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_embedding(embeddings_file, vocab_word, contextual_embedding):\n",
    "    try:\n",
    "        with open(embeddings_file, 'a') as f:\n",
    "            f.write(vocab_word)\n",
    "            for value in contextual_embedding[0]:\n",
    "                f.write(' ' + str(value.item()))\n",
    "            f.write('\\n')\n",
    "        print(f'Saved the embedding for {vocab_word}.')\n",
    "    except:\n",
    "        print('Oh no! Unable to write to the embeddings file.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
